{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LTRDCN-2765","text":"<p>Welcome to Cisco Live LTRDCN-2765: VXLAN EVPN Fabric and NetDevOps/automation using Ansible.</p> <p>For full documentation visit Cisco Live.</p>"},{"location":"#speakers","title":"Speakers","text":"<ul> <li>Faisal Chaudhry <code>Principal Architect, Cisco CX</code></li> <li>Lei Tian <code>Solutions Architect, Cisco CX</code></li> </ul>"},{"location":"Task1-ansible-node/","title":"Task1 ansible node","text":"<p>Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.</p>"},{"location":"Task1-ansible-node/#step-1-connect-to-lab-using-anyconnect-vpn","title":"Step 1: Connect to lab using anyconnect VPN","text":"<p>You will connect to Anyconnect url using Cisco VPN AnyConnect client and with the username &amp; password as documented in below table.  Below screenshot shows an example of that VPN connection.</p> Pod ID Attendee Name \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Anyconnect url\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Anyconnect Username Anyconnect Password POD  1 Andreas Andersson dcloud-lon-anyconnect.cisco.com v903user1 b85d1d POD  2 Andrius Kislas dcloud-lon-anyconnect.cisco.com v138user1 78b1bd POD  3 Arunas  Valancauskas dcloud-lon-anyconnect.cisco.com v557user1 e03fbd POD  4 Donatien    Miramont dcloud-lon-anyconnect.cisco.com v611user1 625a10 POD  5 Florent CHETAIL dcloud-lon-anyconnect.cisco.com v900user1 18dd7b POD  6 Florent Hing dcloud-lon-anyconnect.cisco.com v581user1 36a518 POD  7 Joao    Gaspar dcloud-lon-anyconnect.cisco.com v554user1 c6670a POD  8 Joern   Classen dcloud-lon-anyconnect.cisco.com v2457user1 e36fa8 POD  9 Kamiel  Braet dcloud-lon-anyconnect.cisco.com v1432user1 5b95af POD  10 Martin Hauser dcloud-lon-anyconnect.cisco.com v1680user1 31bd05 POD  11 Martin Simek dcloud-lon-anyconnect.cisco.com v10user1 538e81 POD  12 Norbert    Kalbe dcloud-lon-anyconnect.cisco.com v1590user1 670599 POD  13 Olaf   Damis dcloud-lon-anyconnect.cisco.com v490user1 e860d0 POD  14 \u00d8yvind Westrum dcloud-lon-anyconnect.cisco.com v2666user1 94ef6d POD  15 Renato Lopez dcloud-lon-anyconnect.cisco.com v1756user1 65a1e8 POD  16 Victor Peral Severiano dcloud-lon-anyconnect.cisco.com v2805user1 1fe4fa POD  17 dcloud-lon-anyconnect.cisco.com v212user1 41d959 POD  18 dcloud-lon-anyconnect.cisco.com v2751user1 cc5fc5 <p>Note</p> <p>lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.</p> <p></p>"},{"location":"Task1-ansible-node/#step-2-enter-vpn-credentials","title":"Step 2: Enter VPN credentials","text":"<p>After prompted for credentials, use the credentials documented in above table or provided by the lab admin.</p> <ul> <li>Below is an example of user logging into a reference POD:</li> </ul> <p></p> <ul> <li>Hit accept when the prompt appears to accept the VPN connection login</li> </ul> <p></p>"},{"location":"Task1-ansible-node/#step-3-rdp-to-workstation","title":"Step 3: RDP to workstation","text":"<p>In this step, you will connect to the workstation with Remote Desktop (RDP) client on your machines.  Use below details for this RDP session:</p> <ul> <li>Workstation: 198.18.133.36</li> <li>Username: dcloud\\demouser</li> <li>Password: C1sco12345</li> </ul> <p>Below screenshot is only an example for this RDP connection:</p> <p></p>"},{"location":"Task1-ansible-node/#step-4-ssh-client-mtputty","title":"Step 4: SSH client - MTputty","text":"<p>Once you have the RDP session to the remote workstation, then you will use MTPutty client to connect to all devices (Nexus and Ansible server/node) in this lab.</p> <p>MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking the MTPutty icon on the desktop:</p> <p></p>"},{"location":"Task1-ansible-node/#step-5-ssh-into-ansible-node","title":"Step 5: SSH into Ansible node","text":"<p>In the MTPutty client, SSH to Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pane.</p> <p>The passwords are pre-configured however if prompted then use credentials of username root and password C1sco12345 for SSH access</p> <p></p> <p>On this Ansible node various software packages will be installed from repositories that are hosted on the Internet.  A default route is added, to access those repositories on the internet, by executing the following command on Ansible node:</p> <pre><code>sudo route add default gw 198.18.128.1\n</code></pre>"},{"location":"Task1-ansible-node/#step-6-verify-python","title":"Step 6: Verify Python","text":"<p>Ansible can on any machine that has Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansible.</p> <p>Hence, once successfully SSH into the ansible node, verify that python version 2.7 is pre-installed by running below command:</p> <pre><code>python --version\n</code></pre> <p>The output of above command confirms python version installed on Ansible node.</p> <p></p>"},{"location":"Task1-ansible-node/#step-7-update-ubuntu-package-tool-and-install-pip-packages","title":"Step 7: Update Ubuntu package tool and install PIP &amp; Packages","text":"<ul> <li>After verifying we have the minimum version of python installed, we are now going to update Ubuntu package tool (using <code>apt-get update</code>) and install PIP python package (using <code>apt install pip</code>) commands.  PIP is a package manager that is used to install and maintain Python software packages.</li> </ul> <p>Note: At the <code>Do you want to continue? [Y/n]</code> prompt, you must enter <code>Y</code> to proceed with update/install:</p> <pre><code>apt-get update\napt install pip\n</code></pre> <p>Below two screenshot shows the partial execution of above commands:</p> <p></p> <p></p> <ul> <li>Next, update pip to version 20.3.4 by executing below command:</li> </ul> <pre><code>pip install --upgrade pip==20.3.4\n</code></pre> <p>Note</p> <p>You can ignore the warning related to python version.  Below screenshot shows the execution of above command.</p> <p></p> <p>After installing PIP package, we are going to update pyopenssl package and also install packages required for this Ansible based VXLAN lab.  Here is the list of the required python packages in this lab:</p> <ul> <li>Paramiko</li> <li>PyYAML</li> <li>Jinj2</li> <li>Httplib2</li> </ul> <p>Let's proceed with upgrading the pyopenssl packge.</p> <ul> <li>Run the below command to update pyopenssl package:</li> </ul> <pre><code>pip install pyopenssl --upgrade\n</code></pre> <p>Below screenshot shows the execution of the above command:</p> <p></p> <ul> <li>Run the below command to install the above listed required packages:</li> </ul> <pre><code>pip install paramiko PyYAML jinja2 httplib2\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"Task1-ansible-node/#step-8-ansible-installation","title":"Step 8: Ansible Installation","text":"<p>In this step, Ansible and the required modules for this lab are going to be installed on this node/server (running Ubuntu).</p> <ul> <li>Initiate the installation of ansible using below command:</li> </ul> <p>Note</p> <p>It may take few minutes for it to download and install.</p> <pre><code>pip install ansible==2.10.6\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Once the installation is complete, check Ansible version by executing below command:</li> </ul> <pre><code>ansible --version\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Starting from Ansible 2.9, plugins and modules for network platform are moved in to Collection.  To install NX-OS collection 2.9.1 with Ansible Galaxy by using below <code>ansible-galaxy ...</code> command:</li> </ul> <pre><code>ansible-galaxy collection install -f cisco.nxos:2.9.1\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <p>Note</p> <p>Your output might show different version than the installed version. This is due to the separation of ansible-base (ansible-core) and community package starting from version 2.10. The above command shows ansible-base version. You can find more detail explanation from Ansible documentation page https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html</p>"},{"location":"Task1-ansible-node/#step-9-create-ansible-settings-inventory-and-variables","title":"Step 9: Create Ansible Settings, Inventory and variables","text":"<p>Ansible server uses host inventory to communicate with target devices (called as hosts). Like other programming languages, ansible uses variables to represent variations among different target devices.   This is important as Ansible works against multiple systems by selecting portions of systems listed in Ansible inventory.  Additionally, Ansible settings such as how SSH keys verification is done, name/location of inventory file to be used etc are adjustable via a configuration file named <code>ansible.cfg</code>.  It is an INI based file.</p> <ul> <li>Create folder named EVPN-Ansible as working environment and verify that it\u2019s empty by issuing below commands:</li> </ul> <pre><code>mkdir EVPN-Ansible &amp;&amp; cd EVPN-Ansible\nls\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>Next:</p> <ul> <li>Create Ansible inventory file to include Spine and Leaf switches.</li> <li>By default Ansible has inventory file saved in location /etc/ansible/hosts.</li> <li>In this lab we will create hosts file in the working environment. This file is created from ansible host prompt using <code>cat</code> command.   </li> </ul> <p>Note</p> <p>You should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code> (also shown in subsequent screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; hosts\n#define global variables, groups and host variables\n[all:vars]\nansible_connection = ansible.netcommon.network_cli\nansible_network_os = cisco.nxos.nxos\nansible_user=admin\nansible_password=C1sco12345\nansible_command_timeout=180\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\nEOF\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more hosts\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create Ansible config (ansible.cfg) using the same steps as above.  </li> </ul> <p>Note</p> <p>you should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code>  (as shown in below screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ansible.cfg\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\ndeprecation_warnings = False\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more ansible.cfg\n</code></pre> <p>Below screenshot shows the output of above commands:</p> <p></p> <ul> <li>To verify the file that you just created under project folder EVPN-Ansible, issue the list command:</li> </ul> <pre><code>ls\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create host variable folder named <code>host_vars</code> in folder <code>EVPN-Ansible</code> by using below command.  In this lab, we will use host_vars file to define the variables for various hosts (in next bullets):</li> </ul> <pre><code>mkdir host_vars &amp;&amp; cd host_vars\n</code></pre> <ul> <li>Create host variable file for each host in inventory by using the cat command.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.101.yml\n---\nhostname: leaf_1\nloopback0: 192.168.0.8\nloopback1: 192.168.0.18\nrouter_id: 192.168.0.8\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.101.yml</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.101.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.   The variable file for a switch is created using the below <code>cat</code> command:</li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot).  The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.103.yml\n---\nhostname: leaf_3\nloopback0: 192.168.0.10\nloopback1: 192.168.0.110\nrouter_id: 192.168.0.10\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.103.yml</code> as shown below:</li> </ul> <pre><code>more 198.18.4.103.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.104.yml\n---\nhostname: leaf_4\nloopback0: 192.168.0.11\nloopback1: 192.168.0.111\nrouter_id: 192.168.0.11\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.104.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.201.yml\n---\nhostname: spine-1\nloopback0: 192.168.0.6\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.6\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.201.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.202.yml\n---\nhostname: spine-2\nloopback0: 192.168.0.7\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.7\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.202.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p>"},{"location":"Task1-ansible-node/#step-10-ansible-role-structure","title":"Step 10: Ansible role structure","text":"<p>Role is very useful technique to manage a set of playbooks in Ansible. In this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches.</p> <p>We will use role structure and manage the two plays into single playbook. A role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.</p> <p>In this lab:</p> <ul> <li>we will use <code>vars</code>, <code>templates</code> and <code>tasks</code> folders</li> <li><code>main.yml</code> file in <code>/vars</code>  folder contains dictionary of variables for this role</li> <li><code>main.yml</code> file in <code>/tasks</code> folder contains the Ansible playbook for this role</li> </ul> <p>To proceed further with roles in subsequent Tasks:</p> <ul> <li>Create roles directory in folder EVPN-Ansible by issuing below commands:</li> </ul> <pre><code>cd /root/EVPN-Ansible\nmkdir roles\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>This will be used in the subsequent tasks in this lab.</p>"},{"location":"Task1-ansible-node0/","title":"Task 1 - Prepare Ansible node","text":"<p>Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.</p>"},{"location":"Task1-ansible-node0/#step-1-connect-to-lab-using-anyconnect-vpn","title":"Step 1: Connect to lab using anyconnect VPN","text":"<p>You will connect to Anyconnect url using Cisco VPN AnyConnect client and with the username &amp; password as documented in below table.  Below screenshot shows an example of that VPN connection.</p> Pod ID Attendee Name Anyconnect url Anyconnect Username Anyconnect Password POD  1 dcloud-lon-anyconnect.cisco.com POD  2 dcloud-lon-anyconnect.cisco.com POD  3 dcloud-lon-anyconnect.cisco.com POD  4 dcloud-lon-anyconnect.cisco.com POD  5 dcloud-lon-anyconnect.cisco.com POD  6 dcloud-lon-anyconnect.cisco.com POD  7 dcloud-lon-anyconnect.cisco.com POD  8 dcloud-lon-anyconnect.cisco.com POD  9 dcloud-lon-anyconnect.cisco.com POD  10 dcloud-lon-anyconnect.cisco.com POD  11 dcloud-lon-anyconnect.cisco.com POD  12 dcloud-lon-anyconnect.cisco.com POD  13 dcloud-lon-anyconnect.cisco.com POD  14 dcloud-lon-anyconnect.cisco.com POD  15 dcloud-lon-anyconnect.cisco.com POD  16 dcloud-lon-anyconnect.cisco.com POD  17 dcloud-lon-anyconnect.cisco.com POD  18 dcloud-lon-anyconnect.cisco.com <p>Note</p> <p>lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.</p> <p></p>"},{"location":"Task1-ansible-node0/#step-2-enter-vpn-credentials","title":"Step 2: Enter VPN credentials","text":"<p>After prompted for credentials, use the credentials documented in above table or provided by the lab admin.</p> <ul> <li>Below is an example of user logging into a reference POD:</li> </ul> <p></p> <ul> <li>Hit accept when the prompt appears to accept the VPN connection login</li> </ul> <p></p>"},{"location":"Task1-ansible-node0/#step-3-rdp-to-workstation","title":"Step 3: RDP to workstation","text":"<p>In this step, you will connect to the workstation with Remote Desktop (RDP) client on your machines.  Use below details for this RDP session:</p> <ul> <li>Workstation: 198.18.133.36</li> <li>Username: dcloud\\demouser</li> <li>Password: C1sco12345</li> </ul> <p>Below screenshot is only an example for this RDP connection:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-4-ssh-client-mtputty","title":"Step 4: SSH client - MTputty","text":"<p>Once you have the RDP session to the remote workstation, then you will use MTPutty client to connect to all devices (Nexus and Ansible server/node) in this lab.</p> <p>MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking the MTPutty icon on the desktop:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-5-ssh-into-ansible-node","title":"Step 5: SSH into Ansible node","text":"<p>In the MTPutty client, SSH to Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pane.</p> <p>The passwords are pre-configured however if prompted then use credentials of username root and password C1sco12345 for SSH access</p> <p></p> <p>On this Ansible node various software packages will be installed from public repositories on the Internet.</p>"},{"location":"Task1-ansible-node0/#step-6-verify-python","title":"Step 6: Verify Python","text":"<p>Ansible can on any machine that has Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansible.</p> <p>Hence, once successfully SSH into the ansible node, verify that python version 2.7 is pre-installed by running below command:</p> <pre><code>python --version\n</code></pre> <p>The output of above command confirms python version installed on Ansible node.</p> <p></p>"},{"location":"Task1-ansible-node0/#step-7-update-ubuntu-package-tool-and-install-pip-packages","title":"Step 7: Update Ubuntu package tool and install PIP &amp; Packages","text":"<ul> <li>After verifying we have the minimum version of python installed, we are now going to update Ubuntu package tool (using <code>apt-get update</code>) </li> </ul> <pre><code>apt-get update\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Next, installation of PIP python package (using <code>apt install pip</code>) command is done.  PIP is a package manager that is used to install and maintain Python software packages.</li> </ul> <p>Note: At the <code>Do you want to continue? [Y/n]</code> prompt, you must enter <code>Y</code> to proceed with update/install:</p> <pre><code>apt install pip\n</code></pre> <p>Below screenshot shows the partial execution of above command:</p> <p></p> <ul> <li>Next, upgrade pip by executing below command:</li> </ul> <pre><code>pip install --upgrade pip\n</code></pre> <p>Note</p> <p>You can ignore the warning related to python version.  Below screenshot shows the execution of above command.</p> <p></p> <p>PIP package will be used to update pyopenssl package and install following python packages that are required for Ansible NXOS modules:</p> <ul> <li>paramiko</li> <li>PyYAML</li> <li>jinj2</li> <li>httplib2</li> <li> <p>ansible-pylibssh</p> </li> <li> <p>Run the below command to update pyopenssl package:</p> </li> </ul> <pre><code>pip install pyopenssl --upgrade\n</code></pre> <p>Below screenshot shows the execution of the above command:</p> <p></p> <ul> <li>Run the below command to install the above listed required packages:</li> </ul> <pre><code>pip install paramiko PyYAML jinja2 httplib2 ansible-pylibssh\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-8-ansible-installation","title":"Step 8: Ansible Installation","text":"<p>In this step, Ansible and the required modules for this lab are going to be installed on this node/server (running Ubuntu).</p> <ul> <li>Initiate the installation of ansible using below command:</li> </ul> <p>Note</p> <p>It may take few minutes for it to download and install.  You can ignore the pip WARNING, related to root user, after successful installation of ansible.</p> <pre><code>pip install ansible==2.10.6\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Once the installation is complete, check Ansible version by executing below command:</li> </ul> <pre><code>ansible --version\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <p>Note</p> <p>Your output might show different version than the installed version. This is due to the separation of ansible-base (ansible-core) and community package starting from version 2.10. The above command shows ansible-base version. You can find more detail explanation from Ansible documentation page https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html</p> <ul> <li>Starting from Ansible 2.9, plugins and modules for network platform are moved in to Collection.  To install NX-OS collection 2.9.1 with Ansible Galaxy by using below <code>ansible-galaxy ...</code> command:</li> </ul> <pre><code>ansible-galaxy collection install -f cisco.nxos:4.1.0\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-9-create-ansible-settings-inventory-and-variables","title":"Step 9: Create Ansible Settings, Inventory and variables","text":"<p>Ansible server uses host inventory to communicate with target devices (called as hosts). Like other programming languages, ansible uses variables to represent variations among different target devices.   This is important as Ansible works against multiple systems by selecting portions of systems listed in Ansible inventory.  Additionally, Ansible settings such as how SSH keys verification is done, name/location of inventory file to be used etc are adjustable via a configuration file named <code>ansible.cfg</code>.  It is an INI based file.</p> <ul> <li>Create folder named EVPN-Ansible as working environment and verify that it\u2019s empty by issuing below commands:</li> </ul> <pre><code>mkdir EVPN-Ansible &amp;&amp; cd EVPN-Ansible\nls\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>Next:</p> <ul> <li>Create Ansible inventory file to include Spine and Leaf switches.</li> <li>By default Ansible has inventory file saved in location /etc/ansible/hosts.</li> <li>In this lab we will create hosts file in the working environment. This file is created from ansible host prompt using <code>cat</code> command.   </li> </ul> <p>Note</p> <p>You should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code> (also shown in subsequent screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; hosts\n#define global variables, groups and host variables\n[all:vars]\nansible_connection = ansible.netcommon.network_cli\nansible_network_os = cisco.nxos.nxos\nansible_user=admin\nansible_password=C1sco12345\nansible_command_timeout=180\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\nEOF\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more hosts\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create Ansible config (ansible.cfg) using the same steps as above.  </li> </ul> <p>Note</p> <p>you should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code>  (as shown in below screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ansible.cfg\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\ndeprecation_warnings = False\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more ansible.cfg\n</code></pre> <p>Below screenshot shows the output of above commands:</p> <p></p> <ul> <li>To verify the file that you just created under project folder EVPN-Ansible, issue the list command:</li> </ul> <pre><code>ls\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create host variable folder named <code>host_vars</code> in folder <code>EVPN-Ansible</code> by using below command.  In this lab, we will use host_vars file to define the variables for various hosts (in next bullets):</li> </ul> <pre><code>mkdir host_vars &amp;&amp; cd host_vars\n</code></pre> <ul> <li>Create host variable file for each host in inventory by using the cat command.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.101.yml\n---\nhostname: leaf_1\nloopback0: 192.168.0.8\nloopback1: 192.168.0.18\nrouter_id: 192.168.0.8\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.101.yml</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.101.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.   The variable file for a switch is created using the below <code>cat</code> command:</li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot).  The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.103.yml\n---\nhostname: leaf_3\nloopback0: 192.168.0.10\nloopback1: 192.168.0.110\nrouter_id: 192.168.0.10\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.103.yml</code> as shown below:</li> </ul> <pre><code>more 198.18.4.103.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.104.yml\n---\nhostname: leaf_4\nloopback0: 192.168.0.11\nloopback1: 192.168.0.111\nrouter_id: 192.168.0.11\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.104.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.201.yml\n---\nhostname: spine-1\nloopback0: 192.168.0.6\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.6\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.201.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.202.yml\n---\nhostname: spine-2\nloopback0: 192.168.0.7\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.7\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.202.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-10-ansible-role-structure","title":"Step 10: Ansible role structure","text":"<p>Role is very useful technique to manage a set of playbooks in Ansible. In this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches.</p> <p>A role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.  Here are few folders:</p> <ul> <li><code>main.yml</code> file in <code>/vars</code>  folder contains variables for a role</li> <li><code>main.yml</code> file in <code>/tasks</code> folder contains Ansible playbook for a role</li> <li><code>templates</code> folder may contain Jinja2 files, if these are used for a role </li> </ul> <p>To proceed further with roles in subsequent Tasks:</p> <ul> <li>Create roles directory in folder EVPN-Ansible by issuing below commands:</li> </ul> <pre><code>cd /root/EVPN-Ansible\nmkdir roles\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>This will be used in the subsequent tasks in this lab.</p>"},{"location":"appendixA-F5/","title":"Appendix A: L4-L7 insertion","text":"<p>In this section, you will insert F5 BIG-IP load balancer into the fabric.  - First send out a http request from server-4 to a VIP, and you will notice no http service enabled on VIP.  - Run the playbook to enable http service on the VIP with server-1, server-2, server-3 and server-4 in the server pool. - After successful execute the playbook, you will notice http request to VIP is load balanced across three servers. </p>"},{"location":"appendixA-F5/#step-1-power-on-f5-virtual-machine","title":"Step 1: Power on F5 Virtual Machine","text":"<ul> <li> <p>open VMware vSphere Client to login ESXi (198.18.133.33) using crendentials of root/C1sco12345</p> <p></p> </li> <li> <p>Open up the VM under host 198.18.133.33, and power on VM named F5_LTM as shown below:</p> <p></p> </li> </ul>"},{"location":"appendixA-F5/#step-2-install-ansible-pre-reqs","title":"Step 2: Install Ansible pre-reqs","text":"<ul> <li>Switch to MTPuTTY install prerequisites packages on Ansible node by using below <code>pip install ...</code> commands (Note: the bold commands need to be executed):<pre><code>[root@rhel7-tools LTRDCN-1572]# pip install f5-sdk\n[root@rhel7-tools LTRDCN-1572]# pip install setuptools --upgrade\n[root@rhel7-tools LTRDCN-1572]# pip install bigsuds\n[root@rhel7-tools LTRDCN-1572]# pip install netaddr\n</code></pre> </li> </ul>"},{"location":"appendixA-F5/#step-3-check-virtual-ip-using-curl","title":"Step 3: Check Virtual IP using curl","text":"<ul> <li>Switch to \u2018MTPuTTY\u2019 and ssh into server-4.  Then send http request to Virtual IP (VIP) 172.21.140.100 from server-4 using <code>curl http://172.21.140.100</code> command  </li> <li>Below output shows the execution of above command i.e., HTTP request to VIP should fail:<pre><code>[root@server-4 ~]# curl http://172.21.140.100\ncurl: (7) couldn't connect to host`\n[root@server-4 ~]#\n</code></pre> </li> </ul>"},{"location":"appendixA-F5/#step-4","title":"Step 4:","text":"<ul> <li>Switch to Atom, create new file name \u2018f5ltm.yml\u2019 under project folder LTRDCN-1572.  </li> <li>Enter below data in this new file (named \u2018f5ltm.yml\u2019)  </li> <li>Make sure to click <code>File</code> and <code>Save</code> to ftp this data to Ansible server:</li> </ul> <pre><code>---\n- name: Configurating BIG-IP\nhosts: localhost\ngather_facts: false\nvars:\nprovider:\npassword: admin\nserver: 198.18.4.10\nuser: admin\nvalidate_certs: False\n\ntasks:\n- name: Configure server facing port to L2\nnxos_interface:\ninterface: eth1/4\nmode: layer2\nusername: \"{{ user }}\"\npassword: \"{{ pwd }}\"\ntransport: nxapi\nhost: \"198.18.4.104\"\n- name: Configure VLAN for F5 port\nnxos_switchport:\ninterface: eth1/4\nmode: access\naccess_vlan: 140\nusername: \"{{ user }}\"\npassword: \"{{ pwd }}\"\ntransport: nxapi\nhost: \"198.18.4.104\"\n- name: Configure VLANs on the BIG-IP\nbigip_vlan:\nname: \"External\"\ntag: \"140\"\nuntagged_interface: \"1.1\"\nprovider: \"{{ provider }}\"\n- name: Configure SELF-IPs on the BIG-IP\nbigip_selfip:\nname: \"172.21.140.50\"\naddress: \"172.21.140.50\"\nnetmask: \"255.255.255.0\"\nvlan: \"External\"\nallow_service: \"default\"\nprovider: \"{{ provider }}\"\n- name: Create static route\nbigip_static_route:\nprovider: \"{{ provider }}\"\ndestination: 0.0.0.0\nnetmask: 0.0.0.0\ngateway_address: 172.21.140.1\nname: \"defult\"\n- name: Create nodes\nbigip_node:\nprovider: \"{{ provider }}\"\nhost: \"{{item}}\"\nname: \"{{item}}\"\nwith_items:\n- 172.21.140.10\n- 172.21.140.11\n- 172.21.141.10\n- 172.21.141.11\n\n- name: Create pool\nbigip_pool:\nprovider: \"{{ provider }}\"\nname: \"web-pool\"\nlb_method: \"round-robin\"\nmonitors: \"/Common/http\"\nmonitor_type: \"and_list\"\n\n- name: Add Pool members\nbigip_pool_member:\nprovider: \"{{ provider }}\"\nname: \"{{item}}\"\nhost: \"{{item}}\"\nport: \"80\"\npool: \"web-pool\"\nwith_items:\n- 172.21.140.10\n- 172.21.140.11\n- 172.21.141.10\n- 172.21.141.11\n\n- name: Add Virtual Server\nbigip_virtual_server:\nprovider: \"{{ provider }}\"\nname: \"http-virtualserver\"\ndestination: \"172.21.140.100\"\nport: \"80\"\nenabled_vlans: \"ALL\"\nall_profiles:\n- http\npool: \"web-pool\"\nsnat: \"Automap\"\n</code></pre>"},{"location":"appendixA-F5/#step-5-run-playbook","title":"Step 5: Run playbook","text":"<ul> <li> <p>On the Ansible node (using MTputty SSH), run playbook f5ltm.yml to provision VIP (172.21.140.100) on F5 and to also put all four servers into the server pool using <code>ansible-playbook f5ltm.yml</code> command.  </p> </li> <li> <p>Below shows the output of above command: </p> </li> </ul> <p> </p>"},{"location":"appendixA-F5/#step-6-check-load-balancer","title":"Step 6: Check load balancer","text":"<ul> <li>Switch to \u2018MTPuTTY\u2019 and login to server-4,</li> <li>Run <code>curl http://172.21.140.100</code> command multiple times, <ul> <li>Note that the request is load balanced to differert servers in the sever pool</li> </ul> </li> <li>The execution and output of above command (when its run multiple times) is shown below: </li> </ul> <pre><code>[root@server-4 ~]# curl http://172.21.140.100\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;\n&lt;p&gt; Server-1 172.21.140.10 &lt;/p&gt;\n&lt;p&gt;This is the default web page for this server.&lt;/p&gt;\n&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\n[root@server-4 ~]# curl http://172.21.140.100\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;\n&lt;p&gt; Server-3 172.21.140.11 &lt;/p&gt;\n&lt;p&gt;This is the default web page for this server.&lt;/p&gt;\n&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\n[root@server-4 ~]# curl http://172.21.140.100\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;\n&lt;p&gt; Server-4 172.21.141.11 &lt;/p&gt;\n&lt;p&gt;This is the default web page for this server.&lt;/p&gt;\n&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>This concludes Appendix A using ansible to insert F5 (as load balancer) in the DC Fabric.  Well done!</p>"},{"location":"appendixB-POAP/","title":"Appendix B: Day 0 automation using POAP","text":"<p>In this section, you will deploy Power On Auto Provisioning (POAP) for leaf4 in DCNM. </p> mgmt Loopback0 Loopback1 Eth1/1 Eth1/2 198.18.4.104 198.168.0.11 198.168.0.111 10.0.128.2 10.0.128.18"},{"location":"appendixB-POAP/#step-1","title":"Step 1:","text":"<ul> <li>Open Google Chrome and login Cisco DCNM (https://198.18.134.200) using username/password admin/C1sco12345 </li> </ul>"},{"location":"appendixB-POAP/#step-2","title":"Step 2:","text":"<ul> <li>On the left side menu, click Configuration &gt; Deploy &gt; POAP</li> </ul>"},{"location":"appendixB-POAP/#step-3","title":"Step 3:","text":"<ul> <li>Click DHCP Scopes on POAP page</li> </ul>"},{"location":"appendixB-POAP/#step-4","title":"Step 4:","text":"<ul> <li>Click plus sign to add new DHCP scope, name it \u2018vxlan_evpn_leaf\u2019 add DCHP range \u2018198.18.4.100-198.18.4.104\u2019 as IP pool </li> </ul>"},{"location":"appendixB-POAP/#step-5","title":"Step 5:","text":"<ul> <li>click OK to close the DHCP scope window</li> </ul>"},{"location":"appendixB-POAP/#step-6","title":"Step 6:","text":"<ul> <li>Click left side menu Configure &gt; Deploy &gt; POAP</li> </ul>"},{"location":"appendixB-POAP/#step-7","title":"Step 7:","text":"<ul> <li>Click Images and Configuration on the POAP page</li> </ul>"},{"location":"appendixB-POAP/#step-8","title":"Step 8:","text":"<ul> <li>Check the Default_SCP_Repository, click File Browser to verify image nxos.7.0.3.I7.0.154.bin file is in the repository. </li> </ul>"},{"location":"appendixB-POAP/#step-9","title":"Step 9:","text":"<ul> <li>Click left side menu Configure &gt; Deploy &gt; POAP</li> </ul>"},{"location":"appendixB-POAP/#step-10","title":"Step 10:","text":"<ul> <li>Client \u2018template\u2019 under POAP Definitions</li> </ul>"},{"location":"appendixB-POAP/#step-11","title":"Step 11:","text":"<ul> <li>Click import Template from the Templates page</li> </ul>"},{"location":"appendixB-POAP/#step-12","title":"Step 12:","text":"<ul> <li>Locate simple_template.template file on desktop, and open the template. </li> </ul>"},{"location":"appendixB-POAP/#step-13","title":"Step 13:","text":"<ul> <li>Save the template </li> </ul>"},{"location":"appendixB-POAP/#step-14","title":"Step 14:","text":"<ul> <li>Click left side menu Configure &gt; Deploy &gt; POAP</li> </ul>"},{"location":"appendixB-POAP/#step-15","title":"Step 15:","text":"<ul> <li>Click plus sign to add new POAP Definitions </li> </ul>"},{"location":"appendixB-POAP/#step-16","title":"Step 16:","text":"<ul> <li>Check Generate Definition and click next</li> </ul>"},{"location":"appendixB-POAP/#step-17","title":"Step 17:","text":"<ul> <li>Switch to MTPuTTY open connection to switch leaf-4, type in command \u2018show module\u2019 and write down the Serial-Num</li> </ul> <p>Note: The Serial-Num from your output might be different</p> <ul> <li> <p>Put the Serial Number (9VSCMHEMJ69) for switch leaf-4 in \u2018Switches\u2019 field. Fill in other information on the page as following. </p> </li> <li> <p>Use drop down list to fill in rest information </p> <ul> <li>Switch Type: N9K</li> <li>Image Server: Default_SCP_Repository</li> <li>Switch Username: admin</li> <li>Switch Password: C1sco12345</li> </ul> </li> </ul> <p></p>"},{"location":"appendixB-POAP/#step-18","title":"Step 18:","text":"<ul> <li>Click Next and select simple_template from template drop down list, and fill in other information as below<ul> <li>Switch Name : leaf-4</li> <li>Administrative Username: Admin</li> <li>Administrative Password: c1sco12345</li> <li>Management IP: 198.18.4.104</li> <li>Management Prefix: 24</li> <li>Default Gateway: 198.18.4.1</li> <li>Console timeout: 0</li> <li>Console Speed : 9600</li> </ul> </li> </ul>"},{"location":"appendixB-POAP/#step-19","title":"Step 19:","text":"<ul> <li> <p>Select switch leaf-4 with Serial Number \u201891AQACU3U3UH9\u2019</p> </li> <li> <p>Click Next</p> </li> </ul>"},{"location":"appendixB-POAP/#step-20","title":"Step 20:","text":"<ul> <li>Click Preview CLI as shown below</li> <li>Then click Finish </li> </ul>"},{"location":"appendixB-POAP/#step-21","title":"Step 21:","text":"<ul> <li>select switch leaf-4, Click Write Erase and Reload to delte leaf-4 configuration and kick off the POAP process </li> </ul> <ul> <li>Click Continue on the popup warning </li> </ul>"},{"location":"appendixB-POAP/#step-22","title":"Step 22:","text":"<ul> <li>Connect to Leaf-4 console port from MTPuTTY, watch the POAP process</li> </ul> <p>If the POAP scripts fail, verify the S/N in console. If the S/N doesn\u2019t match what you configured in POAP definition, you need to configure new POAP definition matches the S/N. </p> <p></p>"},{"location":"appendixB-POAP/#step-23","title":"Step 23:","text":"<ul> <li>Login leaf-4 to verify running configuration after POAP is completed</li> </ul> <p>POAP will take 20 mins to bootup; up to this point, you have completed all tasks.</p>"},{"location":"appendixB-POAP/#congratulation-you-have-completed-the-whole-lab","title":"Congratulation! You have completed the whole lab.","text":""},{"location":"appendixB-Upgrade/","title":"Appendix B: Software compliance check and remediation","text":"<p>In this section, we will run software version compliance check using Ansible. For fabric switch that is not running on standard software version, we will perform software upgrade and bring all fabric switches into the standard version.  </p> <p>In this playbook, we will use \u201cnxos_facts\u201d to find the software version on each fabric switch. Then we will compare with standard software version, 7.0(3)I7(4) in this lab. For fabric switch that is not running on standard version, the playbook will upgrade and reboot the switch.</p> <p>The playbook will use \u201cnxos_file_copy\u201d module to copy image from remote repository to bootflash.</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and create new file under this folder (\u201cEVPN-Ansible\u201d). Name the new file \u201ccode_upgrade.yml\u201d. and add below content (you may copy and paste):</li> </ul> <pre><code>---\n#Appendix code upgrade\n- hosts: spine,leaf\nvars:\n- standard: 7.0(3)I7(4)\n- image_file: nxos.7.0.3.I7.4.bin\ntasks:\n- name: \"software complaince check\"\ncisco.nxos.nxos_facts:\ngather_subset: all\n- name: \"change to standard code\"\nblock:\n- debug: msg=\"{{ansible_net_hostname}} is not running standard {{standard}}\"\n- cisco.nxos.nxos_feature:\nfeature: scp-server\nstate: enabled\n- name: \"upload image file\"\ncisco.nxos.nxos_file_copy:\nfile_pull: True\nfile_pull_timeout: 1200\nremote_file: \"/root/downloads/{{image_file}}\"\nremote_scp_server: \"198.18.4.150\"\nremote_scp_server_user: \"root\"\nremote_scp_server_password: \"C1sco12345\"\n- name: \"change boot statement\"\ncisco.nxos.nxos_config:\nlines: boot nxos bootflash:{{image_file}}\nsave_when: modified\n- name: \"reload switch\"\ncisco.nxos.nxos_command:\ncommands:\n- command: reload\nprompt: '(y/n)?'\nanswer: 'y'\nwhen: ansible_net_version != standard\nrescue:\n- debug:\nmsg: \"{{ansible_net_hostname}} is reloading\"\n- name: Wait For Device To Come Back Up\nwait_for:\nport: 22\nstate: started\ntimeout: 900\ndelay: 60\nhost: \"{{ inventory_hostname }}\"\nalways:\n- debug:\nmsg: \"All devices are running {{standard}}\"\n</code></pre> <ul> <li>On the Ansible server, run the playbook for software compliance check and code upgrade by issuing the <code>ansible-playbook code_upgrade.yml</code> command as shown below: <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook code_upgrade.yml\n</code></pre></li> </ul> <p>Note: It is expected to see timeout error message when playbook reloads the switch.</p> <p>Below screenshot shows the output of above command:</p> <p></p> <p>Switch will take 20 mins to bootup; up to this point, you have completed all tasks.* </p>"},{"location":"appendixB-Upgrade/#congratulations-you-have-completed-the-whole-lab-including-the-optional-appendix-sections-well-done","title":"Congratulations! You have completed the whole lab including the Optional (Appendix) sections. Well done!","text":""},{"location":"intro/","title":"Introduction","text":""},{"location":"intro/#vxlan","title":"VXLAN","text":"<p>VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments. VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. Below are some of the terminologies that will be used in the lab:</p> <ul> <li>VNI / VNID \u2013 VXLAN Network Identifier. This replaces VLAN ID</li> <li>VTEP \u2013 VXLAN Tunnel End Point.<ul> <li>This is the end point where the box performs VXLAN encap / decap This could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)</li> </ul> </li> <li>VXLAN Segment -  The resulting layer 2 overlay network</li> <li>VXLAN Gateway \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding</li> <li>NVE \u2013 Network Virtualization Edge<ul> <li>NVE is tunnel interface. It represents VTEP</li> </ul> </li> </ul>"},{"location":"intro/#ansible","title":"Ansible","text":"<p>Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module.</p> <p>You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html Below are some of the terminologies that will be used in the lab:</p> <ul> <li>Host: remote machines that Ansible manages  </li> <li>Group: several hosts that can be configured together and share common verables</li> <li>Inventory: file descripts hosts and groups in Ansible.</li> <li>Variable: names of value (int, str, dic, list) referenced in playbook or template</li> <li>YAML: data format for Playbook or Variables in Ansible</li> <li>Playbook: the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays.</li> <li>Roles: group of tasks, templates to implement specific behavior</li> <li>Jinja2: a Python based templating language</li> </ul> <p></p>"},{"location":"intro/#ci-pipeline-and-gitlab","title":"CI Pipeline and GitLab","text":"<p>CI or continuous integration is core principle in DevOps practices. Software developers keep code in central repository and make changes multiples times a day. Every change in CI triggers automated build and test for the project. In NetDevOps, validation and test are even more important; and configuration change or adding new service needs to be validate and tested in staging or lab enviroment.</p> <p>GitLab is an opensource DevOps platform for software development. GitLab provides the complete solution including configure, monitor, verify, package etc. For this lab exercise, you will use the verison control and CI pipeline functions from GitLab.  </p>"},{"location":"intro/#about-this-lab","title":"About this lab","text":"<p>As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode. As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. This lab demonstrates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations.</p>"},{"location":"intro/#lab-flow","title":"Lab Flow","text":"<p>Lab guide will walk the attendees through the below activities:</p> <ol> <li>Installation of Ansible on a server</li> <li>Use of Ansible playbooks</li> <li>Day 1 automation using Ansible</li> <li>CI Pipeline and Day 2 automation using Ansible</li> </ol>"},{"location":"intro/#lab-access","title":"Lab Access","text":"<p>Below table provides the IP addresses and credentials for the devices used in this lab:</p> Device SSH or Console (C) Credentials Spine-1 C: 198.18.133.33:1030 .... SSH: 198.18.4.201 admin/C1sco12345 Spine-2 C: 198.18.133.33:1040 .... SSH: 198.18.4.202 admin/C1sco12345 Leaf-1 C: 198.18.133.33:1050 .... SSH: 198.18.4.101 admin/C1sco12345 Leaf-3 C: 198.18.133.33:1070 .... SSH: 198.18.4.103 admin/C1sco12345 Leaf-4 C: 198.18.1333.33:1080 ... SSH:  198.18.4.104 admin/C1sco12345 Server-1 SSH: 198.18.134.50 root/C1sco12345 Server-3 SSH: 198.18.134.52 root/C1sco12345 Server-4 SSH: 198.18.134.53 root/C1sco12345 Ansible Server 198.18.134.150 root/C1sco12345 Remote Workstation 198.18.133.36 demouser/C1sco12345"},{"location":"intro/#lab-topology","title":"Lab topology","text":"<p>Below picture shows the lab topology:</p> <p></p>"},{"location":"missing_file_leaf/","title":"Steps for missing <code>leaf.j2</code> file","text":"<ol> <li>On MTputty, change Directory to folder EVPN-Ansible on Ansible server (198.18.134.150) using below command:</li> </ol> <pre><code>cd ~/EVPN-Ansible\n</code></pre> <ol> <li>further, change Directory (cd) to folder roles/jinja2_leaf/templates using below command:</li> </ol> <pre><code>cd roles/jinja2_leaf/templates\n</code></pre> <ol> <li>Type <code>touch leaf.j2</code></li> </ol> <pre><code>touch leaf.j2\n</code></pre> <ol> <li>After entering the command, go back to ATOM,  right click on folder <code>EVPN-Ansible</code>, scroll to choose option <code>Remote Sync</code> option and choose <code>Download Folder</code></li> </ol> <p>Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps in your lab task</p>"},{"location":"missing_file_spine/","title":"Steps for missing <code>spine.j2</code> file","text":"<p>NOTE: If the file does not appear on the Atom, then go ahead and execute below 4 steps to get it sync.  If the <code>spine.j2</code> file appears in above folder then you can skip below 4 steps and proceed with your lab steps:</p> <ol> <li>On Ansible server (198.18.134.150) using your SSH session, change Directory to folder EVPN-Ansible using below command:</li> </ol> <pre><code>cd ~/EVPN-Ansible/\n</code></pre> <ol> <li>further, change Directory (cd) to folder roles/jinja2_spine/templates using below command:</li> </ol> <pre><code>cd roles/jinja2_spine/templates/\n</code></pre> <ol> <li>Type <code>touch spine.j2</code> as shown below:</li> </ol> <pre><code>touch spine.j2\n</code></pre> <ol> <li>After entering the command, go back to ATOM,  right click on folder <code>EVPN-Ansible</code>, scroll to choose option <code>Remote Sync</code> option and choose <code>Download Folder</code> as shown below:</li> </ol> <p></p> <p>Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps on your lab guide</p>"},{"location":"task2-first-ansible/","title":"Task 2 - First Simple Ansible Playbook","text":"<p>In this section, your will create the first Ansible Playbook for this lab.  This Ansible Playbook will be used to configure VLANs on leaf switches, and to assign VLANs to the server facing port.  Further, you will create and learn about:</p> <ul> <li>using variables inside playbook,</li> <li>loop within Ansible by using \u201cwith_items\u201d,</li> <li>logic of using \u201cwhen\u201d and \u201ctags\u201d to isolate tasks from whole playbook</li> </ul>"},{"location":"task2-first-ansible/#step-1-using-atom-text-editor","title":"Step 1: Using \"Atom\" text Editor","text":"<ul> <li> <p>Open \u201cAtom\u201d text editor by double click the icon on desktop.  Atom is the recommended text editor for this lab:</p> <p></p> </li> <li> <p>After opening Atom, you may see a \u201cRegister as default atom:// URI handler\u201d message as show in below screenshot.  If this message appears then Click the <code>\"No, Never\"</code> button, else proceed further:</p> <p></p> </li> </ul>"},{"location":"task2-first-ansible/#step-2-lab-folder-on-atom","title":"Step 2: Lab folder on Atom","text":"<p>Atom provides a Remote sync package that allows to read and push contents to a remote server.  In this case, Atom has been preconfigured to integrate with Ansible node.  As per this integration, Atom Secure copy (SCP) any new files or changes to the Ansible node upon saving.  And Atom also downloads the content from Ansible node as per instructions in this lab.  Atom displays the EVPN-Ansible folder that has been downloaded from the ansible server.</p> <ul> <li> <p>After opening Atom, there should be a folder in the left pane named: <code>EVPN-Ansible</code></p> </li> <li> <p>Right click on this pre-configured project folder <code>EVPN-Ansible</code> and select <code>New File</code> as shown below:</p> <p></p> </li> <li> <p>Name the new file <code>vlan_provision.yml</code> and hit Enter.  This will create the new file:</p> <p></p> </li> <li> <p>Also, on the lower bar right of the ATOM, verify that Language Mode (grammar) of YAML is selected instead of default <code>\"Plain Text\"</code>.  If <code>\"YAML\"</code> is not selected, then you should Select YAML from the listed options as shown in below screenshot:</p> <p></p> </li> </ul>"},{"location":"task2-first-ansible/#step-3-define-variables-tasks-for-playbook","title":"Step 3: Define variables, tasks for playbook","text":"<p>In this step, we are going to create a playbook and define the variable &amp; tasks in the playbook</p> <ul> <li>In the Atom application, under the <code>vlan_provision.yml</code> file, enter the below content:</li> </ul> <p>Note</p> <p>YAML is space sensitive. Hence be careful with the spaces in below section.  You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly.</p> <pre><code>---                     #Task 2: Simple playbook assign VLAN to server facing port\n- hosts: leaf:jinja2_leaf\n</code></pre> <p>Note</p> <ul> <li>\u201chosts:\u201d defines the scope of this playbook applies to all switches in group <code>\u2018leaf\u2019</code> and <code>\u2018jinja2_leaf\u2019</code> (these were added within the \"hosts\" file created in pervious task)</li> <li>You can review the IP addresses of the two \u201cleaf\u201d switches and one \u201cjinja2_leaf\u201d switch in the \u201chosts\u201d file (configured in previous steps).  For reference, the IP addresses are:<ul> <li>jinja2_leaf: 198.18.4.104</li> <li>leaf: 198.18.4.101</li> <li>leaf: 198.18.4.103</li> </ul> </li> </ul>"},{"location":"task2-first-ansible/#step-4-vlan-tasks-in-playbook","title":"Step 4: VLAN tasks in playbook","text":"<ul> <li>In the Atom, in the same <code>vlan_provision.yml</code> add below tasks below the previously added content:</li> </ul> <p>Note</p> <p>YAML is space sensitive. Hence be careful with the spaces in below section.  You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly.</p> <pre><code>    tasks:\n- name: provision VLAN\ncisco.nxos.nxos_vlans:\nconfig:\n- vlan_id: \"{{item}}\"\nstate: active\nwith_items:\n-  140\n-  141\ntags: add vlans\n</code></pre> <p>Note</p> <ul> <li>This task creates multiple VLANs using cisco.nxos.nxos_vlans module</li> <li>Above only one task is added but multiple tasks can be defined in one playbook under \u201ctasks\u201d, each starts with <code>\u201c-\u201c</code></li> <li>At the end of this play, we use \u201ctags\u201d to name the task \u201cadd vlans\u201d. This is useful to run a specific part of the configuration without running the complete playbook.</li> </ul> <ul> <li> <p>Below screenshot shows how playbook will look:</p> <p></p> </li> </ul> <p>Note</p> <p>Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct.</p> <ul> <li> <p>Click <code>File</code> and <code>Save</code> .  This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> <p></p> </li> </ul> <p>Note</p> <p>Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansible host (198.18.134.150) and saving the vlan_provision.yml file.</p>"},{"location":"task2-first-ansible/#step-5-execute-playbook","title":"Step 5: Execute playbook","text":"<p>After creating the playbook, it is now time to execute the playbook.  Before executing the playbook, we will verify the leaf switch that desired vlan configurations are not present</p> <ul> <li>Using the MTPuTTy client, Login (SSH) to <code>leaf-3</code> switch (or any leaf switch as per hosts: variable in the <code>vlan_provision.yml</code> file).  Then execute below command:</li> </ul> <pre><code>show vlan brief\n</code></pre> <p>This command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:</p> <p></p> <ul> <li> <p>Now, from the MTPuTTy, launch a new ssh into Ansible node (198.18.134.150)</p> </li> <li> <p>Execute the ansible playbook using below commands under directory <code>EVPN-Ansible</code> :</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook vlan_provision.yml --tags \"add vlans\"\n</code></pre> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message. The below screenshot shows the execution of the playbook:</p> <p></p> <p>Note</p> <p>If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.</p> <ul> <li>After playbook is run successfully, go back to MTPuTTy and login via SSH into <code>leaf 3</code> again (or the other switch that you logged in earlier in this step) and check if vlan 140 and vlan 141 appears by executing below command again:</li> </ul> <pre><code>show vlan brief\n</code></pre> <p>Below screenshot shows the execution of above command in the switch:</p> <p></p>"},{"location":"task2-first-ansible/#step-6-server-port-vlan-tasks-in-playbook","title":"Step 6: Server port VLAN tasks in playbook","text":"<p>We have just tested our first playbook with basic configuration (i.e, by adding 2 VLANs).  Now we are going to add more tasks in the same/existing playbook <code>vlan_provision.yml</code> in this step:</p> <ul> <li> <p>Lets add new tasks in the Ansible playbook to assign VLANs to server facing port.  We will configure VLAN towards the server facing ports</p> </li> <li> <p>Go back to ATOM and add the following tasks to the existing playbook</p> </li> </ul> <p>Note</p> <p>YAML is space sensitive. Hence be careful with the spaces in below section.  You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly.</p> <pre><code>      - name: configure server facing port to L2\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: eth1/3\nmode: layer2\n- name: configure VLAN for server port\nwhen: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname)\ncisco.nxos.nxos_l2_interfaces:\nconfig:\n- name: eth1/3\naccess:\nvlan: 140\nstate: overridden\n- name: configure VLAN for server port\nwhen: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname)\ncisco.nxos.nxos_l2_interfaces:\nconfig:\n- name: eth1/3\naccess:\nvlan: 141\nstate: overridden\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> on ATOM.  This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</li> </ul> <p>In this new play, we used nxos module \u201ccisco.nxos.nxos_interfaces\u201d and \u201ccisco.nxos.nxos_l2_interfaces\u201d:</p> <ul> <li>\u201ccisco.nxos.nxos_interfaces\u201d provides the capability to manage the physical attributes of an interface.  In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3</li> <li>\u201ccisco.nxos.nxos_l2_interfaces\u201d provides the capability to manage the Layer 2 switchport attributes.  In this example, it is used to configure access mode on Ethernet ports 1/3</li> <li>We used \u201cwhen\u201d argument to provide little logic of looking at the IP addresses (as per hosts i.e., inventory file) to the play and then appropriately assign VLANs to those hosts.  In our example, the playbook assigned:<ul> <li>VLAN 140 on leaf-1 and leaf-3 switches (by matching <code>101</code> and <code>103</code> in the <code>hosts</code> file)</li> <li>VLAN 141 on leaf-4 switch (by matching <code>104</code> in the <code>hosts</code> file)</li> </ul> </li> </ul>"},{"location":"task2-first-ansible/#step-7-execute-playbook","title":"Step 7: Execute playbook","text":"<p>Now, we are going to execute this playbook:</p> <ul> <li>Before executing the ansible playbook, log into switch (leaf1, leaf3 or leaf4) using MTPuTTy client.  And check the existing configuration by executing the below command.  In this lab, the default configuration of Leaf-1 and Leaf-3 Ethernet 1/3 ports are configured as trunks.  While Ethernet 1/3 port of Leaf-4 has \"no switchport\" configured.</li> </ul> <pre><code>show run interface ethernet1/3\n</code></pre> <ul> <li> <p>On MTPuTTy, log back into (or launch a new ssh) into \u201cAnsible\u201d node</p> </li> <li> <p>Execute below command to run the Ansible playbook in the <code>EVPN-Ansible</code> directory:</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook vlan_provision.yml\n</code></pre> <p>Below screeenshot shows the execution of above playbook:</p> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message. </p> <ul> <li>After we push the configuration, login to the leaf-1 or leaf-3 switch, and confirm if the server facing port has the access vlan 140 configured with the below command:</li> </ul> <pre><code>show run interface ethernet1/3\n</code></pre> <p>The output of above command on leaf-3 is shown in below screenshot:</p> <p></p> <p>The output of above command on leaf-4 confirms that vlan 141 has been configured as shown in below screenshot:</p> <p></p> <p>Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.</p>"},{"location":"task3-vxlan-jinja2-new/","title":"Task 3 - Use of Jinja2 templates with Ansible Playbook","text":"<p>In this section, we will use Jinja2 templating engine to create templates for spine and leaf switches.  Use of Jinja template provide flexibility and agility.  Jinja2 template will appear similar to the NXOS configurations. We abstract the variables out of the configuration and use simple <code>for</code> loop to feed variables into the template.  This shows the power of using Jinja2 templating engine.</p> <p>In this task, you will configure VXLAN fabric using this Jinja2 templates for one leaf (leaf-4) and one Spine (spine2) switch.</p> <p></p>"},{"location":"task3-vxlan-jinja2-new/#step-1-install-jinja2","title":"Step 1: Install jinja2","text":"<ul> <li>On the Ansible node, install jinja2 using <code>pip install jinja2</code> command below.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d:</li> </ul> <pre><code>pip install jinja2\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p>"},{"location":"task3-vxlan-jinja2-new/#step-2-playbook-for-jinja2-spine","title":"Step 2: Playbook for jinja2 Spine","text":"<p>In this step, we will use Jina2 template and Ansible to provision the VXLAN Fabric.</p> <ul> <li> <p>Switch to Atom, then Right Click on the folder <code>EVPN-Ansible</code> and Click <code>New File</code> to create a new playbook named <code>jinja2_fabric.yml</code> as shown below</p> <p></p> </li> <li> <p>Name the new file <code>jinja2_fabric.yml</code> and hit enter as shown below. It will create this new file:</p> </li> </ul> <p></p> <ul> <li> <p>Also, on the lower bar of the Atom, verify that the language/grammar of YAML is selected instead of default \"Plain Text\".  If YAML is not selected, then you should choose it from the listed options.</p> </li> <li> <p>Now enter (copy and paste) below data in this playbook:</p> </li> </ul> <pre><code>---\n- hosts: jinja2_spine\nconnection: local\nroles:\n- jinja2_spine\n</code></pre> <p>The contents of the jinja2_fabric.yml file should look like below screenshot:</p> <p></p> <ul> <li> <p>On Atom, you should Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</p> </li> <li> <p>On the MTPuTTy, go back to the ssh session of the Ansible Server node (198.18.134.150)</p> </li> <li> <p>On Ansible server node (198.18.134.150), verify that the (below highlighted) 2 groups named  <code>jinja2_spine</code> and <code>jinja2_leaf</code> exists in the inventory file name \"hosts\" (under EVPN-Ansible directory) exists by issuing below commands:</p> </li> </ul> <pre><code>cd /root/EVPN-Ansible\nmore hosts\n</code></pre> <p>Below screenshot confirms the ouptut and appropriate IP addresses of above command:</p> <p></p>"},{"location":"task3-vxlan-jinja2-new/#step-3-create-new-roles-and-vars","title":"Step 3: Create new roles and vars","text":"<p>In this section, we will create two new roles for provisioning Fabric with Jina2 template.</p> <ul> <li>On the MTPuTTy, go back to Ansible Server node (198.18.134.150).  Switch to \u2018roles\u2019 directory and then create <code>\u2018jinja2_spine\u2019</code> and <code>\u2018jinja2_leaf\u2019</code> roles using <code>ansible-galaxy init</code> as per below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/\ncd roles/\nansible-galaxy init jinja2_spine &amp;&amp; ansible-galaxy init jinja2_leaf\n</code></pre> <ul> <li> <p>Below screenshot shows the output of above command:</p> <p></p> </li> </ul> <p>Note</p> <p>\u2018ansible-galaxy\u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.</p> <ul> <li>change directory path to <code>EVPN-Ansible/roles/jinja2_spine</code> and check the content of local directory (<code>ls</code>) as per below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/roles/jinja2_spine/\nls\n</code></pre> <ul> <li> <p>Below screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps</p> <p></p> </li> </ul> <p>Next:</p> <ul> <li>Create empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/roles\ntouch jinja2_spine/templates/spine.j2\ntouch jinja2_leaf/templates/leaf.j2\n</code></pre> <ul> <li>Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder <code>EVPN-Ansible</code>, then Click <code>Remote Sync</code>, and Select <code>Download Folder</code> as shown in below screenshot:</li> </ul> <p></p> <ul> <li>Once the download is complete, you should see the new folder (such as <code>roles</code>) and all files appear on Atom as well.</li> </ul>"},{"location":"task3-vxlan-jinja2-new/#step-4-create-variable-file-for-jina2_spine-role","title":"Step 4: Create variable file for \u201cjina2_spine\u201d role","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit this <code>main.yml</code> file to include the variables that will be used in jinja2 template.</p> <ul> <li> <p>Switch to Atom, then open up the project folder <code>EVPN-Ansible</code> from the left pane and Open <code>main.yml</code> file under \u201croles/jinja2_spine/vars/\u201d as shown below:</p> <p></p> </li> <li> <p>use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file to include the below variables that will be used in jinja2 template.  You can copy and paste all of the below to replace any existing content into <code>main.yml</code> file.</p> </li> </ul> <p>Note</p> <p>As per steps in previous tasks, be careful with content since its space sensitive.</p> <pre><code>---\n# vars file for jinja2_spine\nasn: 65000\nbgp_neighbors:\n- remote_as: 65000\nneighbor: 192.168.0.8\nupdate_source: Loopback0\n- remote_as: 65000\nneighbor: 192.168.0.10\nupdate_source: Loopback0\n- remote_as: 65000\nneighbor: 192.168.0.11\nupdate_source: Loopback0\nL3_interfaces:\n-  interface: Ethernet 1/1\n-  interface: Ethernet 1/2\n-  interface: Ethernet 1/3\n-  interface: Ethernet 1/4\n-  interface: loopback 0\n-  interface: loopback 1\ns1_loopback: 192.168.0.6\ns2_loopback: 192.168.0.7\n</code></pre> <ul> <li> <p>Contents of the \u2018main.yml\u2019 file should look like below screenshot:</p> <p></p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> </ul>"},{"location":"task3-vxlan-jinja2-new/#step-5-create-jinja2-template-for-spine-role","title":"Step 5: Create Jinja2 template for spine role","text":"<ul> <li>On Atom, Select the project folder <code>EVPN-Ansible</code> from the left pane.  Then under \u201croles/jinja2_spine/templates\u201d,  Open <code>spine.j2</code> file as shown in below screenshot:</li> </ul> <p>Note</p> <p>If the file does not appear on the ATOM, then go ahead and then execute steps outlined on this page to get it sync.  If the <code>spine.j2</code> file appears in above folder then can proceed below**</p> <ul> <li>use \u201cAtom\u201d to edit the \u201cspine.j2\u201d file for jinja2 template.  You can copy and paste all of the below content into <code>spine.j2</code> file.  </li> </ul> <p>Note</p> <p>As per steps in previous tasks, be careful with content since its space sensitive.</p> <pre><code>feature bgp\nfeature nv overlay\nfeature vn-segment-vlan-based\nnv overlay evpn\nfeature pim\n!\nrouter bgp {{ asn }}\nrouter-id {{ router_id }}\naddress-family ipv4 unicast\naddress-family l2vpn evpn\nretain route-target all\n!\n#for loop to configure bgp neighbor for each leaf\n{% for neighbor in bgp_neighbors %}\nneighbor {{ neighbor['neighbor'] }}\nremote-as {{neighbor['remote_as']}}\nupdate-source {{neighbor['update_source']}}\naddress-family ipv4 unicast\nsend-community both\nroute-reflector-client\naddress-family l2vpn evpn\nsend-community both\nroute-reflector-client\n!\n{% endfor %}\ninterface loopback 1\nip address {{loopback1}}/32\nip pim sparse-mode\nip router ospf 1 area 0.0.0.0\n\n!\nip pim rp-address {{loopback1}}\nip pim anycast-rp {{loopback1}} {{s1_loopback}}\nip pim anycast-rp {{loopback1}} {{s2_loopback}}\n!\n#for loop to enable pim on link to each leaf\n{% for interface in L3_interfaces %}\ninterface {{interface['interface']}}\nip pim sparse-mode\n!\n{% endfor %}\n</code></pre> <ul> <li> <p>Next on Atom, Click on <code>File</code> and then <code>Save</code> to push template file to Ansible node.</p> </li> <li> <p>You can verify that updated file content is on Ansible server (198.18.134.150) using your SSH session by issuing below commands:</p> </li> </ul> <pre><code>cd /root/EVPN-Ansible/roles/jinja2_spine/templates\nmore spine.j2\n</code></pre> <p>Partial output of above command is shown in below screenshot confirming the content:</p> <p></p>"},{"location":"task3-vxlan-jinja2-new/#step-6-create-playbook-for-jinja2_spine-role","title":"Step 6: Create playbook for jinja2_spine role","text":"<p>The playbook for <code>jinja2_spine</code> roles has two tasks.</p> <ul> <li>First play in the task uses ansible \"template\" module to generate configuration file based on jinja2 template created in last step.  The configuration file is saved in \u201cfile\u201d folder.</li> <li>Second play in the task pushes the configuration to switch by using ansible \"cisco.nxos.nxos_config\" module.</li> </ul> <p>Note</p> <p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit the main.yml file.</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code>.  Then open <code>main.yml</code> file under <code>roles/jinja2_spine/tasks/</code> to include below content:</li> </ul> <pre><code>---\n# tasks file for jinja2_spine\n- name: Generate Spine Config\ntemplate: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n- name: Push Spine Config\ncisco.nxos.nxos_config:\nsrc: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\nmatch: none\n</code></pre> <p>Contents of the \u2018main.yml\u2019 file should look like below:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</li> </ul> <p>In the above <code>tasks/main.yml</code> file, ansible module named \u201ccisco.nxos.nxos_config\u201d is used.  This module performs below activities:</p> <ul> <li>It uses source path of the file (\u201csrc\u201d) that contains the configuration or configuration template to load into spine.</li> <li>Since \u201cmatch\u201d option is set to none, hence the module will not attempt to compare the source configuration with the running configuration on the remote device.</li> </ul>"},{"location":"task3-vxlan-jinja2-new/#step-7-run-jinja2_fabric-playbook","title":"Step 7: Run Jinja2_fabric playbook","text":"<p>In this section you will run the playbook created in step 2 (in this task 3).  The execution of this playbook will generate configuration file for Spine-2 switch from the template.  Further, The playbook will also push the configuration file to Spine-2 switches.</p> <ul> <li>Run the ansible playbook by going to folder EVPN-Ansible and executing the below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/\nansible-playbook jinja2_fabric.yml\n</code></pre> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message.</p> <p>Note</p> <p>It will take few minutes to push configuration</p> <p>Below screenshot shows the execution of above playbook:</p> <p></p> <p>To verify the execution of this playbook, you can:</p> <ul> <li> <p>Login/SSH to Spine-2 switch using MTPuTTy to verify that configuration has been pushed by double clicking the Spine-2 icon in the left pane on MTPuTTy.    If prompted, then login with credentials of <code>admin</code> and <code>C1sco12345</code></p> </li> <li> <p>Execute below command on Spine-2 switch to confirm the configurations have been provisioned:</p> </li> </ul> <pre><code>show run bgp\n</code></pre> <p>The execution of this command is captured in below screenshot:</p> <p></p>"},{"location":"task3-vxlan-jinja2-new/#step-8-modify-playbook-for-leaf","title":"Step 8: Modify playbook for Leaf","text":"<p>In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4.   We are going to add jinja2_leaf this time in the hosts list to the already created playbook in step 2.</p> <ul> <li> <p>Switch to \u201cAtom\u201d, then click on the folder <code>EVPN-Ansible</code>, select the existing playbook <code>jinja2_fabric.yml</code> file for a role for leaf (named jinja2_leaf)</p> </li> <li> <p>Add (i.e., Copy and Paste) the below content at the end of existing file i.e., add the below content to existing content in this file.  </p> </li> </ul> <p>Note</p> <p>Do not overwrite existing content of this file.  You must add below content to this file.</p> <pre><code>- hosts: jinja2_leaf\nconnection: local\nroles:\n- jinja2_leaf\n</code></pre> <p>Below screenshot shows the contents of jinja2_fabric.yml file in Atom after adding the above configs:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</li> </ul>"},{"location":"task3-vxlan-jinja2-new/#step-9-variable-file-for-jinja2_leaf-role","title":"Step 9: Variable file for jinja2_leaf role","text":"<ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/jinja2_leaf/vars/</code> to include following:</li> </ul> <pre><code>---\n# vars file for jinja2_leaf\nasn: 65000\nbgp_neighbors:\n-  remote_as: 65000\nneighbor: 192.168.0.6\nupdate_source: Loopback0\n-  remote_as: 65000\nneighbor: 192.168.0.7\nupdate_source: Loopback0\nrp_address: 192.168.0.100\nL3_interfaces:\n-  interface: Ethernet 1/1\n-  interface: Ethernet 1/2\n-  interface: loopback 0\n-  interface: loopback 1\nL2VNI:\n-  vlan_id: 140\nvni: 50140\nip_add: 172.21.140.1\nmask: 24\nvlan_name: L2-VNI-140-Tenant1\nmcast: 239.0.0.140\n-  vlan_id: 141\nvni: 50141\nip_add: 172.21.141.1\nmask: 24\nvlan_name: L2-VNI-141-Tenant1\nmcast: 239.0.0.141\nL3VNI:\n-  vlan_id: 999\nvlan_name: L3-VNI-999-Tenant1\nvni: 50999\n</code></pre> <p>Below screenshot shows the contents of <code>roles/jinja2_leaf/vars/main.yml</code> file in Atom:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task3-vxlan-jinja2-new/#step-10-jinja2-template-for-leaf-role","title":"Step 10: Jinja2 template for leaf role","text":"<ul> <li>On Atom, you can Select the project folder <code>EVPN-Ansible</code> and open <code>leaf.j2</code> file under <code>roles/jinja2_leaf/templates/</code></li> </ul> <p>Note</p> <p>If the file does not appear on the ATOM, then go ahead and execute the steps on this page to get it sync.  If the <code>leaf.j2</code> file appears in above folder then you can proceed with below steps further.</p> <ul> <li>On \u201cAtom\u201d, edit this <code>leaf.j2</code> file for jinja2 template by copy and paste all of the below content in it.  !!! Note     As per steps in previous tasks, be careful with content since its space sensitive.</li> </ul> <pre><code>feature bgp\nfeature nv overlay\nfeature vn-segment-vlan-based\nnv overlay evpn\nfeature pim\n!\nip pim rp-address {{rp_address}}\nspanning-tree vlan 1,140,141,999 priority 4096\n{% for L2VNI in L2VNI %}\nvlan {{L2VNI['vlan_id']}}\nname {{L2VNI['vlan_name']}}\nvn-segment {{L2VNI['vni']}}\n!\n{% endfor %}\n\n{% for L3VNI in L3VNI %}\nvlan {{L3VNI['vlan_id']}}\nvn-segment {{L3VNI['vni']}}\nvrf context Tenant-1\nvni {{L3VNI['vni']}}\nrd auto\naddress-family ipv4 unicast\nroute-target both auto\nroute-target both auto evpn\n!\n{% endfor %}\n\nfabric forwarding anycast-gateway-mac 0000.2222.3333\n!\n#for loop to configure SVI\n{% for L2VNI in L2VNI %}\ninterface Vlan{{L2VNI['vlan_id']}}\nno shutdown\nvrf member Tenant-1\nno ip redirects\nip address {{L2VNI['ip_add']}}/{{L2VNI['mask']}}\nfabric forwarding mode anycast-gateway\n!\n{% endfor %}\n{% for L3VNI in L3VNI %}\ninterface vlan{{L3VNI['vlan_id']}}\nno shutdown\nvrf member Tenant-1\nip forward\n!\n{% endfor %}\n#for loop to enable PIM on L3 interface\n{% for interface in L3_interfaces %}\ninterface {{interface['interface']}}\nip pim sparse-mode\n!\n{% endfor %}\n\ninterface nve1\nno shutdown\nsource-interface loopback1\nhost-reachability protocol bgp\n{% for L2VNI in L2VNI %}\nmember vni {{L2VNI['vni']}}\nmcast-group {{L2VNI['mcast']}}\n{% endfor %}\n{% for L3VNI in L3VNI %}\nmember vni {{L3VNI['vni']}} associate-vrf\n!\n{% endfor %}\n\nrouter bgp {{ asn }}\nrouter-id {{ router_id }}\naddress-family ipv4 unicast\naddress-family l2vpn evpn\nretain route-target all\n#for loop to configure bgp neighbor with spine\n{% for neighbor in bgp_neighbors %}\nneighbor {{neighbor['neighbor']}}\nremote-as {{neighbor['remote_as']}}\nupdate-source {{neighbor['update_source']}}\naddress-family ipv4 unicast\nsend-community both\naddress-family l2vpn evpn\nsend-community both\n!\n{% endfor %}\nevpn\n{% for L2VNI in L2VNI %}\nvni {{L2VNI['vni']}} l2\nrd auto\nroute-target import auto\nroute-target export auto\n{% endfor %}\n</code></pre> <ul> <li>Next on Atom, you can Select <code>File</code> and <code>Save</code> to push template file to Ansible node.</li> </ul>"},{"location":"task3-vxlan-jinja2-new/#step-11-create-playbook-for-jinja2_leaf-role","title":"Step 11: Create playbook for jinja2_leaf role","text":"<p>The playbook for jinja2_leaf roles has two tasks:</p> <ol> <li>First task uses ansible \"template\" module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in <code>files</code> directory.</li> <li>Second task is to push the configuration using ansible \"cisco.nxos.nxos_config\" module to switch.</li> </ol> <p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit this main.yml file.</p> <ul> <li>On Atom, you can Select project folder <code>EVPN-Ansible</code> and Click to edit \u201c<code>main.yml</code>\u201d file under \u201c<code>roles/jinja2_leaf/tasks/</code>\u201d and include following content:</li> </ul> <pre><code>---\n# tasks file for jinja2_leaf\n- name: Generate Leaf Config\ntemplate: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n- name: Push Leaf Config\ncisco.nxos.nxos_config:\nsrc: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\nmatch: none\n</code></pre> <p>Below screenshot shows how the contents of <code>jinja2_leaf/taks/main.yml</code> file looks like in Atom:</p> <p></p>"},{"location":"task3-vxlan-jinja2-new/#step-12-run-jinja2_fabric-playbook","title":"Step 12: Run Jinja2_fabric playbook","text":"<p>In this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches.</p> <ul> <li>Before running the ansible-playbook, on the MTPuTTy you can login/SSH into the leaf-4, and verify that no bgp configurations exist by running below command:</li> </ul> <pre><code>show running bgp\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), run the below command (<code>ansible-playbook jinja2_fabric.yml</code>) to execute the playbook:</li> </ul> <pre><code>ansible-playbook jinja2_fabric.yml\n</code></pre> <p>Note</p> <p>It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background.</p> <p>Below screenshot shows the execution of above command:</p> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message.</p> <p></p> <ul> <li>After the configuration push is successful, login/SSH (on MTpuTT SSH session) to leaf-4 switch to verify configuration has been pushed by running below command:</li> </ul> <pre><code>show running-config bgp\n</code></pre> <p>The output of above command is shown below:</p> <p></p> <p>Congratulations: you have successfully concluded this task of using jinja2 templates with Ansible for Cisco Nexus switches</p>"},{"location":"task3-vxlan-jinja2/","title":"Task3 vxlan jinja2","text":"<p>In this task, we are going to install Jinja2 - that provides templating option.  In this section, we use Jinja2 to create template for spine and leaf and configure VXLAN fabric using this Jinja2 templates.</p> <p></p> <p>Jinja2 template looks just like the NXOS configurations. We abstract the variables out of the configuration and use simple for loop to feed variables into the template.</p>"},{"location":"task3-vxlan-jinja2/#step-1-install-jinja2","title":"Step 1: Install jinja2","text":"<ul> <li>On the Ansible node, install jinja2 using <code>pip install jinja2</code> command.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d: <pre><code>        root@ubuntu:~# pip install jinja2\n</code></pre></li> </ul> <p>Below screenshot shows the output of above command:</p> <p></p>"},{"location":"task3-vxlan-jinja2/#step-2-playbook-for-jinja2-spine","title":"Step 2: Playbook for jinja2 Spine","text":"<p>In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric.</p> <ul> <li> <p>Switch to \u201cAtom\u201d, right click on the folder <code>EVPN-Ansible</code> and Click <code>New File</code> to create a new playbook named <code>jinja2_fabric.yml</code>.</p> <p></p> </li> <li> <p>Name the new file <code>jinja2_fabric.yml</code> and hit enter as shown below. It will create this new file:</p> </li> </ul> <p></p> <ul> <li> <p>Also, on the lower bar of the ATOM, verify that  file grammar of YAML is selected instead of default \"Plain Text\".  If YAML is not selected, then you should choose it from the listed options.</p> </li> <li> <p>Now enter below data in this playbook:</p> </li> </ul> <pre><code>---\n- hosts: jinja2_spine\nconnection: local\nroles:\n- jinja2_spine\n</code></pre> <ul> <li> <p>The contents of the jinja2_fabric.yml file should look like</p> <p></p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</p> </li> <li> <p>On the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150).</p> </li> <li> <p>On Ansible server node (198.18.134.150), verify that the (below highlighted) 2 groups exists of jinja2_spine and jinja2_leaf in the inventory file named \"hosts\" (under EVPN-Ansible directory) exists by issuing below commands:</p> </li> </ul> <pre><code>root@ubuntu:~# cd /root/EVPN-Ansible\nroot@ubuntu:~/EVPN-Ansible# more hosts\n</code></pre> <p>Below screenshot confirms the ouptut and appropriate IP addresses of above command:</p> <p></p>"},{"location":"task3-vxlan-jinja2/#step-3-create-new-roles-and-vars","title":"Step 3: Create new roles and vars","text":"<p>In this section, we will create two new roles for provisioning Fabric with Jina2 template.</p> <ul> <li> <p>On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018roles\u2019 directory; create \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands:</p> <pre><code>root@ubuntu:~# cd ~/EVPN-Ansible/\nroot@ubuntu:~/EVPN-Ansible# cd roles/\nroot@ubuntu:~/EVPN-Ansible/roles# ansible-galaxy init jinja2_spine&amp;&amp;ansible-galaxy init jinja2_leaf\n</code></pre> </li> <li> <p>Below screenshot shows the output of above command:</p> </li> </ul> <p></p> <p>Note: \u2018ansible-galaxy\u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.</p> <ul> <li> <p>change directory path to EVPN-Ansible/roles/jinja2_spine and check the content of local directory (<code>ls</code>) as per below commands:</p> <pre><code>root@ubuntu:~# cd ~/EVPN-Ansible/roles/jinja2_spine/\nroot@ubuntu:~/EVPN-Ansible/roles/jinja2_spine# ls\n</code></pre> </li> <li> <p>Below screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps.</p> <p></p> </li> </ul> <p>Next:</p> <ul> <li> <p>Create empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:</p> <pre><code>root@ubuntu:~/EVPN-Ansible/roles# cd ~/EVPN-Ansible/roles\nroot@ubuntu:~/EVPN-Ansible/roles# touch jinja2_spine/templates/spine.j2\nroot@ubuntu:~/EVPN-Ansible/roles# touch jinja2_leaf/templates/leaf.j2\n</code></pre> </li> <li> <p>Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder <code>EVPN-Ansible</code>, then open <code>Remote Sync</code> select <code>Download Folder</code> as shown below:</p> </li> </ul> <p></p> <ul> <li>Once the download is done you should see the new folder &amp; files (<code>roles</code>) appear on ATOM as well.</li> </ul>"},{"location":"task3-vxlan-jinja2/#step-4-create-variable-file-for-jina2_spine-role","title":"Step 4: Create variable file for \u201cjina2_spine\u201d role","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file to include the following variables that will be used in jinja2 template.</p> <ul> <li> <p>Switch to ATOM, then open up the project folder <code>EVPN-Ansible</code> from the left pane and open <code>main.yml</code> file under \u201croles/jinja2_spine/vars/\u201d as shown below:</p> <p></p> </li> <li> <p>use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file to include the following variables that will be used in jinja2 template.  You can copy and paste all of the below content into main.yml file.  Note: as per steps in previous tasks, be careful with YAML content since its space sensitive.</p> </li> </ul> <pre><code>---\n# vars file for jinja2_spine\nasn: 65000\nbgp_neighbors:\n-  remote_as: 65000\nneighbor: 192.168.0.8\nupdate_source: Loopback0\n-  remote_as: 65000\nneighbor: 192.168.0.10\nupdate_source: Loopback0\n-  remote_as: 65000\nneighbor: 192.168.0.11\nupdate_source: Loopback0\nL3_interfaces:\n-  interface: Ethernet 1/1\n-  interface: Ethernet 1/2\n-  interface: Ethernet 1/3\n-  interface: Ethernet 1/4\n-  interface: loopback 0\n-  interface: loopback 1\ns1_loopback: 192.168.0.6\ns2_loopback: 192.168.0.7\n</code></pre> <ul> <li> <p>Contents of the \u2018main.yml\u2019 file should look like below:</p> <p></p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> </ul>"},{"location":"task3-vxlan-jinja2/#step-5-create-jinja2-template-for-spine-role","title":"Step 5: Create Jinja2 template for spine role","text":"<ul> <li>On ATOM open up the project folder <code>EVPN-Ansible</code> from the left pane and open <code>spine.j2</code> file under \u201croles/jinja2_spine/templates\u201d as shown below:</li> </ul> <p>NOTE: If the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the <code>spine.j2</code> file appears in above folder then you can skip below 4 steps:</p> <ol> <li> <p>On Ansible server (198.18.134.150) using your SSH session, change Directory to folder EVPN-Ansible using below command:</p> <pre><code>root@ubuntu:~# cd ~/EVPN-Ansible/\n</code></pre> </li> <li> <p>further, change Directory (cd) to folder roles/jinja2_spine/templates using below command:</p> <pre><code>root@ubuntu:~/EVPN-Ansible# cd roles/jinja2_spine/templates/\n</code></pre> </li> <li> <p>Type <code>touch spine.j2</code> as shown below:</p> <pre><code>root@ubuntu:~/EVPN-Ansible/roles/jinja2_spine/templates# touch spine.j2\n</code></pre> </li> <li> <p>After entering the command, go back to ATOM,  right click on folder <code>EVPN-Ansible</code>, scroll to choose option <code>Remote Sync</code> option and choose <code>Download Folder</code> as shown below:</p> </li> </ol> <p></p> <p>Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps:</p> <ul> <li> <p>To reduce the typo, you can move (or copy) spine.j2 file from local playbook folder</p> <p><code>TFTP_Data (\\\\AD1) (X:)</code> --&gt;  <code>playbook</code> --&gt; <code>roles</code> --&gt; <code>jinja2_spine</code> --&gt; `templates``</p> </li> </ul> <p>and paste the file in the projects folder</p> <pre><code>`TFTP_Data (\\\\AD1) (X:)` --&gt;  `EVPN-Ansible` --&gt; `roles` --&gt; `jinja2_spine` --&gt; `templates`\n</code></pre> <p>as shown below:</p> <p></p> <ul> <li> <p>After moving/copying this file to above folder location, open this file on ATOM.  You do this by going to <code>File</code> then <code>Open File\u2026</code> on ATOM, and browse to this <code>spine.j2</code> file that was just saved in <code>X:\\EVPN-Ansible\\roles\\jinja2_spine\\templates</code> as shown below:     </p> </li> <li> <p>After opening \u201cspine.j2\u201d file from ATOM (as shown in below screenshot confirming the updated content), go to <code>File</code> \u2013-&gt; <code>Save</code> to push template file to Ansible node:</p> <p></p> </li> <li> <p>You can verify that updated file content is on Ansible server (198.18.134.150) using your SSH session by issuing below commands: <pre><code>root@ubuntu:~/EVPN-Ansible/roles/jinja2_spine/templates# cd /root/EVPN-Ansible/roles/jinja2_spine/templates\nroot@ubuntu:~/EVPN-Ansible/roles/jinja2_spine/templates# more spine.j2\n</code></pre></p> </li> </ul> <p>Partial output of above command is shown in below screenshot confirming :   </p>"},{"location":"task3-vxlan-jinja2/#step-6-create-playbook-for-jinja2_spine-role","title":"Step 6: Create playbook for jinja2_spine role","text":"<p>The playbook for jinja2_spine roles has two tasks. First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder. Second task is push the configuration to switch.</p> <p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit the main.yml file.</p> <ul> <li>On ATOM, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/jinja2_spine/tasks/</code> to include following:</li> </ul> <pre><code>---\n# tasks file for jinja2_spine\n- name: Generate Spine Config\ntemplate: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n- name: Push Spine Config\ncisco.nxos.nxos_config:\nsrc: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\nmatch: none\n</code></pre> <ul> <li> <p>Contents of the \u2018main.yml\u2019 file should look like below:</p> <p></p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> </ul> <p>NOTE: In the above YAML file, ansible module named \u201ccisco.nxos.nxos_config\u201d is used.  This module performs below activities:</p> <ul> <li>It uses source path of the file (\u201csrc\u201d) that contains the configuration or configuration template to load into spine</li> <li>Since \u201cmatch\u201d option is set to none (yes), hence the module will not attempt to compare the source configuration with the running configuration on the remote device.</li> </ul>"},{"location":"task3-vxlan-jinja2/#step-7-run-jinja2_fabric-playbook","title":"Step 7: Run Jinja2_fabric playbook","text":"<p>In this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switch from the template.</p> <p>The playbook will also push the configuration file to Spine-2 switches.</p> <ul> <li> <p>Run the ansible playbook by going to folder EVPN-Ansible and executing the below commands:</p> <pre><code>root@ubuntu:~/EVPN-Ansible/roles# cd ~/EVPN-Ansible/\nroot@ubuntu:~/EVPN-Ansible# ansible-playbook jinja2_fabric.yml\n</code></pre> <p>Note: It will take few minutes to push configuration</p> <p>Below screenshot shows the execution of above playbook:</p> <p></p> </li> </ul> <p>To verify the execution of this playbook, you can:</p> <ul> <li> <p>Login to Spine-2 switch (on MTputty) to verify configuration has been pushed by double clicking the spine-2 icon in the left pane on MTputty.  If prompted then login with credentials admin/C1sco12345</p> </li> <li> <p>Execute <code>show run bgp</code> command on the switch to confirm the configurations have been provisioned (as shown below):</p> <p></p> </li> </ul>"},{"location":"task3-vxlan-jinja2/#step-8-modify-playbook-for-leaf","title":"Step 8: Modify playbook for Leaf","text":"<p>In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4.   We are going to add jinja2_leaf this time to the already created playbook in step 2.</p> <ul> <li>Switch to \u201cAtom\u201d, click on the folder <code>EVPN-Ansible</code>, edit the existing playbook <code>jinja2_fabric.yml</code> file for a role for leaf (named jinja2_leaf).  Add the below content at the end of existing file (i.e., add the below content to existing content in this file (do not overwrite existing content):</li> </ul> <pre><code>- hosts: jinja2_leaf\nconnection: local\nroles:\n- jinja2_leaf\n</code></pre> <ul> <li>Below screenshot shows the contents of jinja2_fabric.yml file in Atom after adding the above configs:</li> </ul> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task3-vxlan-jinja2/#step-9-variable-file-for-jinja2_leaf-role","title":"Step 9: Variable file for jinja2_leaf role","text":"<ul> <li>On ATOM, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/jinja2_leaf/vars/</code> to include following:</li> </ul> <pre><code>---\n# vars file for jinja2_leaf\nasn: 65000\nbgp_neighbors:\n-  remote_as: 65000\nneighbor: 192.168.0.6\nupdate_source: Loopback0\n-  remote_as: 65000\nneighbor: 192.168.0.7\nupdate_source: Loopback0\nrp_address: 192.168.0.100\nL3_interfaces:\n-  interface: Ethernet 1/1\n-  interface: Ethernet 1/2\n-  interface: loopback 0\n-  interface: loopback 1\nL2VNI:\n-  vlan_id: 140\nvni: 50140\nip_add: 172.21.140.1\nmask: 24\nvlan_name: L2-VNI-140-Tenant1\nmcast: 239.0.0.140\n-  vlan_id: 141\nvni: 50141\nip_add: 172.21.141.1\nmask: 24\nvlan_name: L2-VNI-141-Tenant1\nmcast: 239.0.0.141\nL3VNI:\n-  vlan_id: 999\nvlan_name: L3-VNI-999-Tenant1\nvni: 50999\n</code></pre> <ul> <li> <p>Below screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom:</p> <p></p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> </ul>"},{"location":"task3-vxlan-jinja2/#step-10-jinja2-template-for-leaf-role","title":"Step 10: Jinja2 template for leaf role","text":"<ul> <li>On ATOM, open up the project folder <code>EVPN-Ansible</code> and open leaf.j2 file under \u201croles/jinja2_leaf/templates/\u201d</li> </ul> <p>NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the <code>leaf.j2</code> file appears in above folder then you can skip below 4 steps:</p> <ol> <li> <p>On MTputty, change Directory to folder EVPN-Ansible on Ansible server (198.18.134.150) using below command:</p> <p><code>cd ~/EVPN-Ansible</code></p> </li> <li> <p>further, change Directory (cd) to folder roles/jinja2_leaf/templates using below command:</p> <p><code>cd roles/jinja2_leaf/templates</code></p> </li> <li> <p>Type <code>touch leaf.j2</code></p> </li> <li>After entering the command, go back to ATOM,  right click on folder <code>EVPN-Ansible</code>, scroll to choose option <code>Remote Sync</code> option and choose <code>Download Folder</code></li> </ol> <p>Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps:</p> <ul> <li> <p>To reduce the typo, you can move (or copy) leaf.j2 file from local playbook folder</p> <p><code>TFTP_Data (\\\\AD1) (X:)</code> --&gt;  <code>playbook</code> --&gt; <code>roles</code> --&gt; <code>jinja2_leaf</code> --&gt; `templates``</p> </li> </ul> <p>and paste the file in the projects folder</p> <pre><code>`TFTP_Data (\\\\AD1) (X:)` --&gt;  `EVPN-Ansible` --&gt; `roles` --&gt; `jinja2_leaf` --&gt; `templates`\n</code></pre> <p>as shown below:</p> <p></p> <ul> <li> <p>On ATOM, go to <code>File</code> then <code>Open File\u2026</code> and browse to this <code>leaf.j2</code> that was just saved in <code>X:\\EVPN-Ansible\\roles\\jinja2_leaf\\templates</code> as shown below screenshot:</p> <p></p> </li> <li> <p>After opening \u201cleaf.j2\u201d file from ATOM, go to <code>File</code> \u2013 <code>Save</code> to push template file to Ansible node:</p> </li> </ul>"},{"location":"task3-vxlan-jinja2/#step-11-create-playbook-for-jinja2_leaf-role","title":"Step 11: Create playbook for jinja2_leaf role","text":"<p>The playbook for jinja2_leaf roles has two tasks.</p> <ol> <li>First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder.</li> <li>Second task is to push the configuration to switch.</li> </ol> <p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit this main.yml file.</p> <ul> <li>On ATOM, open up the project folder <code>EVPN-Ansible</code> and edit \u201cmain.yml\u201d file under \u201croles/jinja2_leaf/tasks/\u201d to include following:</li> </ul> <pre><code>---\n# tasks file for jinja2_leaf\n- name: Generate Leaf Config\ntemplate: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n- name: Push Leaf Config\ncisco.nxos.nxos_config:\nsrc: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\nmatch: none\n</code></pre> <p>Below screenshot shows how the contents of jinja2_leaf/taks/main.yml file looks like in Atom:</p> <p></p>"},{"location":"task3-vxlan-jinja2/#step-12-run-jinja2_fabric-playbook","title":"Step 12: Run Jinja2_fabric playbook","text":"<p>In this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches.</p> <ul> <li> <p>Before running the ansible-playbook, you may log into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running <code>show running bgp</code> command as shown below:</p> <p></p> </li> </ul> <p>NOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background.</p> <ul> <li> <p>On the Ansible node (in MTputty SSH session), run the command (<code>ansible-playbook jinja2_fabric.yml</code>) to execute the playbook as shown below:</p> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook jinja2_fabric.yml\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> </li> <li> <p>After the configuration push is successful, login (on MTputty SSH session) to leaf-4 switch to verify configuration has been pushed by running below command:</p> <pre><code>show running-config bgp\n</code></pre> <p>The output of above command is shown below:</p> <p></p> </li> </ul> <p>Congrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches</p>"},{"location":"task4-vxlan-nxos/","title":"Task 4 - Config switches using Ansible NXOS modules","text":"<p>In this section, you will build remaining VXLAN fabric using Ansible NXOS modules.  These modules will be used to configure leaf-1, leaf-3 and spine-1 switches within the VXLAN Fabric.  We will configure BGP neighbors between spine and leaf switching by using Ansible NXOS modules.  The following NXOS ansible modules are used:</p> cisco.nxos.nxos_feature Manage features on Nexus switches cisco.nxos.nxos_bgp Manage BGP config cisco.nxos.nxos_bgp_neighbor Manage BGP neighbor config cisco.nxos.nxos_bgp_af Manage BGP address-famility config cisco.nxos.nxos_bgp_neighbor_af Manage BGP neighbor address-famility config <p></p> <p>In comparison to Jinja2, NXOS modules are more abstract from NXOS CLI based configuration. There is no need to have knowledge of exact NXOS CLI syntax to use NXOS modules.  You will follow the steps to configure BGP, Multicast, VXLAN and EVPN.  In each step, you will use different Ansible NXOS modules to accomplish the step.</p> <p></p> <p>After each step, you can login the switches to verify configuration changes.</p>"},{"location":"task4-vxlan-nxos/#step-1-create-new-playbook","title":"Step 1: Create new playbook","text":"<p>Again we will use roles structure to make the playbook more modular.  The roles included in the new playbook are \u201cspine\u201d and \u201cleaf\u201d.  </p> <ul> <li> <p>Switch to \u201cAtom\u201d, then right click on the folder <code>EVPN-Ansible</code> and create a new playbook named <code>nxos_fabric.yml</code>.  Enter this file name and hit enter</p> </li> <li> <p>In the <code>nxos_fabric.yml</code> enter the content as shown below:</p> </li> </ul> <pre><code>---\n\n- hosts: spine\nconnection: local\nroles:\n- spine\n\n- hosts: leaf\nconnection: local\nroles:\n- leaf\n</code></pre> <p>Below screenshot shows the actual content:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-2-create-spine-leaf-roles","title":"Step 2: Create Spine &amp; Leaf roles","text":"<ul> <li>On the MTpuTTy, go back (or initiate a new login/SSH) to Ansible Server node (198.18.134.150).  Switch to \u2018roles\u2019 directory; then create \u2018spine\u2019 and \u2018leaf\u2019 roles using ansible-galaxy as per below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/roles/\nansible-galaxy init spine &amp;&amp; ansible-galaxy init leaf\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder <code>EVPN-Ansible</code>, then open <code>Remote Sync</code> select <code>Download Folder</code> as shown below:</li> </ul> <p></p>"},{"location":"task4-vxlan-nxos/#step-3-spine-role-tasks","title":"Step 3: Spine role - tasks","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit the main.yml file.</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/spine/tasks/</code> to include following:</li> </ul> <pre><code>---\n# tasks file for spine\n#task to configure bgp neighbor to all leaf switches\n- name: Enable BGP\ncisco.nxos.nxos_feature:\nfeature: bgp\nstate: enabled\ntags: bgp\n- name: Configure BGP AS\ncisco.nxos.nxos_bgp:\nasn: \"{{ asn }}\"\nrouter_id: \"{{ router_id }}\"\nstate: present\ntags: bgp\n- name: Configure BGP AF\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: ipv4\nsafi: unicast\ntags: bgp\n- name: Configure iBGP neighbors\ncisco.nxos.nxos_bgp_neighbor:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nremote_as: \"{{ item.remote_as }}\"\nupdate_source: \"{{ item.update_source }}\"\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n- name: Configure iBGP neighbor AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: ipv4\nsafi: unicast\nroute_reflector_client: \"true\"\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul> <p>In the above <code>task/main.yml</code> file multiple NXOS ansible modules have been used:</p> <ul> <li> <p>\u201ccisco.nxos.nxos_feature\u201d module provides the capability to manage features in NX-OS features.  It is used to enable bgp as a feature in above configurations</p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp\u201d module provides the capability to manage BGP configuration in NX-OS.  Here it is used to configure bgp</p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp_af\u201d module provides the capability to manage BGP Address-family configuration in NX-OS.  </p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp_neighbor\u201d module is used to configure the BGP Neighbour in NX-OS.  </p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp_neighbor_af\u201d module provides the capability to manage BGP Address-family Neighbour configuration in NX-OS.  </p> </li> </ul>"},{"location":"task4-vxlan-nxos/#step-4-spine-role-vars","title":"Step 4: Spine role - vars","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file</p> <ul> <li>Switch to Atom, then open up the project folder <code>EVPN-Ansible</code> from the left pane and open <code>main.yml</code> file under \u201croles/spine/vars/\u201d and enter below content:</li> </ul> <pre><code>---\n# vars file for spine\nasn: 65000\n\nbgp_neighbors:\n- { remote_as: 65000, neighbor: 192.168.0.8, update_source: Loopback0 }\n- { remote_as: 65000, neighbor: 192.168.0.10, update_source: Loopback0 }\n- { remote_as: 65000, neighbor: 192.168.0.11, update_source: Loopback0 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-5-leaf-role-tasks","title":"Step 5:  Leaf role - tasks","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/leaf/tasks/</code> to include following:</li> </ul> <pre><code>---\n# tasks file for leaf\n#task to configure bgp neighbor to all spine switches\n- name: Enable BGP\ncisco.nxos.nxos_feature:\nfeature: bgp\nstate: enabled\ntags: bgp\n- name: Configure BGP AS\ncisco.nxos.nxos_bgp:\nasn: \"{{ asn }}\"\nrouter_id: \"{{ router_id }}\"\nstate: present\ntags: bgp\n- name: Configure BGP AF\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: ipv4\nsafi: unicast\ntags: bgp\n- name: Configure iBGP neighbors\ncisco.nxos.nxos_bgp_neighbor:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nremote_as: \"{{ item.remote_as }}\"\nupdate_source: \"{{ item.update_source }}\"\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n- name: Configure iBGP neighbor AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: ipv4\nsafi: unicast\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-6-leaf-role-vars","title":"Step 6: Leaf role - vars","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file</p> <ul> <li>Switch to ATOM, then open up the project folder <code>EVPN-Ansible</code> from the left pane and open <code>main.yml</code> file under \u201croles/leaf/vars/\u201d and enter below content:</li> </ul> <pre><code>---\n# vars file for leaf\nasn: 65000\n\nbgp_neighbors:\n- { remote_as: 65000, neighbor: 192.168.0.6, update_source: Loopback0 }\n- { remote_as: 65000, neighbor: 192.168.0.7, update_source: Loopback0 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-7-execute-playbook","title":"Step 7: Execute playbook","text":"<ul> <li>On the Ansible node (in MTputty SSH session), run the command (<code>ansible-playbook nxos_fabric.yml --tags \"bgp\"</code>) to execute the playbook as shown below:</li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook nxos_fabric.yml --tags \"bgp\"\n</code></pre> <p>Below screenshots shows the execution of above playbook:  </p> <ul> <li>After the configuration push is successful, login (on MTputty SSH session) to leaf-1, leaf-3 or spine-1 switch to verify configuration has been pushed and BGP neighbours are operational by running below command:</li> </ul> <pre><code>show ip bgp summary\n</code></pre> <p>The output of above command providing the BGP neighbours info on Spine-1 and Leaf-3 is shown below:</p> <p></p> <p></p>"},{"location":"task4-vxlan-nxos/#step-8-multicast-config-with-ansible-nxos-modules","title":"Step 8: Multicast config with Ansible NXOS modules","text":"<p>In this section, we will be configuring underlay multicast to support BUM traffic in the VXLAN fabric. The NXOS modules we will be using in this section are nxos_feature    Manage fatures on Nexus switchs nxos_pim_interface  Manage PIM interface configuration</p> cisco.nxos.nxos_pim_rp_address Manage static RP configuration cisco.nxos.nxos_config Manage NXOS arbitrary configuration command cisco.nxos.nxos_interface_ospf Manage configuration OSPF interface instance cisco.nxos.nxos_interfaces Manage physical attribute of interface cisco.nxos.nxos_l3_interfaces Manage L3 interfaces on Cisco NXOS network devices cisco.nxos.nxos_pim_interface Manages PIM interface configuration"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role","title":"Edit playbook for spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the <code>\u201cmain.yml\u201d</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and open <code>\u201cmain.yml\u201d</code> file under <code>\u201croles/spine/tasks/\u201d</code> and save the below tasks at the end the file.</li> </ul> <p>Note</p> <p>Do not replace existing content</p> <pre><code>#task to enable pim and configure anycast rp for underlay multicast\n- name: Enable PIM\ncisco.nxos.nxos_feature:\nfeature: pim\nstate: enabled\ntags: multicast\n- name: Configure Anycast RP interfce\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: loopback1\nenabled: true\ntags: multicast\n- name: Configure IP Address on New LP1\ncisco.nxos.nxos_l3_interfaces:\nconfig:\n- name: loopback1\nipv4:\n- address: \"{{ loopback1 }}/32\"\ntags: multicast\n- name: Configure PIM int\ncisco.nxos.nxos_pim_interface:\ninterface: \"{{ item.interface }}\"\nsparse: true\nwith_items: \"{{L3_interfaces}}\"\ntags: multicast\n- name: Enable OSPF on New LP1\ncisco.nxos.nxos_ospf_interfaces:\nconfig:\n- name: loopback1\naddress_family:\n- afi: ipv4\nprocesses:\n- process_id: \"1\"\narea:\narea_id: 0.0.0.0\ntags: multicast\n- name: Configure PIM RP\ncisco.nxos.nxos_pim_rp_address:\nrp_address: \"{{ loopback1 }}\"\ntags: multicast\n- name: Configure Anycast RP\ncisco.nxos.nxos_config:\nlines:\n- \"ip pim anycast-rp {{ loopback1 }} {{ s1_loopback }}\"\n- \"ip pim anycast-rp {{ loopback1 }} {{ s2_loopback }}\"\ntags: multicast\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role","title":"Edit variable file for Spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the variables file for Spine i.e. <code>\u201cmain.yml\u201d</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and add the below content at the end of <code>\u201cmain.yml\u201d</code> file under <code>\u201croles/spine/vars/\u201d</code></li> </ul> <p>Note</p> <p>Do not replace existing content.</p> <pre><code>  L3_interfaces:\n- { interface: Ethernet1/1 }\n- { interface: Ethernet1/2 }\n- { interface: Ethernet1/3 }\n- { interface: Ethernet1/4 }\n- { interface: loopback0 }\n- { interface: loopback1 }\ns1_loopback: 192.168.0.6\ns2_loopback: 192.168.0.7\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role","title":"Edit playbook for leaf role","text":"<ul> <li>use \u201cAtom\u201d to edit the <code>\u201cmain.yml\u201d</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and add below content at the end of <code>\u201cmain.yml\u201d</code> file under <code>\u201croles/leaf/tasks/\u201d</code>.</li> </ul> <p>Note</p> <p>Do not replace existing content.  </p> <ul> <li>On Atom, Make sure to click <code>File-&gt;Save</code> after entering the below data in this file so it is pushed to Ansible server:</li> </ul> <pre><code>#task to enable PIM for underlay multicast\n- name: Enable PIM\ncisco.nxos.nxos_feature:\nfeature: pim\nstate: enabled\ntags: multicast\n- name: Configure PIM int\ncisco.nxos.nxos_pim_interface:\ninterface: \"{{ item.interface }}\"\nsparse: true\nwith_items: \"{{L3_interfaces}}\"\ntags: multicast\n- name: Configure PIM RP\ncisco.nxos.nxos_pim_rp_address:\nrp_address: \"{{ rp_address }}\"\ntags: multicast\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role","title":"Edit variable file for leaf role","text":"<ul> <li>Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cEVPN-Ansible\u201d and add below content at the end of \u201cmain.yml\u201d file under \u201croles/leaf/vars/\u201d.  On Atom, Make sure to click File-&gt;Save after entering the below data in this file so it is pushed to Ansible server:</li> </ul> <pre><code>  rp_address: 192.168.0.100\nL3_interfaces:\n- { interface: Ethernet1/1 }\n- { interface: Ethernet1/2 }\n- { interface: loopback0 }\n- { interface: loopback1 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes","title":"Run the playbook and verify configuration changes","text":"<ul> <li>Execute the playbook by running <code>ansible-playbook nxos_fabric.yml --tags \"multicast\"</code> command on Ansible server as shown below :</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml --tags \"multicast\"\n</code></pre> <p>Below screenshots shows the output of above command:  </p> <ul> <li> <p>login to any leaf (leaf1, leaf3, leaf4) or Spine-1 switch (via SSH using MTPutty) to verify multicast configuration and PIM neighbors by executing command: <pre><code>show ip pim neighbor\n</code></pre></p> </li> <li> <p>Below screenshot shows the output of above command (<code>show ip pim neighbor</code>) from Spine-1:</p> </li> </ul> <p></p> <p>This confirms Multicast has been enabled using Ansible modules</p>"},{"location":"task4-vxlan-nxos/#step-9-vxlan-config-with-ansible-nxos-modules","title":"Step 9: VXLAN config with Ansible NXOS modules","text":"<p>In this section, we will be configuring VXLAN on leaf and spine switches. The NXOS modules we will be using in this section are</p> nxos_feature Manages features on Nexus switches cisco.nxos.nxos_evpn_global Handles EVPN control plane for VXLAN cisco.nxos.nxos_vlan Manages VLAN resources and attributes cisco.nxos.nxos_vrf Manages global VRF configuration cisco.nxos.nxos_vrf_af Manages VRF address falimily cisco.nxos.nxos_overlay_global Configuration anycast gateway MAC cisco.nxos.nxos_vxlan_vtep Manages VXLAN Network Virtualization Endpoint cisco.nxos.nxos_vxlan_vtep_vni Creates Virtual Network Identifier member"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role_1","title":"Edit playbook for spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file. Open up the project folder \u201cEVPN-Ansible\u201d and open \u201cmain.yml\u201d file under \u201croles/spine/tasks/\u201d and enter the below content (you may copy &amp; paste with correct spaces) at the end of the file</li> </ul> <p>Note</p> <p>Do not replace existing content</p> <pre><code>#task to configure vxlan fabric\n- name: Enable VXLAN Feature\ncisco.nxos.nxos_feature:\nfeature: \"{{item}}\"\n# provider: \"{{cisco.nxos.nxos_provider }}\"\nstate: enabled\nwith_items:\n- nv overlay\n- vn-segment-vlan-based\ntags: vxlan\n- name: Enable NV Overlay\ncisco.nxos.nxos_evpn_global:\nnv_overlay_evpn: true\ntags: vxlan\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role_1","title":"Edit variable file for Spine role","text":"<p>No new variable required for Spine</p>"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role_1","title":"Edit playbook for leaf role","text":"<ul> <li>use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cEVPN-Ansible\u201d and open \u201cmain.yml\u201d file under \u201croles/leaf/tasks/\u201d.  Add the below content in the file at the end of the file (i.e., in addition to existing content), and then make sure to click \u201cFile\u201d-&gt;\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server.</li> </ul> <pre><code>#task to configure VXLAN fabric\n- name: Enable VXLAN Feature\ncisco.nxos.nxos_feature:\nfeature: \"{{ item }}\"\nstate: enabled\nwith_items:\n- nv overlay\n- vn-segment-vlan-based\ntags: vxlan\n- name: Enable VXLAN Feature\ncisco.nxos.nxos_feature:\nfeature: \"{{ item }}\"\nstate: enabled\nwith_items:\n- nv overlay\n- vn-segment-vlan-based\ntags: vxlan\n- name: Enable NV Overlay\ncisco.nxos.nxos_evpn_global:\nnv_overlay_evpn: true\ntags: vxlan\n- name: Configure VLAN to VNI\ncisco.nxos.nxos_vlans:\nconfig:\n- vlan_id: \"{{ item.vlan_id }}\"\nmapped_vni: \"{{ item.vni }}\"\nname: \"{{ item.vlan_name }}\"\nwith_items:\n- \"{{ L2VNI }}\"\n- \"{{ L3VNI }}\"\ntags: vxlan\n- name: Configure Tenant VRF\ncisco.nxos.nxos_vrf:\nvrf: Tenant-1\nrd:  auto\nvni: \"{{ L3VNI[0].vni }}\"\ntags: vxlan\n- name: Configure VRF AF\ncisco.nxos.nxos_vrf_af:\nvrf: Tenant-1\nroute_target_both_auto_evpn: true\nafi: ipv4\ntags: vxlan\n- name: Configure Anycast GW\ncisco.nxos.nxos_overlay_global:\nanycast_gateway_mac: 0000.2222.3333\ntags: vxlan\n- name: Configure L2VNI\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: vlan\"{{ item.vlan_id }}\"\nfabric_forwarding_anycast_gateway: true\nwith_items: \"{{ L2VNI }}\"\ntags: vxlan\n- name: Configure L3VNI\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: vlan\"{{ L3VNI[0].vlan_id }}\"\nip_forward: true\ntags: vxlan\n- name: No shut VLAN\ncisco.nxos.nxos_config:\nlines:\n- no shutdown\nparents: interface vlan{{ item.vlan_id }}\nwith_items:\n- \"{{ L2VNI }}\"\n- \"{{ L3VNI }}\"\ntags: vxlan\n- name: Assign interface to Tenant VRF\ncisco.nxos.nxos_vrf_interface:\nvrf: Tenant-1\ninterface: \"vlan{{ item.vlan_id }}\"\nwith_items:\n- \"{{ L2VNI }}\"\n- \"{{ L3VNI }}\"\ntags: vxlan\n- name: Configure SVI IP\ncisco.nxos.nxos_l3_interfaces:\nconfig:\n- name: \"vlan{{ item.vlan_id }}\"\nipv4:\n- address: \"{{ item.ip_add }}/{{ item.mask }}\"\nwith_items: \"{{ L2VNI }}\"\ntags: vxlan\n- name: Configure VTEP Tunnel\ncisco.nxos.nxos_vxlan_vtep:\ninterface: nve1\nshutdown: \"false\"\nsource_interface: Loopback1\nhost_reachability: \"true\"\ntags: vxlan\n- name: Configure L2VNI to VTEP\ncisco.nxos.nxos_vxlan_vtep_vni:\ninterface: nve1\nvni: \"{{ item.vni }}\"\nmulticast_group: \"{{ item.mcast }}\"\nwith_items: \"{{ L2VNI }}\"\ntags: vxlan\n- name: Configure L3VNI to VTEP\ncisco.nxos.nxos_vxlan_vtep_vni:\ninterface: nve1\nvni: \"{{ L3VNI[0].vni }}\"\nassoc_vrf: true\ntags: vxlan\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role_1","title":"Edit variable file for leaf role","text":"<ul> <li>Use \u201cAtom\u201d to edit the <code>main.yml</code> file. Open up the project folder <code>EVPN-Ansible</code> and open <code>main.yml</code> file under <code>roles/leaf/vars/</code>.  Add the below content in the file at the end of the file and then make sure to click <code>\u201cFile\u201d -&gt; \u201cSave\u201d</code> on Atom, so that the updated file is pushed to Ansible server</li> </ul> <p>Note</p> <p>Do not replace existing content. Below contents must be added at the end of the file.</p> <pre><code>  L2VNI:\n- { vlan_id: 140, vni: 50140, ip_add: 172.21.140.1, mask: 24, vlan_name: L2-VNI-140-Tenant1, mcast: 239.0.0.140 }\n- { vlan_id: 141, vni: 50141, ip_add: 172.21.141.1, mask: 24, vlan_name: L2-VNI-141-Tenant1, mcast: 239.0.0.141 }\nL3VNI:\n- { vlan_id: 999, vlan_name: L3-VNI-999-Tenant1, vni: 50999 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_1","title":"Run the playbook and verify configuration changes","text":"<ul> <li>On Ansible server (via SSH connection on MTputty), run the below command:</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml --tags \"vxlan\"\n</code></pre> <ul> <li> <p>Below screenshot show partial output of above command:</p> <p> </p> </li> <li> <p>After finishing this task: login to any leaf switch (on MTPutty) to verify VXLAN configuration and the VNI by issuing the command:  </p> </li> </ul> <pre><code>show nve vni\n</code></pre> <p>Below screenshot shows the output of above command from Leaf-3:</p> <p></p>"},{"location":"task4-vxlan-nxos/#step-10-evpn-config-with-ansible-nxos-modules","title":"Step 10: EVPN config with Ansible NXOS modules","text":"<p>In this section, we will be configuring BGP EVPN on leaf and spine switches. The NXOS modules we will be using in this section are:</p> cisco.nxos.nxos_bgp_af Manage BGP address-famility config cisco.nxos.nxos_bgp_neighbor_af Manage BGP neighbor address-famility config cisco.nxos.nxos_evpn_vni Manage Cisco EVPN VXLAN Network Identifier"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role_2","title":"Edit playbook for spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the <code>main.yml</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and open <code>main.yml</code> file under <code>\u201croles/spine/tasks/\u201d</code>.  Add the below content in the file in addition to existing content.</li> <li>Then make sure to click <code>\u201cFile\u201d-&gt;\u201cSave\u201d</code> on Atom, so that the updated file is pushed to Ansible server.</li> </ul> <pre><code># task to configure BGP EVPN\n- name: Configure BGP EVPN\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: l2vpn\nsafi: evpn\ntags: evpn\n- name: Configure iBGP neighbor EVPN AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: l2vpn\nsafi: evpn\nroute_reflector_client: \"true\"\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: evpn\n</code></pre>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role_2","title":"Edit variable file for Spine role","text":"<p>No new variables required for Spine</p>"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role_2","title":"Edit playbook for leaf role","text":"<ul> <li>use \u201cAtom\u201d to edit the <code>main.yml</code> file. Open up the project folder <code>EVPN-Ansible</code> and open <code>main.yml</code> file under <code>\u201croles/leaf/tasks/\u201d</code>.  Add the below content in the file in addition to existing content.</li> <li>Then make sure to click <code>\u201cFile\u201d-&gt;\u201cSave\u201d</code> on Atom, so that the updated file is pushed to Ansible server.</li> </ul> <pre><code>#task to configure BGP EVPN\n- name: Configure BGP EVPN\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: l2vpn\nsafi: evpn\ntags: evpn\n- name: Configure iBGP neighbor EVPN AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: l2vpn\nsafi: evpn\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: evpn\n- name: Configure L2VNI RD/RT\ncisco.nxos.nxos_evpn_vni:\nvni: \"{{ item.vni }}\"\nroute_distinguisher: auto\nroute_target_both: auto\nwith_items: \"{{ L2VNI }}\"\ntags: evpn\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role_2","title":"Edit variable file for leaf role","text":"<p>No new variables required for Leaf</p>"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_2","title":"Run the playbook and verify configuration changes","text":"<ul> <li>On the Ansible server (using MTPutty) run this playbook by running the command <code>ansible-playbook nxos_fabric.yml --tags \"evpn\"</code> as shown below:</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml --tags \"evpn\"\n</code></pre> <p>Below screenshot shows the output of above playbook:</p> <p></p> <ul> <li>After successful execution of the playbook: login to any leaf or spine switch to verify BGP EVPN configuration and evpn neighbor by issuing command: <pre><code>show bgp l2vpn evpn summary\n</code></pre></li> </ul> <p>Below screenshot shows the output of above command from leaf-3 switch.  As expected it shows the spine-1 and spine-2 as neighbors:</p> <p></p>"},{"location":"task4-vxlan-nxos/#step-11-run-nxos_fabric-playbook","title":"Step 11: Run nxos_fabric playbook","text":"<ul> <li>Up to this point, you have run the playbook for each step separately (using tags).  You could re-run the whole playbook without giving any tags, but no new changes should be maded to the switches.</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml\n</code></pre>"},{"location":"task4-vxlan-nxos/#step-12-verify-end-to-end-ip-connectivity","title":"Step 12: Verify end-to-end IP connectivity","text":"<p>Now Let\u2019s verify the VXLAN bridging and VXLAN routing from servers that are pre-configured in following VLANs and IPs</p> Server Name Connect to switch In VLAN IP of server Server-1 Leaf-1 140 172.21.140.10 Server-3 Leaf-3 140 172.21.140.11 Server-4 Leaf-4 141 172.21.141.11 <p>Below figure shows the topology &amp; connectivity of servers to Leaf switches and their respective IP addresses:</p> <p></p> <ul> <li> <p>Switch to MTPuTTY and SSH to <code>server-1</code>.  If prompted enter the credentials of <code>root</code>and <code>C1sco12345</code></p> </li> <li> <p>Ping default gateway from server-1 by issuing command <code>ping 172.21.140.1 -c 5</code> as shown below:</p> </li> </ul> <pre><code>[root@server-1 ~]# ping 172.21.140.1 -c 5\nPING 172.21.140.1 (172.21.140.1) 56(84) bytes of data.\n64 bytes from 172.21.140.1: icmp_seq=2 ttl=255 time=15.7 ms\n64 bytes from 172.21.140.1: icmp_seq=3 ttl=255 time=4.11 ms\n</code></pre> <ul> <li>Next, Ping server 3 and server 4 from server-1 (in same VLAN and inter-VLAN respectively) by using <code>ping 172.21.140.11 -c 5</code> and <code>ping 172.21.141.11 -c 5</code> commands as shown below:</li> </ul> <pre><code>[[root@server-1 ~]# ping 172.21.140.11 -c 5\nPING 172.21.140.11 (172.21.140.11) 56(84) bytes of data.\n64 bytes from 172.21.140.11: icmp_seq=1 ttl=64 time=1032 ms\n64 bytes from 172.21.140.11: icmp_seq=2 ttl=64 time=35.7 ms\n64 bytes from 172.21.140.11: icmp_seq=3 ttl=64 time=14.4 ms\n^C\n--- 172.21.140.11 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2112ms\nrtt min/avg/max/mdev = 14.431/360.839/1032.335/474.899 ms, pipe 2\n[root@server-1 ~]# ping 172.21.141.11 -c 5\nPING 172.21.141.11 (172.21.141.11) 56(84) bytes of data.\n64 bytes from 172.21.141.11: icmp_seq=994 ttl=62 time=30.2 ms\n64 bytes from 172.21.141.11: icmp_seq=995 ttl=62 time=16.1 ms\n64 bytes from 172.21.141.11: icmp_seq=996 ttl=62 time=18.0 ms\n</code></pre> <p>Congratulation! You have successfully built VXLAN fabric using ansible + Jinja2 template and ansible + NXOS modules.</p>"},{"location":"task5-day2-operation/","title":"Task 5: Day 2 operation using Ansible","text":"<p>In this section, we will use automation to perform following day 2 operation tasks.</p> <ul> <li>Backup running configurations on all leaf and spine switches</li> <li>Verify underlay ospf, bgp and pim neighbors</li> <li>Verify overlay nve peer, host route, bgp update</li> <li>Baseline configuration comparison</li> <li>Add new VNIs into the existing fabric</li> </ul>"},{"location":"task5-day2-operation/#step-1-backup-running-configurations","title":"Step 1: Backup running configurations","text":"<p>In this section, you will use ios_config module to backup running configuration on each switch, the backup file will be saved to a local \u201cbackup\u201d folder.  The backup argument create a full backup of the current running-config of each switch.  The backup file is written to the backup folder in the playbook root directory. If the directory does not exist, it is created.</p> <ul> <li> <p>On Atom, open up the project folder \u201cEVPN-Ansible\u201d and create new file under <code>EVPN-Ansible</code>. Name the new file <code>get_config.yml</code>.</p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> </ul> <pre><code>---\n- hosts: spine,leaf,jinja2_leaf,jinja2_spine\ntasks:\n- name: save running\ncisco.nxos.nxos_config:\nbackup: yes\n</code></pre> <ul> <li> <p>On the Ansible node (using MTputty via SSH), run \u201cget_config.yml\u201d playbook and verify the backup configurations in \u201cbackup\u201d folder by using below commands:</p> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook get_config.yml\n[root@rhel7-tools EVPN-Ansible]# ls -lrt\n[root@rhel7-tools EVPN-Ansible]# ls backup\n</code></pre> <p>You may further view the contents of the files under backup folder by using cat, less or more commands.  Below screenshot shows the output of above commands</p> <p></p> </li> </ul>"},{"location":"task5-day2-operation/#step-2-verify-underlay-and-overlay","title":"Step 2: Verify underlay and overlay","text":"<p>In this step, you will verify underlay and overlay operation using ansible playbook. The playbook will be applied to all leaf switches to verify the below commands:</p> <p>Underlay</p> <pre><code>-   show ip ospf neighbor\n-   show ip bgp sum\n-   show ip pim neighbor\n</code></pre> <p>Overlay</p> <pre><code>-   show nve vni\n-   show nve peer\n-   show ip route vrf Tenant-1\n-   show bgp l2vpn evpn\n-   show l2route evpn mac-ip all\n</code></pre> <ul> <li>Switch to \u201cAtom\u201d, right click on the folder <code>EVPN-Ansible</code> and create a new playbook named <code>verify_fabric.yml</code>. Enter this file name and hit enter.</li> </ul> <pre><code>---\n- hosts: leaf, jinja2_leaf\nconnection: local\ngather_facts: false\ntasks:\n- name: verify underlay\nregister: underlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show ip ospf neighbors\n- show ip bgp sum\n- show ip pim neighbor\ntags: underlay\n- debug: var=underlay_output.stdout_lines\ntags: underlay\n- name: Verify Overlay\nregister: overlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show nve vni\n- show nve peer\n- show ip route vrf Tenant-1\n- show bgp l2vpn evpn\n- show l2route evpn mac-ip all\ntags: overlay\n- debug: var=overlay_output.stdout_lines\ntags: overlay\n</code></pre> <ul> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> <li> <p>On the Ansible node (via MTPutty), run verify_fabric.yml playbook and verify the output for underlay by executing below command (using respective tag): <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook verify_fabric.yml --tags \"underlay\"\n</code></pre></p> </li> </ul> <p>The output shows ospf, bgp and pim neighbors for all leaf switches</p> <ul> <li> <p>Below screenshot shows the partial output of above command:</p> <p></p> </li> </ul> <p>Here is a log of execution of above command:</p> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook verify_fabric.yml --tags \"underlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [verify underlay] ***********************************************************************************************************************************************************************************************\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.101] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:12 10.0.0.21       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:12 10.0.128.5      Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.8, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     161     106        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     161     106        6    0    0 01:23:15 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.21       Ethernet1/1          01:23:08  00:01:35  1        yes     n/a     no\",\n            \"10.0.128.5      Ethernet1/2          01:23:07  00:01:31  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.104] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          1d04h    10.0.128.1      Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          1d04h    10.0.128.17     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.11, local AS number 65000\",\n            \"BGP table version is 5, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     672     662        5    0    0 07:03:12 0         \",\n            \"192.168.0.7     4 65000    1441    1433        5    0    0 22:36:01 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.128.1      Ethernet1/1          08:51:43  00:01:28  1        yes     n/a     no\",\n            \"10.0.128.17     Ethernet1/2          22:36:23  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:11 10.0.0.29       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:10 10.0.128.13     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.10, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     148     107        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     150     107        6    0    0 01:23:17 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.29       Ethernet1/1          01:23:09  00:01:37  1        yes     n/a     no\",\n            \"10.0.128.13     Ethernet1/2          01:23:08  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p>Next:</p> <ul> <li>Run verify_fabric.yml playbook and verify the output for overlay using the respective tag in the command (as shown below):</li> </ul> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook verify_fabric.yml --tags \"overlay\"\n</code></pre> <p>The output shows nve tunnel peer, host route in bgp EVPN from all leaf switches</p> <ul> <li> <p>Below screenshot of the partial output of above command:</p> <p></p> </li> <li> <p>Below shows the complete log output of execution of above playbook command.   Verify the output for vne vni status, vne dynamic neighbors, type host mac+ip evpn route update for each L2VNI, l2fib information.</p> </li> </ul> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook verify_fabric.yml --tags \"overlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [Verify Overlay] ************************************************************************************************************************************************************************************************\nok: [198.18.4.104]\nok: [198.18.4.103]\nok: [198.18.4.101]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.104] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.110    Up    CP        01:23:17 000c.2939.f53f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:52, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:23:18, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.11, Vlan141, [190/0], 07:04:08, hmm\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 458, Local Router ID is 192.168.0.11\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"* i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908    (L2VNI 50141)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d HMM    --            0          172.21.141.11  Local\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.111    Up    CP        01:23:20 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:53, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.11, Vlan140, [190/0], 01:23:18, hmm\",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:23:20, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 195, Local Router ID is 192.168.0.10\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 HMM    --            0          172.21.140.11  Local          \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\nok: [198.18.4.101] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.110    Up    CP        01:07:59 000c.2939.f53f   \",\n            \"nve1      192.168.0.111    Up    CP        01:07:59 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.10, Vlan140, [190/0], 01:24:18, hmm\",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 210, Local Router ID is 192.168.0.8\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907    (L2VNI 50140)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"* i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 HMM    --            0          172.21.140.10  Local          \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n[root@rhel7-tools EVPN-Ansible]#\n</code></pre>"},{"location":"task5-day2-operation/#step-3-baseline-configuration-comparison","title":"Step 3: Baseline configuration comparison","text":"<p>In this section we will compare the running configuration with baseline configuration for configuration compliance check. The configuration file that we backed in tak 1 will be used as baseline configuration.</p> <p>In this playbook, you will use \u201clookup\u201d module to find the backup filename generated in Step 1. Then you will use diff_against function in nxos_config module to compare running configuration.</p> <ul> <li>On Atom, Open up the project folder <code>EVPN-Ansible</code> and create new file under \u201cEVPN-Ansible\u201d. Name the new file <code>verify_config.yml</code> and enter below data in this playbook:</li> </ul> <pre><code>---\n- hosts: jinja2_leaf,leaf,jinja2_spine,spine\nvars:\nfilename: \"{{ lookup('pipe', 'ls backup/{{ inventory_hostname}}_config.*')}}\"\ntasks:\n- name: configure compliance\nregister: diff_config\ncisco.nxos.nxos_config:\ndiff_against: intended\nintended_config: \"{{ lookup('file', '{{filename}}') }}\"\n</code></pre> <ul> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> <li> <p>Before you run this playbook, SSH into leaf-4 to make some configuration changes by issuing below commands: <pre><code>config t\nno router bgp 65000\ncopy run start\nend\n</code></pre></p> </li> </ul> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>On the Ansible server (via MTputty SSH session), run the playbook for configuration compliance check by executing <code>ansible-playbook --diff verify_config.yml</code> as shown below below:</li> </ul> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook --diff verify_config.yml\n</code></pre> <p>The delta between current running config and base line config are highlighted in RED from the result</p> <ul> <li> <p>Below partial screenshot shows the output of above command:</p> <p></p> </li> <li> <p>Bring leaf-4 back to the baseline config by executing <code>ansible-playbook jinja2_fabric.yml --limit=198.18.4.104</code> command as shown below:</p> </li> </ul> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook jinja2_fabric.yml --limit=198.18.4.104\n</code></pre> <ul> <li> <p>Below screenshot shows the output of above command.  You can also log into leaf-4 and verify that bgp configurations are back:</p> <p></p> </li> </ul>"},{"location":"task5-day2-operation/#step-4-add-new-vni","title":"Step 4: Add new VNI","text":"<p>In this section, we will introduce following new VNI into the VXLAN fabric.</p> VLAN ID VLAN Name VNI IP_Add mask Mcast 200 L2-VNI-200-Tenant1 50200 172.21.200.1 24 239.0.0.200 201 L2-VNI-201-Tenant1 50201 172.21.201.1 24 239.0.0.201 <ul> <li>First we will creat a new role, and name it \u201cvni_provision\u201d under folder roles using ansible-galaxy using below commands on the Ansible node (using MTputty via SSH connection):</li> </ul> <pre><code>[root@rhel7-tools EVPN-Ansible]# cd /root/EVPN-Ansible/roles/\n[root@rhel7-tools roles]# ansible-galaxy init vni_provision\n</code></pre> <ul> <li> <p>Verify vni_provision was created successfully</p> </li> <li> <p>Ansible-galaxy init will create new role with base role structure and empty main.yml file as role requires.  </p> </li> <li> <p>Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and remote desktop. Right click on project folder \u201cEVPN-Ansible\u201d, open \u201cRemote Sync\u201d select \u201cDownload Folder\u201d</p> <p></p> </li> <li> <p>Edit variable file main.yml for \u201cvni_provision\u201d role under \u201c/root/EVPN-Ansible/roles/vni_provision/vars\u201d and enter below data.  Make sure to click <code>File</code> and <code>Save</code> on Atom to push this to Ansible server:</p> </li> </ul> <pre><code>---\n# vars file for vni_provision\nL2VNI:\n- { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }\n</code></pre> <p></p> <ul> <li>Edit playbook file main.yml for \u201cvni_provision\u201d role under \u201c/root/EVPN-Ansible/roles/vni_provision/tasks\u201d and enter below data.   Make sure to click <code>File</code> and <code>Save</code> on Atom to push this to Ansible server:</li> </ul> <pre><code>---\n# tasks file for vni_provision\n- name: Configure VLAN to VNI\ncisco.nxos.nxos_vlan:\nvlan_id: \"{{ item.vlan_id }}\"\nmapped_vni: \"{{ item.vni }}\"\nname: \"{{ item.vlan_name }}\"\nwith_items:\n- \"{{ L2VNI }}\"\n- name: Configure L2VNI\ncisco.nxos.nxos_interface:\ninterface: vlan\"{{ item.vlan_id }}\"\nwith_items: \"{{ L2VNI }}\"\n- name: Assign interface to Tenant VRF\ncisco.nxos.nxos_vrf_interface:\nvrf: Tenant-1\ninterface: \"vlan{{ item.vlan_id }}\"\nwith_items:\n- \"{{ L2VNI }}\"\n- name: Configure SVI IP\ncisco.nxos.nxos_l3_interfaces:\nconfig:\n- name: \"vlan{{ item.vlan_id }}\"\nipv4:\n- address: \"{{ item.ip_add }}/{{ item.mask }}\"\nwith_items: \"{{ L2VNI }}\"\n- name: Configure L2VNI SVI\ncisco.nxos.nxos_interface:\ninterface: vlan\"{{ item.vlan_id }}\"\nfabric_forwarding_anycast_gateway: true\nwith_items: \"{{ L2VNI }}\"\n- name: Configure L2VNI to VTEP\ncisco.nxos.nxos_vxlan_vtep_vni:\ninterface: nve1\nvni: \"{{ item.vni }}\"\nmulticast_group: \"{{ item.mcast }}\"\nwith_items: \"{{ L2VNI }}\"\n- name: Configure L2VNI RD/RT\ncisco.nxos.nxos_evpn_vni:\nvni: \"{{ item.vni }}\"\nroute_distinguisher: auto\nroute_target_both: auto\nwith_items: \"{{ L2VNI }}\"\n</code></pre> <p>this is shown in below screenshot:</p> <p></p> <ul> <li>Switch to \u201cAtom\u201d  create new playbook \u2018vni_provision.yml\u2019 under project folder EVPN-Ansible and enter below data.  Make sure to click <code>File</code> and <code>Save</code> on Atom to push this to Ansible server:</li> </ul> <pre><code>---\n- hosts: leaf,jinja2_leaf\nconnection: local\nroles:\n- vni_provision\n</code></pre> <ul> <li>Run playbook vni_provision.yml to add new VNIs on the fabric by issuing <code>ansible-playbook vni_provision.yml</code> command as shown below:</li> </ul> <pre><code>[root@rhel7-tools EVPN-Ansible]# ansible-playbook vni_provision.yml\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Switch to MTPutty and connect to leaf-4 (SSH connection0, verify the change on leaf switches by issuing below command: <pre><code>show nve vni\n</code></pre></li> </ul> <p>Notice the new created L2VNI as shown in below screenshot:</p> <p></p>"},{"location":"task5-day2-operation/#congratulation-you-have-completed-vxlan-fabric-lab","title":"Congratulation! You have completed VXLAN Fabric Lab.","text":""},{"location":"task5-day2-pipeline/","title":"Task 5: Day 2 operation using CI Pipeline","text":"<p>We will implement CI/CD pipeline for day 2 operation tasks in this section.  GitLab and GitLab runner will be used for this purpose</p> <ul> <li>GitLab is software development platform that incorporates multiple capabilities including Version Control, code review and CI/CD in single system</li> <li>GitLab runner is an application that run tests &amp; results i.e., jobs in a pipeline and then send the results to GitLab</li> <li>The cloud hosted GitLab is used in this lab, while the GitLab runner will be installed on the same local server which is used as Ansible host in this lab setup</li> </ul> <p>The process for creating a CI/CD pipeline with GitLab &amp; GitLab runner include below procedure:</p> <ul> <li>Create a project in GitLab</li> <li>Install and register the GitLab Runner for your project</li> <li>Define a CI/CD job (steps, tests) in a file named <code>.gitlab-ci.yml</code> in the root of the repository</li> <li>Conditions/rules can be defined in <code>.gitlab-ci.yml</code> file (YAML syntax) that will perform a job and display the results in GitLab pipeline e.g. When a commit to repository is done then the runner may execute the job and display the results in GitLab pipeline</li> </ul> <p>In this section, CI/CD pipeline for day 2 operation tasks will be implemented.  Below steps will be done:</p> <ul> <li>Create an Ansible playbook to verify underlay and overlay</li> <li>Version control for the EVPN Ansible playbooks using GitLab version control capabilities</li> <li>Create CI pipeline using GitLab</li> <li>Add new VNIs into existing fabric</li> <li>Verify CI pipeline in test and staging stages</li> <li>Commit the merger and verify CI Pipeline in production stage</li> </ul>"},{"location":"task5-day2-pipeline/#step-1-playbook-to-verify-underlay-and-overlay","title":"Step 1: Playbook to verify underlay and overlay","text":"<p>In this step, you will create the playbook to verify underlay and overlay operation. The playbook will be applied to all leaf switches to verify the below commands:</p> <p>Underlay</p> <pre><code>-   show ip ospf neighbor\n-   show ip bgp sum\n-   show ip pim neighbor\n</code></pre> <p>Overlay</p> <pre><code>-   show nve vni\n-   show nve peer\n-   show ip route vrf Tenant-1\n-   show bgp l2vpn evpn\n-   show l2route evpn mac-ip all\n</code></pre> <ul> <li>Switch to Atom.  On the left page right click on the folder <code>EVPN-Ansible</code> and create a new playbook named <code>verify_fabric.yml</code>.   Enter this file name and hit enter.  Then Copy and Paste the below content to this newly created file:</li> </ul> <pre><code>---\n- hosts: leaf, jinja2_leaf\nconnection: local\ngather_facts: false\ntasks:\n- name: verify underlay\nregister: underlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show ip ospf neighbors\n- show ip bgp sum\n- show ip pim neighbor\ntags: underlay\n- debug: var=underlay_output.stdout_lines\ntags: underlay\n- set_fact:\nsavefile: \"{{underlay_output.stdout_lines | to_nice_yaml}}\"\n- local_action: copy content=\"{{savefile}}\" dest=./underlay.txt\n- name: Verify Overlay\nregister: overlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show nve vni\n- show nve peer\n- show ip route vrf Tenant-1\n- show bgp l2vpn evpn\n- show l2route evpn mac-ip all\ntags: overlay\n- debug: var=overlay_output.stdout_lines\ntags: overlay\n- set_fact:\nsavefile: \"{{overlay_output.stdout_lines | to_nice_yaml}}\"\n- local_action: copy content=\"{{savefile}}\" dest=./overlay.txt\n</code></pre> <ul> <li> <p>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> <li> <p>On the Ansible node (via MTPuTTy), run verify_fabric.yml playbook and verify the output for underlay by executing below command (using respective tag).  This command will show ospf, bgp and pim neighbors for all leaf switches:</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook verify_fabric.yml --tags \"underlay\"\n</code></pre> <ul> <li> <p>Below screenshot shows the partial output of above command and shows ospf, bgp and pim neighbors for all leaf switches:</p> <p></p> </li> </ul> <p>Here is complete log of execution of above playbook/command:</p> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook verify_fabric.yml --tags \"underlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [verify underlay] ***********************************************************************************************************************************************************************************************\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.101] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:12 10.0.0.21       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:12 10.0.128.5      Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.8, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     161     106        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     161     106        6    0    0 01:23:15 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.21       Ethernet1/1          01:23:08  00:01:35  1        yes     n/a     no\",\n            \"10.0.128.5      Ethernet1/2          01:23:07  00:01:31  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.104] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          1d04h    10.0.128.1      Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          1d04h    10.0.128.17     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.11, local AS number 65000\",\n            \"BGP table version is 5, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     672     662        5    0    0 07:03:12 0         \",\n            \"192.168.0.7     4 65000    1441    1433        5    0    0 22:36:01 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.128.1      Ethernet1/1          08:51:43  00:01:28  1        yes     n/a     no\",\n            \"10.0.128.17     Ethernet1/2          22:36:23  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:11 10.0.0.29       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:10 10.0.128.13     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.10, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     148     107        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     150     107        6    0    0 01:23:17 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.29       Ethernet1/1          01:23:09  00:01:37  1        yes     n/a     no\",\n            \"10.0.128.13     Ethernet1/2          01:23:08  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p>Next:</p> <ul> <li>On the Ansible node (via MTPutty), execute the verify_fabric.yml playbook by issuing below command.  As per below, this command uses the <code>--tags</code> in the syntax to execute the respective tasks (as per the tag) in the playbook.  As per the output, verify the overlay outputs (as shown below).  Note: This command will show the nve tunnel peer, host route in BGP EVPN from all leaf switches:</li> </ul> <pre><code>ansible-playbook verify_fabric.yml --tags \"overlay\"\n</code></pre> <ul> <li> <p>Below screenshot of the partial output of above command:</p> <p></p> </li> <li> <p>Below shows the complete log output of execution of above playbook command.   Verify the output for vne vni status, vne dynamic neighbors, mac-ip evpn route update for each L2VNI, l2fib etc. information:</p> </li> </ul> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook verify_fabric.yml --tags \"overlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [Verify Overlay] ************************************************************************************************************************************************************************************************\nok: [198.18.4.104]\nok: [198.18.4.103]\nok: [198.18.4.101]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.104] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.110    Up    CP        01:23:17 000c.2939.f53f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:52, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:23:18, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.11, Vlan141, [190/0], 07:04:08, hmm\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 458, Local Router ID is 192.168.0.11\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"* i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908    (L2VNI 50141)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d HMM    --            0          172.21.141.11  Local\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.111    Up    CP        01:23:20 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:53, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.11, Vlan140, [190/0], 01:23:18, hmm\",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:23:20, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 195, Local Router ID is 192.168.0.10\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 HMM    --            0          172.21.140.11  Local          \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\nok: [198.18.4.101] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.110    Up    CP        01:07:59 000c.2939.f53f   \",\n            \"nve1      192.168.0.111    Up    CP        01:07:59 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.10, Vlan140, [190/0], 01:24:18, hmm\",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 210, Local Router ID is 192.168.0.8\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907    (L2VNI 50140)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"* i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 HMM    --            0          172.21.140.10  Local          \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\nroot@ubuntu:~/EVPN-Ansible#\n</code></pre>"},{"location":"task5-day2-pipeline/#step-2-install-git-and-gitlab-runner","title":"Step 2: Install git and gitlab runner","text":"<p>In this section you will install git and gitlab runner on Anisble node.</p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below package installation command to install git:</li> </ul> <pre><code>cd ~/EVPN-Ansible\napt-get install git\n</code></pre> <p>Below screenshot shows the execution of the command</p> <p></p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below <code>curl</code> command to download and run a shell script that adds the official GitLab repository required for runner installation:</li> </ul> <pre><code>curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash\n</code></pre> <p>Below screenshot shows the execution of the command</p> <p></p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below command to install <code>gitlab-runner</code></li> </ul> <pre><code>apt-get install gitlab-runner\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p>"},{"location":"task5-day2-pipeline/#step-3-version-control-playbooks","title":"Step 3: Version control playbooks","text":"<p>In this step, you will use Gitlab to set version control for the playbooks.</p> <ul> <li> <p>Open chrome browser and enter gitlab.com/users/sign_in in the address bar. Sign in with Username: <code>lab.ciscolive@gmail.com</code> Password: <code>C1sco12345</code> as shown below.</p> <p></p> </li> <li> <p>After sign in to lab.ciscolive gitlab account, click New project on the top right to build a new project as shown below:</p> <p></p> </li> <li> <p>Select Create project to create a blank project with your assigned <code>POD_{ID}</code>.  Note: In the below screenshot:</p> <ul> <li> <p>POD_1 is shown as an example - you must replace the <code>{ID}</code> with your respective pod number.  Find your assigned POD ID from table in task1.   </p> </li> <li> <p>On this page, Select the Visibility Level to <code>Private</code></p> </li> <li>On this page, Click the <code>checkbox</code> to <code>Initialize repository with a README</code> </li> </ul> <p></p> </li> <li> <p>On the project page, copy the project url from Clone with HTTPS as shown below. The project url will be used in next step. The project url looks like https://gitlab.com/lab.ciscolive/pod_{ID}.git.   </p> </li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  </p> <p>The screenshot below uses POD_1 as example.</p> <p></p> <ul> <li>By default the main branch is protected branch.  In order to add and commit files from git command, the branch settings need to be changed to unprotected.  This change is done by cl the main branch to unprotected from project from the left pane Select Settings &gt; Repository and then on right pane Click Protected branches &gt; unprotect as shown in below screenshot:</li> </ul> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), run the below commands to initialize git in appropriate directory:</li> </ul> <pre><code>cd ~/EVPN-Ansible\ngit init\n</code></pre> <p>Below screenshot shows the execution of the initialize commands in correct directory:</p> <p></p> <ul> <li>On the Ansible node, run following commands to add the remote repository as origin and verify it's result:</li> </ul> <pre><code>git remote add origin https://gitlab.com/lab.ciscolive/pod_{ID}.git\ngit remote -v\n</code></pre> <p>Below screenshot shows the execution of the above commands:</p> <p></p> <ul> <li>Then to add all the files (Ansible playbooks, roles etc) to the staging area and check the status by using below commands:</li> </ul> <pre><code>git add .\ngit status\n</code></pre> <p>Below screenshot shows the execution of the above commands:</p> <p></p> <ul> <li>Next, commit and push all files to main branch with below commands.  When prompted for credentials, enter the Username: <code>lab.ciscolive</code> Password: <code>C1sco12345</code></li> </ul> <pre><code>git checkout -b main\ngit commit -m \"initial commit\"\ngit push -f origin main\n</code></pre> <p>Below screenshot shows partial outputs of the commands:</p> <p></p>"},{"location":"task5-day2-pipeline/#step-4-add-ci-pipeline-file","title":"Step 4: Add CI pipeline file","text":"<p>In this step, you will create CI pepeline file for gitlab. Gitlab uses a special file named .gitlab-ci.yml for CI/CD configuration.  </p> <p>Note</p> <p>The file name starts with . and it's not a mistake.  </p> <p>The file needs to be placed in root of the repository.  Inside the file, you will define:</p> <ul> <li> <p>The stages you want run in the pipeline</p> </li> <li> <p>The scripts you want run in each stage</p> </li> <li> <p>The runner you want use for each script</p> </li> <li> <p>How is the runner triggered for each script</p> </li> </ul> <p>Pipeline file can be added from GitLab UI using pipeline file editor, or from local and push to GitLab repo using git commands.  We will use Atom to add pipeline file in this lab.</p> <ul> <li> <p>Switch to \"Atom\" application on your remote desktop.  Right click on the folder EVPN-Ansible and Click New File to create a new file named .gitlab-ci.yml</p> </li> <li> <p>In the <code>.gitlab-ci.yml</code> file enter the contents shown below:</p> </li> </ul> <pre><code>stages:\n- test\n- staging\n- production\n\nverify:\nstage: test\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml --check\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"push\" &amp;&amp; $CI_COMMIT_BRANCH != \"main\"'\nstaging:\nstage: staging\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml\n- ansible-playbook nxos_fabric.yml\n- ansible-playbook verify_fabric.yml\nartifacts:\npaths:\n- overlay.txt\n- underlay.txt\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"push\"  &amp;&amp; $CI_COMMIT_BRANCH != \"main\"'\ndeploy:\nstage: production\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml\n- ansible-playbook nxos_fabric.yml\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\nwhen: manual\n</code></pre> <p>Below screenshot shows the file after added above contents on Atom</p> <p></p> <ul> <li>From Atom, go to File &gt; Save to push the new pipeline to Ansible node</li> </ul> <p>Note, in the above pipeline file, you have configured</p> <ul> <li> <p>Three stages for pipeline named test, staging and production</p> </li> <li> <p>In test stage, pipeline will do dry-run for the ansible playbook using a git-runner with tag EVPN. This script will be triggered by change in branch</p> </li> <li> <p>In staging stage, pipeline will run playbooks to build EVPN fabric for new VNIs, also collrect show outputs files for manual verification. This scripts will be triggered after success of test stage.</p> </li> <li> <p>In production stage, pipeline will run playbook to deploy EVPN fabric for new VNIs in production. This script will be triggered after merge to main branch with manual trigger.</p> </li> </ul> <p>Continuing the lab steps next:</p> <ul> <li>Switch to Ansible node (via MTPuTTY), add the <code>.gitlab-ci.yml</code> file to staging area, and then commit &amp; push the file to remote repository (on GitLab) as shown below.  When prompted for credentials, enter the Username: <code>lab.ciscolive</code> Password: <code>C1sco12345</code>for GitLab access:</li> </ul> <pre><code>git add .gitlab-ci.yml\ngit commit -m \"add ci file\"\ngit push origin main\n</code></pre> <p>Below screenshot shows the outputs of the commands (note your git repository will be based upon your respective Pod_{ID}):</p> <p></p>"},{"location":"task5-day2-pipeline/#step-5-register-local-gitlab-runner","title":"Step 5: Register local gitlab-runner","text":"<p>In this task, you will create local gitlab runner to run pipeline jobs. You will assign EVPN as tag of the runner and register it to your project.</p> <ul> <li>Switch to Chrome brower with gitlab project page.  From the navigation menu on the left side, Select Settings &gt; CI/CD.  Then on the right pane for the Runners, click on Expand setting as shown in below screenshot.</li> </ul> <p>Note</p> <p>Screenshot below uses POD 1 as example, find your assigned POD ID from table in task1.</p> <p></p> <ul> <li>As shown in below screenshot: under the Specific runners, note down the URL of <code>Register the runner with this URL</code>, and the <code>Registration token</code>.  Both of these will be used on next step.   Further, under Shared runners, click the toggle switch named <code>Enable shared runners for this project\"</code> to disable the shared runners for this project.  Below screenshot shows the output once sharing has been disabled.</li> </ul> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), register the GitLab runner by issuing the below command.  Further, when prompted provide the runner registration info as shown below:</li> </ul> <pre><code>gitlab-runner register\n</code></pre> <p>Enter the GitLab instance URL: https://gitlab.com/</p> <p>Enter the registration token:  get registration token from previous step </p> <p>Enter a description for the runner: enter without change </p> <p>Enter tags for the runner (comma-separated): EVPN  (case sensitive)</p> <p>Enter optional maintenance note for the runner: enter without change</p> <p>Enter an executor: shell</p> <p>Below screenshot shows the outputs of the commands:</p> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), run command the below command to start &amp; check status of runner:</li> </ul> <pre><code>gitlab-runner start\ngitlab-runner status\n</code></pre> <p>Below screenshots shown the output of above commands</p> <p></p>"},{"location":"task5-day2-pipeline/#step-6-add-stage-branch-and-add-new-vnis","title":"Step 6: Add stage branch and add new VNIs","text":"<p>In this task, you will modify the variable file to add new networks on EVPN fabric. Instead of applying the change directly to production, you will use the gitlab pipeline to do test and staging first. In order to do that, you will create new branch in your project and push the variable files to the new branch. The push will trigger gitlab-runner to execute scripts configured in pipleline file.</p> <ul> <li>Switch to \"Atom\", Under EVPN-Ansible, scroll to roles &gt; jinja2_leaf &gt; vars and open \"main.yml\" file.   Enter following new VNI information under the existing L2VNI list as shown in the below screenshot:</li> </ul> <pre><code>  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }  </code></pre> <p>Note</p> <p>YAML is case and indentation sensitive, so the above vars/content must be properly added to existing file (Starting from column number 3).  </p> <p>Below screenshot shows the file after addition of above contents on Atom:</p> <p></p> <ul> <li> <p>From Atom, go to File &gt; Save to push the new jinja2 varilabe file to Ansible node.</p> </li> <li> <p>Next, the variables will be added to Leaf role.  From \"Atom\", open \"main.yml\" file under roles &gt; leaf &gt; vars. Enter following new VNI informtion under L2VNI as shown below</p> </li> </ul> <pre><code>   - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }  </code></pre> <p>Note</p> <p>YAML is case and indentation sensitive, so the above vars/content must be properly added to existing file (Starting from column number 3).  Below screenshot shows the file after added above contents on Atom.</p> <p></p> <ul> <li> <p>From Atom, go to File &gt; Save to push the new leaf variable file to Ansible node</p> </li> <li> <p>On the Ansible node (in MTputty SSH session), change directory to EVPN-Ansible project folder</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\n</code></pre> <ul> <li>Run the following git commands to create new branch newvni, <code>commit</code> the updated variable files to that new branch and <code>push</code> to Gitlab project:</li> </ul> <pre><code>git branch -m newvni\ngit add .\ngit commit -m \"newvni\"\ngit push -f origin newvni\n</code></pre> <p>When prompted, Use username: of <code>lab.ciscolive</code> and password: <code>C1sco12345</code> for gitlab access</p> <p></p>"},{"location":"task5-day2-pipeline/#step-7-review-pipeline-testing-and-staging-results","title":"Step 7: Review pipeline testing and staging results","text":"<p>In this task, you will review the script results from pipeline. The git push command in previous task triggered pipeline test and staging stage.</p> <ul> <li>Switch to Chrome brower with GitLab project page.  On the navigation menu on left side, select CI/CD &gt; Pipelines  as shown below:</li> </ul> <p></p> <ul> <li>Wait for the jobs finish, you can also access the runner console by clicking the pipeline stage under Stages and view the Job details as shown below:</li> </ul> <p></p> <ul> <li>After both pipeline stags are passed, you can verify the staging result from the stage artifacts. Click the the three vertical dots (...) on the right most of the <code>pipeline</code> and Download artifacts as shown below.</li> </ul> <p></p> <ul> <li> <p>Unzip the downloaded artifacts.zip on your remote desktop, and you will see overlay.txt and underlay.txt files - these are the outputs from the ansible playbook.</p> </li> <li> <p>Review the overlay show outputs and you can see new vni 50200 and 50201 are deployed on staging environment as shown below:</p> </li> </ul> <pre><code>-   - 'Codes: CP - Control Plane        DP - Data Plane          '\n    - '       UC - Unconfigured         SA - Suppress ARP        '\n    - '       SU - Suppress Unknown Unicast'\n    - ' '\n    - Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\n    - '--------- -------- ----------------- ----- ---- ------------------ -----'\n    - 'nve1      50140    239.0.0.140       Up    CP   L2 [140]                  '\n    - 'nve1      50141    239.0.0.141       Up    CP   L2 [141]                  '\n    - 'nve1      50200    239.0.0.200       Up    CP   L2 [200]                  '\n    - 'nve1      50201    239.0.0.201       Up    CP   L2 [201]                  '\n    - nve1      50999    n/a               Up    CP   L3 [Tenant-1]\n-   - ''\n-   - IP Route Table for VRF \"Tenant-1\"\n    - '''*'' denotes best ucast next-hop'\n    - '''**'' denotes best mcast next-hop'\n    - '''[x/y]'' denotes [preference/metric]'\n    - '''%&lt;string&gt;'' in via output denotes VRF &lt;string&gt;'\n    - ''\n    - '172.21.140.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.140.1, Vlan140, [0/0], 17:56:06, direct'\n    - '172.21.140.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.140.1, Vlan140, [0/0], 17:56:06, local'\n    - '172.21.141.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.141.1, Vlan141, [0/0], 17:56:05, direct'\n    - '172.21.141.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.141.1, Vlan141, [0/0], 17:56:05, local'\n    - '172.21.200.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.200.1, Vlan200, [0/0], 17:56:04, direct'\n    - '172.21.200.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.200.1, Vlan200, [0/0], 17:56:04, local'\n    - '172.21.201.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.201.1, Vlan201, [0/0], 17:56:04, direct'\n    - '172.21.201.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.201.1, Vlan201, [0/0], 17:56:04, local'\n-   - ''\n-   - 'Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link '\n    - (Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\n    - '(Ps):Peer Sync (Ro):Re-Originated '\n    - 'Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      '\n    - '----------- -------------- ------ ---------- --------------- ---------------'\n</code></pre>"},{"location":"task5-day2-pipeline/#step-8-merge-to-main-branch","title":"Step 8: Merge to main branch","text":"<p>After reviewing the test result from staging environment, you confirmed the new NVIs are deployed properly, you will merge the newvni branch to main branch and deploy the new VNIs on production environment.</p> <p>Typically in real world there will (should!) be separate environments for staging and production.  In this lab, we are using same inventory for staging and production environments for simplicity purposes.  So once the merge is done then the changes can be rolled out to production (by applying ansible playbooks)</p> <ul> <li>Switch to Chrome browser with gitlab project page for your respective <code>pod_{ID}</code> repository.  Select Merge requests from navigation menu on the left, and select New merge request as shown below.</li> </ul> <p></p> <ul> <li>Under the <code>Source branch</code> section, from the <code>Select source branch</code> drop down menu select newvni branch.  Further, on the <code>Target branch</code> section, under <code>Select target branch</code> drop down menu make sure that main branch is selected as shown in the screenshot below.  </li> </ul> <p>Note</p> <p>Below screenshot uses POD 1 as example, you must use your assigned POD ID from table in task1.</p> <p></p> <ul> <li> <p>Next, click Compare branches and continue on the above page</p> </li> <li> <p>On the next page, give a Title for the merge request as newvni Pod_<code>#</code> (replace <code>#</code> with your respective Pod ID), leave everything else as default and click Create merge request</p> </li> <li> <p>On the merge request, un-check the Delete source branch after merge option.  You may add Edit merger commit message. And after you have reviewed the merge request, click the Merge button as shown in the screenshot below:</p> </li> </ul> <p></p> <ul> <li> <p>As per the CI pipeline file (named: .gitlab-ci.yml ) the <code>rules</code> was configured for manual trigger for production deployment, hence the job for deploying production will not be triggered automatically.  Instead, you will manually kickoff the production deployment job on pipeline.</p> </li> <li> <p>Switch to Chrome brower with gitlab project page, select CI/CD &gt; Pipelines from navigation menu on the left</p> </li> <li> <p>Deploy the newvni on production enviroment by selecting deploy on the pipeline job from previous merger request as shown in the screenshot below:</p> </li> </ul> <p></p> <ul> <li>Wait for the job finish, you can also access the runner console by clicking the pipeline stage under Stages as shown below:</li> </ul> <p></p>"},{"location":"task5-day2-pipeline/#congratulation-you-have-completed-vxlan-fabric-and-netdevops-automation-lab","title":"Congratulation! You have completed VXLAN Fabric and NetDevOps automation Lab.","text":""},{"location":"task5-day2-pipeline0/","title":"Task 5: Day 2 operation using CI Pipeline","text":"<p>We will implement CI/CD pipeline for day 2 operation tasks in this section.  GitLab and GitLab runner will be used for this purpose</p> <ul> <li>GitLab is software development platform that incorporates multiple capabilities including Version Control, code review and CI/CD in single system. </li> <li>The cloud hosted GitLab (SaaS) is used in this lab.  VCS and CI/CD capabilities of GitLab will be used in this lab. </li> <li>To execute tasks in a private network (i.e. behind Firewall, private infrastructure), GitLab Runner is used.  </li> <li>GitLab Runner is an application that run tests &amp; results i.e., jobs in a pipeline and then send the results to GitLab.  GitLab runner will be installed on the same local server as Ansible host in this lab setup.</li> </ul> <p>As a Version Control System (VCS), a Git repository will created on the GitLab (SaaS).  All the code (Ansible playbooks, roles etc.) will be saved on this repository.  The centralized &amp; Cloud hosted (SaaS) VCS provided by GitLab allows to maintain version control, track changes of files, collaboration among many engineers etc.  In this lab, the CI/CD component of GitLab will be integrated with VCS, so that changes made on repository can automatically trigger execution of job via a pipeline file.</p> <p>The process for creating a CI/CD pipeline with GitLab &amp; GitLab runner include below procedure:</p> <ul> <li>Create a project in GitLab.</li> <li>Install and register the GitLab Runner for your project.</li> <li>Define a CI/CD job (steps, tests) in a file named <code>.gitlab-ci.yml</code> in the root of the repository.</li> <li>Conditions/rules can be defined in <code>.gitlab-ci.yml</code> file (YAML syntax) that will perform a job and display the results in GitLab pipeline e.g. When a commit to repository is done then the runner may execute the job and display the results in GitLab pipeline.</li> </ul> <p>In this section, CI/CD pipeline for day 2 operation tasks will be implemented.  Below steps will be performed:</p> <ul> <li>Create an Ansible playbook to verify underlay and overlay</li> <li>Version control for the EVPN Ansible playbooks using GitLab version control capabilities</li> <li>Create CI pipeline using GitLab</li> <li>Add new VNIs into existing fabric</li> <li>Verify CI pipeline in test and staging stages</li> <li>Commit the merger and verify CI Pipeline in production stage</li> </ul>"},{"location":"task5-day2-pipeline0/#step-1-playbook-to-verify-underlay-and-overlay","title":"Step 1: Playbook to verify underlay and overlay","text":"<p>In this step, you will create the playbook to verify underlay and overlay operation. The playbook will be applied to all leaf switches to verify the below commands:</p> <p>Underlay</p> <pre><code>-   show ip ospf neighbor\n-   show ip bgp sum\n-   show ip pim neighbor\n</code></pre> <p>Overlay</p> <pre><code>-   show nve vni\n-   show nve peer\n-   show ip route vrf Tenant-1\n-   show bgp l2vpn evpn\n-   show l2route evpn mac-ip all\n</code></pre> <ul> <li>Switch to Atom.  On the left page right click on the folder <code>EVPN-Ansible</code> and create a new playbook named <code>verify_fabric.yml</code>.   Enter this file name and hit enter.  Then Copy and Paste the below content to this newly created file:</li> </ul> <pre><code>---\n- hosts: leaf, jinja2_leaf\nconnection: local\ngather_facts: false\ntasks:\n- name: verify underlay\nregister: underlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show ip ospf neighbors\n- show ip bgp sum\n- show ip pim neighbor\ntags: underlay\n- debug: var=underlay_output.stdout_lines\ntags: underlay\n- set_fact:\nsavefile: \"{{underlay_output.stdout_lines | to_nice_yaml}}\"\n- local_action: copy content=\"{{savefile}}\" dest=./underlay.txt\n- name: Verify Overlay\nregister: overlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show nve vni\n- show nve peer\n- show ip route vrf Tenant-1\n- show bgp l2vpn evpn\n- show l2route evpn mac-ip all\ntags: overlay\n- debug: var=overlay_output.stdout_lines\ntags: overlay\n- set_fact:\nsavefile: \"{{overlay_output.stdout_lines | to_nice_yaml}}\"\n- local_action: copy content=\"{{savefile}}\" dest=./overlay.txt\n</code></pre> <ul> <li> <p>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> <li> <p>On the Ansible node (via MTPuTTy), run verify_fabric.yml playbook and verify the output for underlay by executing below command (using respective tag).  This command will show ospf, bgp and pim neighbors for all leaf switches:</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook verify_fabric.yml --tags \"underlay\"\n</code></pre> <ul> <li> <p>Below screenshot shows the partial output of above command and shows ospf, bgp and pim neighbors for all leaf switches:</p> <p></p> </li> </ul> <p>Here is complete log of execution of above playbook/command:</p> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook verify_fabric.yml --tags \"underlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [verify underlay] ***********************************************************************************************************************************************************************************************\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.101] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:12 10.0.0.21       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:12 10.0.128.5      Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.8, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     161     106        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     161     106        6    0    0 01:23:15 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.21       Ethernet1/1          01:23:08  00:01:35  1        yes     n/a     no\",\n            \"10.0.128.5      Ethernet1/2          01:23:07  00:01:31  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.104] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          1d04h    10.0.128.1      Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          1d04h    10.0.128.17     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.11, local AS number 65000\",\n            \"BGP table version is 5, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     672     662        5    0    0 07:03:12 0         \",\n            \"192.168.0.7     4 65000    1441    1433        5    0    0 22:36:01 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.128.1      Ethernet1/1          08:51:43  00:01:28  1        yes     n/a     no\",\n            \"10.0.128.17     Ethernet1/2          22:36:23  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:11 10.0.0.29       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:10 10.0.128.13     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.10, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     148     107        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     150     107        6    0    0 01:23:17 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.29       Ethernet1/1          01:23:09  00:01:37  1        yes     n/a     no\",\n            \"10.0.128.13     Ethernet1/2          01:23:08  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p>Next:</p> <ul> <li>On the Ansible node (via MTPutty), execute the verify_fabric.yml playbook by issuing below command.  As per below, this command uses the <code>--tags</code> in the syntax to execute the respective tasks (as per the tag) in the playbook.  As per the output, verify the overlay outputs (as shown below).  Note: This command will show the nve tunnel peer, host route in BGP EVPN from all leaf switches:</li> </ul> <pre><code>ansible-playbook verify_fabric.yml --tags \"overlay\"\n</code></pre> <ul> <li> <p>Below screenshot of the partial output of above command:</p> <p></p> </li> <li> <p>Below shows the complete log output of execution of above playbook command.   Verify the output for vne vni status, vne dynamic neighbors, mac-ip evpn route update for each L2VNI, l2fib etc. information:</p> </li> </ul> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook verify_fabric.yml --tags \"overlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [Verify Overlay] ************************************************************************************************************************************************************************************************\nok: [198.18.4.104]\nok: [198.18.4.103]\nok: [198.18.4.101]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.104] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.110    Up    CP        01:23:17 000c.2939.f53f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:52, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:23:18, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.11, Vlan141, [190/0], 07:04:08, hmm\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 458, Local Router ID is 192.168.0.11\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"* i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908    (L2VNI 50141)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d HMM    --            0          172.21.141.11  Local\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.111    Up    CP        01:23:20 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:53, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.11, Vlan140, [190/0], 01:23:18, hmm\",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:23:20, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 195, Local Router ID is 192.168.0.10\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 HMM    --            0          172.21.140.11  Local          \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\nok: [198.18.4.101] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.110    Up    CP        01:07:59 000c.2939.f53f   \",\n            \"nve1      192.168.0.111    Up    CP        01:07:59 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.10, Vlan140, [190/0], 01:24:18, hmm\",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 210, Local Router ID is 192.168.0.8\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907    (L2VNI 50140)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"* i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 HMM    --            0          172.21.140.10  Local          \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\nroot@ubuntu:~/EVPN-Ansible#\n</code></pre>"},{"location":"task5-day2-pipeline0/#step-2-install-git-and-gitlab-runner","title":"Step 2: Install git and gitlab runner","text":"<p>In this section you will install git and gitlab runner on Anisble node.</p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below package installation command to install git:</li> </ul> <pre><code>cd ~/EVPN-Ansible\napt-get install git\n</code></pre> <p>Below screenshot shows the execution of the command</p> <p></p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below <code>curl</code> command to download and run a shell script that adds the official GitLab repository required for runner installation:</li> </ul> <pre><code>curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash\n</code></pre> <p>Below screenshot shows the execution of the command</p> <p></p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below command to install <code>gitlab-runner</code></li> </ul> <pre><code>apt-get install gitlab-runner\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-3-version-control-playbooks","title":"Step 3: Version control playbooks","text":"<p>In this step, you will use Gitlab to set version control for the playbooks.</p> <ul> <li> <p>Open chrome browser and enter gitlab.com/users/sign_in in the address bar. Sign in with Username: <code>lab.ciscolive@gmail.com</code> Password: <code>#####</code> as shown below.</p> <p></p> </li> <li> <p>After signing to <code>lab.ciscolive</code> gitlab account, click Create a project from main page as shown below:</p> <p></p> </li> <li> <p>On the next page, click on Create a blank project.  </p> </li> <li> <p>On the next page, enter Project name of your assigned as <code>POD_{ID}</code>.  Note: In the below screenshot of this page:</p> <ul> <li>You must replace the <code>{ID}</code> with your respective pod number.  Find your assigned POD_ID from table in Task1</li> <li><code>Project slug</code> is automatically populated</li> <li>Leave the default setting of <code>Visibility Level</code> to <code>Private</code></li> <li>Leave the default setting of <code>Initialize repository with a README</code> with <code>checkbox</code> selected to automatically add <code>README</code> file in git repository</li> </ul> </li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  Project name of <code>POD_1</code> is shown as an example only.</p> <p>Below screenshot shows execution of above for Pod_1 as a reference only:</p> <p></p> <ul> <li>Then click Create project  at the bottom of this <code>Create a blank project</code> page. </li> </ul> <p>A new project for your respective Pod is now created on GitLab.</p> <ul> <li>On the project page, copy the project url by clicking on Clone button, and then click <code>Copy URL</code> icon next to Clone with HTTPS as shown below in below screenshot.  This project url will be used in next step. The project url looks like https://gitlab.com/lab.ciscolive/pod_{ID}.git.</li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  </p> <p>The screenshot below uses POD_1 as example.</p> <p></p> <p>By default the main branch is protected branch.  In order to add and commit files from git command, the branch settings need to be changed to unprotected.  This change of main branch to unprotected is performed as per below steps and shown in below screenshot:</p> <ul> <li>Navigate on left sidebar (pane) and Select Settings &gt; Repository.  </li> <li>Then on right pane next to Protected branches, Click Expand. </li> <li>Further, Click on Unprotect (in Red color) as shown in below screenshot.  Also, acknolwedge by clicking on Unprotect branch on the pop-up message.</li> </ul> <p></p> <p>Now the Git repository is setup properly for colloboration. And it can be used for keeping all the code i.e., Ansible playbooks and roles files.  Since all the code resides on local machine (Ansible node), so let's push this code to this newlly created centralized repository on GitLab.</p> <ul> <li>On the Ansible node (in MTputty SSH session), run the below commands to initialize git in appropriate directory:</li> </ul> <pre><code>cd ~/EVPN-Ansible\ngit init\n</code></pre> <p>Below screenshot shows the execution of the initialize commands in correct directory:</p> <p></p> <ul> <li>On the Ansible node, run following commands to add the remote repository as origin for your respective <code>Pod {ID}</code> and verify it's result:</li> </ul> <pre><code>git remote add origin https://gitlab.com/lab.ciscolive/pod_{ID}.git\ngit remote -v\n</code></pre> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  </p> <p>Below screenshot shows the execution of the above commands for Pod_1 (as an example):</p> <p></p> <ul> <li>Then to add all the files (Ansible playbooks, roles etc) to the staging area and check the status by using below commands:</li> </ul> <pre><code>git add .\ngit status\n</code></pre> <p>Below screenshot shows the execution of the above commands:</p> <p></p> <ul> <li>Next, commit the files to local repository by executing below commands:</li> </ul> <pre><code>git checkout -b main\ngit commit -m \"initial commit\"\n</code></pre> <ul> <li>Now, lets push all files to main branch on Gitlab repository with below commands.  When prompted for credentials, enter the Username: <code>lab.ciscolive</code> Password: <code>#####</code></li> </ul> <pre><code>git push -f origin main\n</code></pre> <p>Below screenshot shows output of the above command:</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-4-add-cicd-pipeline-file","title":"Step 4: Add CI/CD pipeline file","text":"<p>In this step, you will create CI/CD pipeline file. Gitlab uses a special file named <code>.gitlab-ci.yml</code> for CI/CD configuration.  </p> <p>Note</p> <p>The file name starts with <code>.</code> and it's not a mistake.  </p> <p>This file uses YAML syntax and needs to be placed in root directory of the repository.  In this file, you define your intent for the pipeline using a declarative syntax (in YAML) such as: - The stages you want run in the pipeline - The scripts you want run in each stage - The runner you want use for each script - How is the runner triggered for each script</p> <p>Pipeline file can be added from GitLab UI using pipeline file editor, or from local and push to GitLab repo using git commands.  We will use Atom to add pipeline file in this lab.</p> <ul> <li> <p>Switch to \"Atom\" application on your remote desktop.  Right click on the folder EVPN-Ansible and Click New File to create a new file named .gitlab-ci.yml</p> </li> <li> <p>In the <code>.gitlab-ci.yml</code> file enter the contents shown below:</p> </li> </ul> <pre><code>stages:\n- test\n- staging\n- production\n\nverify:\nstage: test\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml --check\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"push\" &amp;&amp; $CI_COMMIT_BRANCH != \"main\"'\nstaging:\nstage: staging\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml\n- ansible-playbook nxos_fabric.yml\n- ansible-playbook verify_fabric.yml\nartifacts:\npaths:\n- overlay.txt\n- underlay.txt\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"push\"  &amp;&amp; $CI_COMMIT_BRANCH != \"main\"'\ndeploy:\nstage: production\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml\n- ansible-playbook nxos_fabric.yml\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\nwhen: manual\n</code></pre> <p>Below screenshot shows the file after adding the above contents on Atom:</p> <p></p> <ul> <li>From Atom, go to File &gt; Save to push the new pipeline to Ansible node</li> </ul> <p>Note, in the above pipeline file, you have configured</p> <ul> <li> <p>Three stages for pipeline named test, staging and production</p> </li> <li> <p>In test stage, pipeline will perform dry-run for the Ansible playbook using shell command (<code>ansible-playbook</code> with <code>--check</code> flag).  This is executed by using a git-runner with <code>tag</code> of <code>EVPN</code>.  Further, as per the <code>rules</code> setting, the script will be triggered by change in any <code>branch</code> other than <code>main</code>.</p> </li> <li> <p>In staging stage, pipeline will run mutliple playbooks to add new VNIs on the BGP EVPN fabric.  For manual verification, this stage also collects the output of show commands in multiple files - these are referred to as <code>artifacts</code>.  Further, as per the <code>rules</code> setting, this scripts will be triggered after success of <code>test</code> stage and for any <code>branch</code> other than <code>main</code>.</p> </li> <li> <p>In production stage, pipeline will run playbook to deploy EVPN fabric for new VNIs in production. Further, as per the <code>rules</code> setting, this script will be triggered after merge to main branch with manual trigger.</p> </li> </ul> <p>Now lets push this file to GitLab centralized repository by execution of below steps:</p> <ul> <li>Switch to Ansible node (via MTPuTTY), add the <code>.gitlab-ci.yml</code> file to staging area, and then commit &amp; push the file to remote repository (on GitLab) as shown below.  When prompted for credentials, enter the Username: <code>lab.ciscolive</code> Password: <code>#####</code>for GitLab access:</li> </ul> <pre><code>git add .gitlab-ci.yml\ngit commit -m \"add ci file\"\ngit push origin main\n</code></pre> <p>Below screenshot shows the outputs of the commands for Pod_1 (note your git repository will be based upon your respective Pod_{ID}):</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-5-register-local-gitlab-runner","title":"Step 5: Register local gitlab-runner","text":"<p>In this task, you will create local gitlab runner on Ansible server and register it to your project on GitLab (SaaS) to run pipeline jobs. You will assign EVPN as tag of the runner as part of registration to your project on GitLab.</p> <ul> <li>Switch to Chrome brower with gitlab project page.  From the navigation menu on the left sidebar, Select Settings &gt; CI/CD.  Then on the right pane for the <code>Runners</code>, click on Expand setting as shown in below screenshot.</li> </ul> <p>Note</p> <p>Screenshot below uses POD 1 as example, find your assigned POD ID from table in task1.</p> <p></p> <ul> <li>After expanding the <code>Runners</code> setting, under <code>Project runners</code> click on New project runner.  </li> </ul> <p></p> <ul> <li>On the next <code>New project runner</code> page, select Linux Operating systems under the <code>Platform</code>.  Further, enter a <code>Tags</code> value of EVPN, leave other settings as default, and click Submit on this page as shown in below screenshot:</li> </ul> <p></p> <ul> <li> <p>On the next <code>Register runner</code> page, under <code>Step 1</code>, copy the runner token as part of the command line (<code>gitlab-runner register ...</code>) .  This token will be used later to register the gitlab runner that runs on Ansible server/node.  </p> </li> <li> <p>Then click on Go to runners page.  If <code>Leave site?</code> prompt is displayed, then click <code>Leave</code> to proceed.</p> </li> </ul> <p>Note</p> <p>Make sure to copy the runner token in this step.</p> <p></p> <ul> <li>As shown in below screenshot, under <code>Shared runners</code>, click on the toggle switch named Enable shared runners for this project to disable the shared runners for this project.  Below screenshot shows the output once sharing has been disabled.</li> </ul> <p></p> <ul> <li> <p>Next switch to Ansible node (in MTputty SSH session) to register the GitLab runner by issuing the below command.  As part of this registration process, below settings are configured:</p> <ul> <li><code>GitLab URL</code> pointing to the GitLab running as SaaS. </li> <li><code>Registration Token</code> for (authentication) of runner to Gitlab project.</li> <li><code>Tags</code> - When a CI/CD job runs, it knows which runner to use by looking at the assigned tags.  It allows to filter a runner from a list of available runners for a job.  In this lab only a single runner is used.</li> <li><code>Executor</code>: It determines the environment where the job runs in.  In this case Ansible playbooks are executed on a <code>shell</code> enviornment.</li> </ul> </li> </ul> <pre><code>gitlab-runner register\n</code></pre> <p>When prompted provide the runner registration info as shown below:</p> <ul> <li> <p><code>Enter the GitLab instance URL:</code> https://gitlab.com/</p> </li> <li> <p><code>Enter the registration token:</code>  Paste the <code>runner token</code> value generated and copied on GitHub (previous step)</p> </li> <li> <p><code>Enter a name for the runner:</code> EVPN</p> </li> <li> <p><code>Enter an executor:</code> shell</p> </li> </ul> <p>After entering <code>shell</code>, wait few seconds for the registration of the runner.  Below screenshot shows the outputs of the commands:</p> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), run command the below commands to check status of runner:</li> </ul> <pre><code>gitlab-runner status\ngitlab-runner list\n</code></pre> <p>Below screenshots shown the output of above commands</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-6-add-new-vnis-in-staging-and-trigger-pipeline","title":"Step 6: Add new VNIs in Staging and trigger pipeline","text":"<p>In this task, you will modify the variable files to add new networks on the EVPN fabric.  Instead of applying the changes (new VNIs) directly to <code>main</code> repository, you will create a new repository named <code>newvni</code> on gitlab to validate and test it on a staging environment.  Once the changes (new VNIs) are pushed to <code>newvni</code> branch on the GitLab repository, it will automatically trigger execution of scripts, by gitlab runner, as per configuration of pipleline file (.gitlab-ci.yml).  Keep in mind that we only have one physical environment i.e., same set of switches are used for both staging and production.</p> <ul> <li>Switch to \"Atom\".  Under EVPN-Ansible, scroll to roles &gt; jinja2_leaf &gt; vars and open \"main.yml\" file.  Enter the new VNI details in this file below the existing L2VNI list as shown in the below screenshot:</li> </ul> <pre><code>  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }  </code></pre> <p>Note</p> <p>Do not completely replace existing content in this file.  Above content should be added to the end of existing L2VNI and above the existing L3VNI variables.</p> <p>Below screenshot shows the file after addition of above contents on Atom:</p> <p></p> <ul> <li> <p>From Atom, go to File &gt; Save to push the new variables to Ansible node.</p> </li> <li> <p>Next, variables for Leaf role will be added.  From \"Atom\", open \"main.yml\" file under roles &gt; leaf &gt; vars. Enter following new VNI informtion under L2VNI as shown in below screenshot:</p> </li> </ul> <pre><code>  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }\n</code></pre> <p>Note</p> <p>Do not completely replace existing content in this file.  Above content should be added to the end of existing L2VNI and above the existing L3VNI variables.</p> <p>Below screenshot shows the file after addition of above contents on Atom:</p> <p></p> <ul> <li> <p>From Atom, go to File &gt; Save to push the new leaf variable file to Ansible node.</p> </li> <li> <p>On the Ansible node (in MTputty SSH session), change directory to EVPN-Ansible project folder</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\n</code></pre> <ul> <li>Run the following git commands to create new branch newvni, <code>commit</code> the updated variable files to that new branch and <code>push</code> to Gitlab project:</li> </ul> <pre><code>git branch -m newvni\ngit add .\ngit commit -m \"newvni\"\ngit push -f origin newvni\n</code></pre> <p>When prompted, Use username: of <code>lab.ciscolive</code> and password: <code>#####</code> for gitlab access</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-7-review-pipeline-testing-and-staging-results","title":"Step 7: Review pipeline testing and staging results","text":"<p>In this task, you will review the script results from pipeline. The git push command in previous task triggered pipeline test and staging stage.</p> <ul> <li>Switch to Chrome brower with GitLab project page.  On the navigation menu on left side, select CI/CD &gt; Pipelines  as shown below:</li> </ul> <p></p> <ul> <li>Wait for the jobs finish, you can also access the runner console by clicking on any of the stage under Stages heading/column on this page and view the Job details as shown below:</li> </ul> <p></p> <ul> <li>After both pipeline stages are passed, you can verify the result from the artifacts.  Click on the down arrow on the right most of the <code>pipeline</code> page and select Download artifacts to download the <code>staging:artifacts</code> as zip file.</li> </ul> <p></p> <ul> <li> <p>Unzip the downloaded artifacts.zip on your remote desktop, and you will see overlay.txt and underlay.txt files - these are the outputs from the ansible playbook.</p> </li> <li> <p>Review the <code>overlay.txt</code> file and you can see new VNIs 50200 and 50201 deployed on staging environment as shown in partial output below:</p> </li> </ul> <pre><code>-   - 'Codes: CP - Control Plane        DP - Data Plane          '\n    - '       UC - Unconfigured         SA - Suppress ARP        '\n    - '       SU - Suppress Unknown Unicast'\n    - ' '\n    - Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\n    - '--------- -------- ----------------- ----- ---- ------------------ -----'\n    - 'nve1      50140    239.0.0.140       Up    CP   L2 [140]                  '\n    - 'nve1      50141    239.0.0.141       Up    CP   L2 [141]                  '\n    - 'nve1      50200    239.0.0.200       Up    CP   L2 [200]                  '\n    - 'nve1      50201    239.0.0.201       Up    CP   L2 [201]                  '\n    - 'nve1      50203    239.0.0.203       Up    CP   L2 [203]                  '\n    - 'nve1      50204    239.0.0.204       Up    CP   L2 [204]                  '\n    - nve1      50999    n/a               Up    CP   L3 [Tenant-1]\n-   - 'Interface Peer-IP          State LearnType Uptime   Router-Mac       '\n    - '--------- ---------------  ----- --------- -------- -----------------'\n    - nve1      192.168.0.18     Up    CP        1d15h    000c.2997.621c\n-   - IP Route Table for VRF \"Tenant-1\"\n    - '''*'' denotes best ucast next-hop'\n    - '''**'' denotes best mcast next-hop'\n    - '''[x/y]'' denotes [preference/metric]'\n    - '''%&lt;string&gt;'' in via output denotes VRF &lt;string&gt;'\n    - ''\n    - '172.21.140.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.140.1, Vlan140, [0/0], 1d15h, direct'\n    - '172.21.140.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.140.1, Vlan140, [0/0], 1d15h, local'\n    - '172.21.140.10/32, ubest/mbest: 1/0'\n    - '    *via 192.168.0.18%default, [200/0], 1d13h, bgp-65000, internal, tag 65000\n        (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN'\n    - ' '\n    - '172.21.141.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.141.1, Vlan141, [0/0], 1d15h, direct'\n    - '172.21.141.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.141.1, Vlan141, [0/0], 1d15h, local'\n    - '172.21.200.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.200.1, Vlan200, [0/0], 1d15h, direct'\n    - '172.21.200.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.200.1, Vlan200, [0/0], 1d15h, local'\n    - '172.21.201.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.201.1, Vlan201, [0/0], 1d15h, direct'\n    - '172.21.201.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.201.1, Vlan201, [0/0], 1d15h, local'\n    - '172.21.203.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.203.1, Vlan203, [0/0], 1d13h, direct'\n    - '172.21.203.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.203.1, Vlan203, [0/0], 1d13h, local'\n    - '172.21.204.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.204.1, Vlan204, [0/0], 1d13h, direct'\n    - '172.21.204.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.204.1, Vlan204, [0/0], 1d13h, local'\n\n[output omitted]\n</code></pre>"},{"location":"task5-day2-pipeline0/#step-8-merge-code-to-main-branch","title":"Step 8: Merge code to main branch","text":"<p>After reviewing the test result from staging environment, you confirmed the new NVIs are deployed properly, you will merge the newvni branch to main branch and deploy the new VNIs on production environment.</p> <p>Typically in real world there will (should!) be separate environments for staging and production.  In this lab, we are using same inventory for staging and production environments for simplicity purposes.  So once the merge is done then the changes can be rolled out to production (by applying ansible playbooks).</p> <ul> <li>Switch to Chrome browser with GitLab project page for your respective <code>Pod_{ID}</code> and then select Merge requests in the left sidebar.  Select New merge request on the right pane as shown in below screenshot.</li> </ul> <p></p> <ul> <li>Under the <code>Source branch</code> section, from the <code>Select source branch</code> drop down menu select newvni branch.  </li> <li>Further, on the <code>Target branch</code> section, under <code>Select target branch</code> drop down menu make sure that main branch is selected as shown in the screenshot below.</li> </ul> <p>Note</p> <p>Below screenshot uses POD 1 as example, you must use your assigned POD ID from table in task1.</p> <p></p> <ul> <li>Then click Compare branches and continue on this page.</li> </ul> <p>Next, On the New merge request page (below screenshot):</p> <ul> <li> <p>Enter the <code>Title</code> for the merge request as Newvni Pod_<code>#</code> (replace <code>#</code> with your respective Pod ID),</p> </li> <li> <p>And un-check the Delete source branch when merge request is accepted,</p> </li> <li> <p>Then click the Create merge request button as shown in the screenshot below:</p> </li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  Project name of <code>POD_1</code> in below screenshot is shown as an example only.</p> <p></p> <p>On the next page (below screenshot), you are displayed with the details related to Merge: </p> <ul> <li>Review the details and then click on Merge button.   </li> </ul> <p></p> <p>Wait for the <code>Merged by lab.ciscolive</code> message and for <code>Merge details</code> to appear on this page.</p>"},{"location":"task5-day2-pipeline0/#so-what-is-happening","title":"So what is happening?","text":"<p>After validating and testing deployment of VNIs in a branch named <code>newvni</code>, we have merged the new code (to deploy new VNIs) on the <code>main</code> branch.  However, the pipeline has not triggered.  Why?  Since, as per the CI/CD pipeline (filename: .gitlab-ci.yml) the <code>rules</code> setting was configured for manual trigger for production deployment.  This is just to demonstrate that, as per your environment, you may decide to be extra cautios and not deploy to production without additional review.  Hence the job for deploying production will not be triggered automatically.  Instead, you will manually kickoff the production deployment job on pipeline.  Let's proceed to initiate this manual trigger:</p> <ul> <li> <p>Switch to Chrome brower with GitLab project page (your respective <code>Pod_{ID}</code>), select CI/CD &gt; Pipelines from navigation menu on the left sidebar.</p> </li> <li> <p>Deploy the changes to production enviroment by selecting on the <code>Play</code> button and then click on deploy as shown in the screenshot below:</p> </li> </ul> <p></p> <ul> <li>Wait for the job (all the Stages) to finish. You can also access the runner console output, to see execution of Ansible playbooks by clicking the pipeline stage named <code>deploy</code> under Stages on this page.  </li> </ul> <p>Note</p> <p><code>POD_1</code> in below screenshot is shown as an example only.  You must browse through to your own Pod #.</p> <p>Below screenshot shows the successful execution of this deploy stage: </p> <p></p>"},{"location":"task5-day2-pipeline0/#congratulation-you-have-completed-vxlan-fabric-and-netdevops-automation-lab","title":"Congratulation! You have completed VXLAN Fabric and NetDevOps automation Lab.","text":""},{"location":"pic/Readme/","title":"Readme","text":"<p>pic files</p>"}]}