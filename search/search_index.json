{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LTRDCN-2765","text":"<p>Welcome to Cisco Live LTRDCN-2765: VXLAN EVPN Fabric and NetDevOps/automation using Ansible.</p> <p>For full documentation visit Cisco Live.</p>"},{"location":"#speakers","title":"Speakers","text":"<ul> <li>Faisal Chaudhry <code>Principal Architect, Cisco CX</code></li> <li>Lei Tian <code>Solutions Architect, Cisco CX</code></li> </ul>"},{"location":"Guide-lab_access/","title":"Lab Devices Access","text":"<p>Below table provides the IP addresses and credentials for the devices used in this lab:</p> Device SSH Console (telnet IP:port) Credentials (user/password) Spine-1 198.18.4.201 198.18.133.33:1030 admin/C1sco12345 Spine-2 198.18.4.202 198.18.133.33:1040 admin/C1sco12345 Leaf-1 198.18.4.101 198.18.133.33:1050 admin/C1sco12345 Leaf-3 198.18.4.103 198.18.133.33:1070 admin/C1sco12345 Leaf-4 198.18.4.104 198.18.1333.33:1080 admin/C1sco12345 Server-1 198.18.134.50 root/C1sco12345 Server-3 198.18.134.52 root/C1sco12345 Server-4 198.18.134.53 root/C1sco12345 Ansible Server 198.18.134.150 root/C1sco12345 Remote Workstation 198.18.133.36 demouser/C1sco12345"},{"location":"Guide-lab_topology/","title":"Lab Topology","text":"<p>Network topology used in this lab is shown in below picture:</p> <p></p>"},{"location":"Task1-ansible-node0/","title":"Task1 ansible node0","text":"<p>Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.</p>"},{"location":"Task1-ansible-node0/#step-1-connect-to-lab-using-anyconnect-vpn","title":"Step 1: Connect to lab using anyconnect VPN","text":"<p>You will connect to Anyconnect url using Cisco VPN AnyConnect client and with the username &amp; password as documented in below table.  Below screenshot shows an example of that VPN connection.</p> Pod ID Attendee Name Anyconnect url Anyconnect Username Anyconnect Password POD  1 dcloud-lon-anyconnect.cisco.com POD  2 dcloud-lon-anyconnect.cisco.com POD  3 dcloud-lon-anyconnect.cisco.com POD  4 dcloud-lon-anyconnect.cisco.com POD  5 dcloud-lon-anyconnect.cisco.com POD  6 dcloud-lon-anyconnect.cisco.com POD  7 dcloud-lon-anyconnect.cisco.com POD  8 dcloud-lon-anyconnect.cisco.com POD  9 dcloud-lon-anyconnect.cisco.com POD  10 dcloud-lon-anyconnect.cisco.com POD  11 dcloud-lon-anyconnect.cisco.com POD  12 dcloud-lon-anyconnect.cisco.com POD  13 dcloud-lon-anyconnect.cisco.com POD  14 dcloud-lon-anyconnect.cisco.com POD  15 dcloud-lon-anyconnect.cisco.com POD  16 dcloud-lon-anyconnect.cisco.com POD  17 dcloud-lon-anyconnect.cisco.com POD  18 dcloud-lon-anyconnect.cisco.com <p>Note</p> <p>lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.</p> <p></p>"},{"location":"Task1-ansible-node0/#step-2-enter-vpn-credentials","title":"Step 2: Enter VPN credentials","text":"<p>After prompted for credentials, use the credentials documented in above table or provided by the lab admin.</p> <ul> <li>Below is an example of user logging into a reference POD:</li> </ul> <p></p> <ul> <li>Hit accept when the prompt appears to accept the VPN connection login</li> </ul> <p></p>"},{"location":"Task1-ansible-node0/#step-3-rdp-to-workstation","title":"Step 3: RDP to workstation","text":"<p>In this step, you will connect to the workstation with Remote Desktop (RDP) client on your machines.  Use below details for this RDP session:</p> <ul> <li>Workstation: 198.18.133.36</li> <li>Username: dclouddemouser</li> <li>Password: C1sco12345</li> </ul> <p>Below screenshot is only an example for this RDP connection:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-4-ssh-client---mtputty","title":"Step 4: SSH client - MTputty","text":"<p>Once you have the RDP session to the remote workstation, then you will use MTPutty client to connect to all devices (Nexus and Ansible server/node) in this lab.</p> <p>MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking the MTPutty icon on the desktop:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-5-ssh-into-ansible-node","title":"Step 5: SSH into Ansible node","text":"<p>In the MTPutty client, SSH to Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pane.</p> <p>The passwords are pre-configured however if prompted then use credentials of username root and password C1sco12345 for SSH access</p> <p></p> <p>On this Ansible node various software packages will be installed from public repositories on the Internet.</p>"},{"location":"Task1-ansible-node0/#step-6-verify-python","title":"Step 6: Verify Python","text":"<p>Ansible can on any machine that has Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansible.</p> <p>Hence, once successfully SSH into the ansible node, verify that python version 2.7 is pre-installed by running below command:</p> <pre><code>python --version\n</code></pre> <p>The output of above command confirms python version installed on Ansible node.</p> <p></p>"},{"location":"Task1-ansible-node0/#step-7-update-ubuntu-package-tool-and-install-pip--packages","title":"Step 7: Update Ubuntu package tool and install PIP &amp; Packages","text":"<ul> <li>After verifying we have the minimum version of python installed, we are now going to update Ubuntu package tool (using <code>apt-get update</code>) </li> </ul> <pre><code>apt-get update\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Next, installation of PIP python package (using <code>apt install pip</code>) command is done.  PIP is a package manager that is used to install and maintain Python software packages.</li> </ul> <p>Note: At the <code>Do you want to continue? [Y/n]</code> prompt, you must enter <code>Y</code> to proceed with update/install:</p> <pre><code>apt install pip\n</code></pre> <p>Below screenshot shows the partial execution of above command:</p> <p></p> <ul> <li>Next, upgrade pip by executing below command:</li> </ul> <pre><code>pip install --upgrade pip\n</code></pre> <p>Note</p> <p>You can ignore the warning related to python version.  Below screenshot shows the execution of above command.</p> <p></p> <p>PIP package will be used to update pyopenssl package and install following python packages that are required for Ansible NXOS modules:</p> <ul> <li>paramiko</li> <li>PyYAML</li> <li>jinj2</li> <li>httplib2</li> <li> <p>ansible-pylibssh</p> </li> <li> <p>Run the below command to update pyopenssl package:</p> </li> </ul> <pre><code>pip install pyopenssl --upgrade\n</code></pre> <p>Below screenshot shows the execution of the above command:</p> <p></p> <ul> <li>Run the below command to install the above listed required packages:</li> </ul> <pre><code>pip install paramiko PyYAML jinja2 httplib2 ansible-pylibssh\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-8-ansible-installation","title":"Step 8: Ansible Installation","text":"<p>In this step, Ansible and the required modules for this lab are going to be installed on this node/server (running Ubuntu).</p> <ul> <li>Initiate the installation of ansible using below command:</li> </ul> <p>Note</p> <p>It may take few minutes for it to download and install.  You can ignore the pip WARNING, related to root user, after successful installation of ansible.</p> <pre><code>pip install ansible==2.10.6\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Once the installation is complete, check Ansible version by executing below command:</li> </ul> <pre><code>ansible --version\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <p>Note</p> <p>Your output might show different version than the installed version. This is due to the separation of ansible-base (ansible-core) and community package starting from version 2.10. The above command shows ansible-base version. You can find more detail explanation from Ansible documentation page https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html</p> <ul> <li>Starting from Ansible 2.9, plugins and modules for network platform are moved in to Collection.  To install NX-OS collection 2.9.1 with Ansible Galaxy by using below <code>ansible-galaxy ...</code> command:</li> </ul> <pre><code>ansible-galaxy collection install -f cisco.nxos:4.1.0\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-9-create-ansible-settings-inventory-and-variables","title":"Step 9: Create Ansible Settings, Inventory and variables","text":"<p>Ansible server uses host inventory to communicate with target devices (called as hosts). Like other programming languages, ansible uses variables to represent variations among different target devices.   This is important as Ansible works against multiple systems by selecting portions of systems listed in Ansible inventory.  Additionally, Ansible settings such as how SSH keys verification is done, name/location of inventory file to be used etc are adjustable via a configuration file named <code>ansible.cfg</code>.  It is an INI based file.</p> <ul> <li>Create folder named EVPN-Ansible as working environment and verify that it\u2019s empty by issuing below commands:</li> </ul> <pre><code>mkdir EVPN-Ansible &amp;&amp; cd EVPN-Ansible\nls\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>Next:</p> <ul> <li>Create Ansible inventory file to include Spine and Leaf switches.</li> <li>By default Ansible has inventory file saved in location /etc/ansible/hosts.</li> <li>In this lab we will create hosts file in the working environment. This file is created from ansible host prompt using <code>cat</code> command.   </li> </ul> <p>Note</p> <p>You should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code> (also shown in subsequent screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; hosts\n#define global variables, groups and host variables\n[all:vars]\nansible_connection = ansible.netcommon.network_cli\nansible_network_os = cisco.nxos.nxos\nansible_user=admin\nansible_password=C1sco12345\nansible_command_timeout=180\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\nEOF\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more hosts\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create Ansible config (ansible.cfg) using the same steps as above.  </li> </ul> <p>Note</p> <p>you should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code>  (as shown in below screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ansible.cfg\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\ndeprecation_warnings = False\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more ansible.cfg\n</code></pre> <p>Below screenshot shows the output of above commands:</p> <p></p> <ul> <li>To verify the file that you just created under project folder EVPN-Ansible, issue the list command:</li> </ul> <pre><code>ls\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create host variable folder named <code>host_vars</code> in folder <code>EVPN-Ansible</code> by using below command.  In this lab, we will use host_vars file to define the variables for various hosts (in next bullets):</li> </ul> <pre><code>mkdir host_vars &amp;&amp; cd host_vars\n</code></pre> <ul> <li>Create host variable file for each host in inventory by using the cat command.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.101.yml\n---\nhostname: leaf_1\nloopback0: 192.168.0.8\nloopback1: 192.168.0.18\nrouter_id: 192.168.0.8\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.101.yml</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.101.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.   The variable file for a switch is created using the below <code>cat</code> command:</li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot).  The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.103.yml\n---\nhostname: leaf_3\nloopback0: 192.168.0.10\nloopback1: 192.168.0.110\nrouter_id: 192.168.0.10\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.103.yml</code> as shown below:</li> </ul> <pre><code>more 198.18.4.103.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.104.yml\n---\nhostname: leaf_4\nloopback0: 192.168.0.11\nloopback1: 192.168.0.111\nrouter_id: 192.168.0.11\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.104.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.201.yml\n---\nhostname: spine-1\nloopback0: 192.168.0.6\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.6\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.201.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.202.yml\n---\nhostname: spine-2\nloopback0: 192.168.0.7\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.7\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.202.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p>"},{"location":"Task1-ansible-node0/#step-10-ansible-role-structure","title":"Step 10: Ansible role structure","text":"<p>Role is very useful technique to manage a set of playbooks in Ansible. In this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches.</p> <p>A role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.  Here are few folders:</p> <ul> <li><code>main.yml</code> file in <code>/vars</code>  folder contains variables for a role</li> <li><code>main.yml</code> file in <code>/tasks</code> folder contains Ansible playbook for a role</li> <li><code>templates</code> folder may contain Jinja2 files, if these are used for a role </li> </ul> <p>To proceed further with roles in subsequent Tasks:</p> <ul> <li>Create roles directory in folder EVPN-Ansible by issuing below commands:</li> </ul> <pre><code>cd /root/EVPN-Ansible\nmkdir roles\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>This will be used in the subsequent tasks in this lab.</p>"},{"location":"intro/","title":"Overview","text":""},{"location":"intro/#vxlan","title":"VXLAN","text":"<p>VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments. VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. Below are some of the terminologies that will be used in the lab:</p> <ul> <li>VNI / VNID \u2013 VXLAN Network Identifier. This replaces VLAN ID</li> <li>VTEP \u2013 VXLAN Tunnel End Point.<ul> <li>This is the end point where the box performs VXLAN encap / decap This could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)</li> </ul> </li> <li>VXLAN Segment -  The resulting layer 2 overlay network</li> <li>VXLAN Gateway \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding</li> <li>NVE \u2013 Network Virtualization Edge<ul> <li>NVE is tunnel interface. It represents VTEP</li> </ul> </li> </ul>"},{"location":"intro/#ansible","title":"Ansible","text":"<p>Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module.</p> <p>You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html Below are some of the terminologies that will be used in the lab:</p> <ul> <li>Host: remote machines that Ansible manages  </li> <li>Group: several hosts that can be configured together and share common verables</li> <li>Inventory: file descripts hosts and groups in Ansible.</li> <li>Variable: names of value (int, str, dic, list) referenced in playbook or template</li> <li>YAML: data format for Playbook or Variables in Ansible</li> <li>Playbook: the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays.</li> <li>Roles: group of tasks, templates to implement specific behavior</li> <li>Jinja2: a Python based templating language</li> </ul> <p></p>"},{"location":"intro/#ci-pipeline-and-gitlab","title":"CI Pipeline and GitLab","text":"<p>CI or continuous integration is core principle in DevOps practices. Software developers keep code in central repository and make changes multiples times a day. Every change in CI triggers automated build and test for the project. In NetDevOps, validation and test are even more important; and configuration change or adding new service needs to be validate and tested in staging or lab enviroment.</p> <p>GitLab is an opensource DevOps platform for software development. GitLab provides the complete solution including configure, monitor, verify, package etc. For this lab exercise, you will use the verison control and CI pipeline functions from GitLab.  </p>"},{"location":"intro/#about-this-lab","title":"About this lab","text":"<p>As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode. As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. This lab demonstrates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations.</p>"},{"location":"intro/#lab-flow","title":"Lab Flow","text":"<p>Lab guide will walk the attendees through the below activities:</p> <ol> <li>Installation of Ansible on a server</li> <li>Use of Ansible playbooks</li> <li>Day 1 automation using Ansible</li> <li>CI Pipeline and Day 2 automation using Ansible</li> </ol>"},{"location":"task1-VPN/","title":"Task 1 - VPN connection","text":"<p>You will initiate VPN connection to the network setup used in this lab.  All the devices in this lab are running as Virtual Machines using VMware as Hypervisor.  At the end of this task, you will have VPN connection to this remote setup</p>"},{"location":"task1-VPN/#step-1-connect-to-lab-using-anyconnect-vpn","title":"Step 1: Connect to lab using anyconnect VPN","text":"<p>You will connect to Anyconnect url using Cisco VPN AnyConnect client and with the username &amp; password as documented in below table.  Below screenshot shows an example of that VPN connection.</p> Pod ID Attendee Name Anyconnect url Anyconnect Username Anyconnect Password POD  1 dcloud-lon-anyconnect.cisco.com POD  2 dcloud-lon-anyconnect.cisco.com POD  3 dcloud-lon-anyconnect.cisco.com POD  4 dcloud-lon-anyconnect.cisco.com POD  5 dcloud-lon-anyconnect.cisco.com POD  6 dcloud-lon-anyconnect.cisco.com POD  7 dcloud-lon-anyconnect.cisco.com POD  8 dcloud-lon-anyconnect.cisco.com POD  9 dcloud-lon-anyconnect.cisco.com POD  10 dcloud-lon-anyconnect.cisco.com POD  11 dcloud-lon-anyconnect.cisco.com POD  12 dcloud-lon-anyconnect.cisco.com POD  13 dcloud-lon-anyconnect.cisco.com POD  14 dcloud-lon-anyconnect.cisco.com POD  15 dcloud-lon-anyconnect.cisco.com POD  16 dcloud-lon-anyconnect.cisco.com POD  17 dcloud-lon-anyconnect.cisco.com POD  18 dcloud-lon-anyconnect.cisco.com <p>Note</p> <p>lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.</p> <p></p>"},{"location":"task1-VPN/#step-2-enter-vpn-credentials","title":"Step 2: Enter VPN credentials","text":"<p>After prompted for credentials, use the credentials documented in above table or provided by the lab admin.</p> <ul> <li>Below is an example of user logging into a reference POD:</li> </ul> <p></p> <ul> <li>Hit accept when the prompt appears to accept the VPN connection login</li> </ul> <p></p>"},{"location":"task1-VPN/#step-3-rdp-to-workstation","title":"Step 3: RDP to workstation","text":"<p>In this step, you will connect to the workstation with Remote Desktop (RDP) client on your machines.  Use below details for this RDP session:</p> <ul> <li>Workstation: 198.18.133.36</li> <li>Username: dclouddemouser</li> <li>Password: C1sco12345</li> </ul> <p>Below screenshot is only an example for this RDP connection:</p> <p></p>"},{"location":"task1-VPN/#step-4-ssh-client---mtputty","title":"Step 4: SSH client - MTputty","text":"<p>Once you have the RDP session to the remote workstation, then you will use MTPutty client to connect to all devices (Nexus and Ansible server/node) in this lab.</p> <p>MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking the MTPutty icon on the desktop:</p> <p></p>"},{"location":"task2-ansible-node/","title":"Task 2 - Ansible installation","text":"<p>Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.</p>"},{"location":"task2-ansible-node/#step-1-ssh-client---mtputty","title":"Step 1: SSH client - MTputty","text":"<p>Once you have the RDP session to the remote workstation, then you will use MTPutty client to connect to all devices (Nexus and Ansible server/node) in this lab.</p> <p>MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking the MTPutty icon on the desktop:</p> <p></p>"},{"location":"task2-ansible-node/#step-2-ssh-into-ansible-node","title":"Step 2: SSH into Ansible node","text":"<p>In the RDP session, launch the MTPutty client.  Then SSH to Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pane.</p> <p>The passwords are pre-configured however if prompted then use credentials of username root and password C1sco12345 for SSH access</p> <p></p> <p>On this Ansible node various software packages will be installed from public repositories on the Internet.</p>"},{"location":"task2-ansible-node/#step-3-verify-python","title":"Step 3: Verify Python","text":"<p>Ansible can run on any machine that has Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansible.</p> <p>Hence, once successfully SSH into the ansible node, verify that python version 2.7 is pre-installed by running below command:</p> <pre><code>python --version\n</code></pre> <p>The output of above command confirms python version installed on Ansible node.</p> <p></p>"},{"location":"task2-ansible-node/#step-4-update-ubuntu-package-tool-and-install-pip--packages","title":"Step 4: Update Ubuntu package tool and install PIP &amp; Packages","text":"<ul> <li>After verifying we have the minimum version of python installed, we are now going to update Ubuntu package tool (using <code>apt-get update</code>) </li> </ul> <pre><code>apt-get update\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Next, installation of PIP python package (using <code>apt install pip</code>) command is done.  PIP is a package manager that is used to install and maintain Python software packages.</li> </ul> <p>Note: At the <code>Do you want to continue? [Y/n]</code> prompt, you must enter <code>Y</code> to proceed with update/install:</p> <pre><code>apt install pip\n</code></pre> <p>Below screenshot shows the partial execution of above command:</p> <p></p> <ul> <li>Next, upgrade pip by executing below command:</li> </ul> <pre><code>pip install --upgrade pip\n</code></pre> <p>Note</p> <p>You can ignore the warning related to python version.  Below screenshot shows the execution of above command.</p> <p></p> <p>PIP package will be used to update pyopenssl package and install following python packages that are required for Ansible NXOS modules:</p> <ul> <li>paramiko</li> <li>PyYAML</li> <li>jinj2</li> <li>httplib2</li> <li> <p>ansible-pylibssh</p> </li> <li> <p>Run the below command to update pyopenssl package:</p> </li> </ul> <pre><code>pip install pyopenssl --upgrade\n</code></pre> <p>Below screenshot shows the execution of the above command:</p> <p></p> <ul> <li>Run the below command to install the above listed required packages:</li> </ul> <pre><code>pip install paramiko PyYAML jinja2 httplib2 ansible-pylibssh\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"task2-ansible-node/#step-5-ansible-installation","title":"Step 5: Ansible Installation","text":"<p>In this step, Ansible and the required modules for this lab are going to be installed on this node/server (running Ubuntu).</p> <ul> <li>Initiate the installation of ansible using below command:</li> </ul> <p>Note</p> <p>It may take few minutes for it to download and install.  You can ignore the pip WARNING, related to root user, after successful installation of ansible.</p> <pre><code>pip install ansible==2.10.6\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Once the installation is complete, check Ansible version by executing below command:</li> </ul> <pre><code>ansible --version\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <p>Note</p> <p>Your output might show different version than the installed version. This is due to the separation of ansible-base (ansible-core) and community package starting from version 2.10. The above command shows ansible-base version. You can find more detail explanation from Ansible documentation page https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html</p> <ul> <li>Starting from Ansible 2.9, plugins and modules for network platform are moved in to Collection.  To install NX-OS collection 2.9.1 with Ansible Galaxy by using below <code>ansible-galaxy ...</code> command:</li> </ul> <pre><code>ansible-galaxy collection install -f cisco.nxos:4.1.0\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p>"},{"location":"task2-ansible-node/#step-6-create-ansible-settings-inventory-and-variables","title":"Step 6: Create Ansible Settings, Inventory and variables","text":"<p>Ansible server uses host inventory to communicate with target devices (called as hosts). Like other programming languages, ansible uses variables to represent variations among different target devices.   This is important as Ansible works against multiple systems by selecting portions of systems listed in Ansible inventory.  Additionally, Ansible settings such as how SSH keys verification is done, name/location of inventory file to be used etc are adjustable via a configuration file named <code>ansible.cfg</code>.  It is an INI based file.</p> <ul> <li>Create folder named EVPN-Ansible as working environment and verify that it\u2019s empty by issuing below commands:</li> </ul> <pre><code>mkdir EVPN-Ansible &amp;&amp; cd EVPN-Ansible\nls\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>Next:</p> <ul> <li>Create Ansible inventory file to include Spine and Leaf switches.</li> <li>By default Ansible has inventory file saved in location /etc/ansible/hosts.</li> <li>In this lab we will create hosts file in the working environment. This file is created from ansible host prompt using <code>cat</code> command.   </li> </ul> <p>Note</p> <p>You should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code> (also shown in subsequent screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; hosts\n#define global variables, groups and host variables\n[all:vars]\nansible_connection = ansible.netcommon.network_cli\nansible_network_os = cisco.nxos.nxos\nansible_user=admin\nansible_password=C1sco12345\nansible_command_timeout=180\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\nEOF\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more hosts\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create Ansible config (ansible.cfg) using the same steps as above.  </li> </ul> <p>Note</p> <p>you should copy and paste the complete section below i.e., starting from <code>cat</code> till <code>EOF</code>  (as shown in below screenshot):</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ansible.cfg\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\ndeprecation_warnings = False\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using below command:</li> </ul> <pre><code>more ansible.cfg\n</code></pre> <p>Below screenshot shows the output of above commands:</p> <p></p> <ul> <li>To verify the file that you just created under project folder EVPN-Ansible, issue the list command:</li> </ul> <pre><code>ls\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create host variable folder named <code>host_vars</code> in folder <code>EVPN-Ansible</code> by using below command.  In this lab, we will use host_vars file to define the variables for various hosts (in next bullets):</li> </ul> <pre><code>mkdir host_vars &amp;&amp; cd host_vars\n</code></pre> <ul> <li>Create host variable file for each host in inventory by using the cat command.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.101.yml\n---\nhostname: leaf_1\nloopback0: 192.168.0.8\nloopback1: 192.168.0.18\nrouter_id: 192.168.0.8\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.101.yml</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.101.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.   The variable file for a switch is created using the below <code>cat</code> command:</li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot).  The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.103.yml\n---\nhostname: leaf_3\nloopback0: 192.168.0.10\nloopback1: 192.168.0.110\nrouter_id: 192.168.0.10\nEOF\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Now you may verify the content of this file using <code>more 198.18.4.103.yml</code> as shown below:</li> </ul> <pre><code>more 198.18.4.103.yml\n</code></pre> <p>Below screenshot shows the execution of above command:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from <code>cat</code> till <code>EOF</code> (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.104.yml\n---\nhostname: leaf_4\nloopback0: 192.168.0.11\nloopback1: 192.168.0.111\nrouter_id: 192.168.0.11\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.104.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.201.yml\n---\nhostname: spine-1\nloopback0: 192.168.0.6\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.6\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.201.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <ul> <li>Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  </li> </ul> <p>Note</p> <p>You can copy and paste starting from cat till EOF (as shown in below screenshot). The spaces in the file are important so do not remove those.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; 198.18.4.202.yml\n---\nhostname: spine-2\nloopback0: 192.168.0.7\nloopback1: 192.168.0.100\nrouter_id: 192.168.0.7\nEOF\n</code></pre> <ul> <li>Now you may verify the content of this file using <code>more</code> command as shown below:</li> </ul> <pre><code>more 198.18.4.202.yml\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p>"},{"location":"task3-first-ansible/","title":"Task 2 - First Simple Ansible Playbook","text":"<p>In this section, your will create the first Ansible Playbook for this lab.  This Ansible Playbook will be used to configure VLANs on leaf switches, and to assign VLANs to the server facing port.  Further, you will create and learn about:</p> <ul> <li>using variables inside playbook,</li> <li>loop within Ansible by using \u201cwith_items\u201d,</li> <li>logic of using \u201cwhen\u201d and \u201ctags\u201d to isolate tasks from whole playbook</li> </ul>"},{"location":"task3-first-ansible/#step-1-using-atom-text-editor","title":"Step 1: Using \"Atom\" text Editor","text":"<ul> <li> <p>Open \u201cAtom\u201d text editor by double click the icon on desktop.  Atom is the recommended text editor for this lab:</p> <p></p> </li> <li> <p>After opening Atom, you may see a \u201cRegister as default atom:// URI handler\u201d message as show in below screenshot.  If this message appears then Click the <code>\"No, Never\"</code> button, else proceed further:</p> <p></p> </li> </ul>"},{"location":"task3-first-ansible/#step-2-lab-folder-on-atom","title":"Step 2: Lab folder on Atom","text":"<p>Atom provides a Remote sync package that allows to read and push contents to a remote server.  In this case, Atom has been preconfigured to integrate with Ansible node.  As per this integration, Atom Secure copy (SCP) any new files or changes to the Ansible node upon saving.  And Atom also downloads the content from Ansible node as per instructions in this lab.  Atom displays the EVPN-Ansible folder that has been downloaded from the ansible server.</p> <ul> <li> <p>After opening Atom, there should be a folder in the left pane named: <code>EVPN-Ansible</code></p> </li> <li> <p>Right click on this pre-configured project folder <code>EVPN-Ansible</code> and select <code>New File</code> as shown below:</p> <p></p> </li> <li> <p>Name the new file <code>vlan_provision.yml</code> and hit Enter.  This will create the new file:</p> <p></p> </li> <li> <p>Also, on the lower bar right of the ATOM, verify that Language Mode (grammar) of YAML is selected instead of default <code>\"Plain Text\"</code>.  If <code>\"YAML\"</code> is not selected, then you should Select YAML from the listed options as shown in below screenshot:</p> <p></p> </li> </ul>"},{"location":"task3-first-ansible/#step-3-define-variables-tasks-for-playbook","title":"Step 3: Define variables, tasks for playbook","text":"<p>In this step, we are going to create a playbook and define the variable &amp; tasks in the playbook</p> <ul> <li>In the Atom application, under the <code>vlan_provision.yml</code> file, enter the below content:</li> </ul> <p>Note</p> <p>YAML is space sensitive. Hence be careful with the spaces in below section.  You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly.</p> <pre><code>---                     #Task 2: Simple playbook assign VLAN to server facing port\n- hosts: leaf:jinja2_leaf\n</code></pre> <p>Note</p> <ul> <li>\u201chosts:\u201d defines the scope of this playbook applies to all switches in group <code>\u2018leaf\u2019</code> and <code>\u2018jinja2_leaf\u2019</code> (these were added within the \"hosts\" file created in pervious task)</li> <li>You can review the IP addresses of the two \u201cleaf\u201d switches and one \u201cjinja2_leaf\u201d switch in the \u201chosts\u201d file (configured in previous steps).  For reference, the IP addresses are:<ul> <li>jinja2_leaf: 198.18.4.104</li> <li>leaf: 198.18.4.101</li> <li>leaf: 198.18.4.103</li> </ul> </li> </ul>"},{"location":"task3-first-ansible/#step-4-vlan-tasks-in-playbook","title":"Step 4: VLAN tasks in playbook","text":"<ul> <li>In the Atom, in the same <code>vlan_provision.yml</code> add below tasks below the previously added content:</li> </ul> <p>Note</p> <p>YAML is space sensitive. Hence be careful with the spaces in below section.  You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly.</p> <pre><code>\ttasks:\n- name: provision VLAN\ncisco.nxos.nxos_vlans:\nconfig:\n- vlan_id: \"{{item}}\"\nstate: active\nwith_items:\n-  140\n-  141\ntags: add vlans\n</code></pre> <p>Note</p> <ul> <li>This task creates multiple VLANs using cisco.nxos.nxos_vlans module</li> <li>Above only one task is added but multiple tasks can be defined in one playbook under \u201ctasks\u201d, each starts with <code>\u201c-\u201c</code></li> <li>At the end of this play, we use \u201ctags\u201d to name the task \u201cadd vlans\u201d. This is useful to run a specific part of the configuration without running the complete playbook.</li> </ul> <ul> <li> <p>Below screenshot shows how playbook will look:</p> <p></p> </li> </ul> <p>Note</p> <p>Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct.</p> <ul> <li> <p>Click <code>File</code> and <code>Save</code> .  This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> <p></p> </li> </ul> <p>Note</p> <p>Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansible host (198.18.134.150) and saving the vlan_provision.yml file.</p>"},{"location":"task3-first-ansible/#step-5-execute-playbook","title":"Step 5: Execute playbook","text":"<p>After creating the playbook, it is now time to execute the playbook.  Before executing the playbook, we will verify the leaf switch that desired vlan configurations are not present</p> <ul> <li>Using the MTPuTTy client, Login (SSH) to <code>leaf-3</code> switch (or any leaf switch as per hosts: variable in the <code>vlan_provision.yml</code> file).  Then execute below command:</li> </ul> <pre><code>show vlan brief\n</code></pre> <p>This command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:</p> <p></p> <ul> <li> <p>Now, from the MTPuTTy, launch a new ssh into Ansible node (198.18.134.150)</p> </li> <li> <p>Execute the ansible playbook using below commands under directory <code>EVPN-Ansible</code> :</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook vlan_provision.yml --tags \"add vlans\"\n</code></pre> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message. The below screenshot shows the execution of the playbook:</p> <p></p> <p>Note</p> <p>If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.</p> <ul> <li>After playbook is run successfully, go back to MTPuTTy and login via SSH into <code>leaf 3</code> again (or the other switch that you logged in earlier in this step) and check if vlan 140 and vlan 141 appears by executing below command again:</li> </ul> <pre><code>show vlan brief\n</code></pre> <p>Below screenshot shows the execution of above command in the switch:</p> <p></p>"},{"location":"task3-first-ansible/#step-6-server-port-vlan-tasks-in-playbook","title":"Step 6: Server port VLAN tasks in playbook","text":"<p>We have just tested our first playbook with basic configuration (i.e, by adding 2 VLANs).  Now we are going to add more tasks in the same/existing playbook <code>vlan_provision.yml</code> in this step:</p> <ul> <li> <p>Lets add new tasks in the Ansible playbook to assign VLANs to server facing port.  We will configure VLAN towards the server facing ports</p> </li> <li> <p>Go back to ATOM and add the following tasks to the existing playbook</p> </li> </ul> <p>Note</p> <p>YAML is space sensitive. Hence be careful with the spaces in below section.  You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly.</p> <pre><code>\t- name: configure server facing port to L2\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: eth1/3\nmode: layer2\n- name: configure VLAN for server port\nwhen: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname)\ncisco.nxos.nxos_l2_interfaces:\nconfig:\n- name: eth1/3\naccess:\nvlan: 140\nstate: overridden\n- name: configure VLAN for server port\nwhen: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname)\ncisco.nxos.nxos_l2_interfaces:\nconfig:\n- name: eth1/3\naccess:\nvlan: 141\nstate: overridden\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> on ATOM.  This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</li> </ul> <p>In this new play, we used nxos module \u201ccisco.nxos.nxos_interfaces\u201d and \u201ccisco.nxos.nxos_l2_interfaces\u201d:</p> <ul> <li>\u201ccisco.nxos.nxos_interfaces\u201d provides the capability to manage the physical attributes of an interface.  In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet \u2153</li> <li>\u201ccisco.nxos.nxos_l2_interfaces\u201d provides the capability to manage the Layer 2 switchport attributes.  In this example, it is used to configure access mode on Ethernet ports \u2153</li> <li>We used \u201cwhen\u201d argument to provide little logic of looking at the IP addresses (as per hosts i.e., inventory file) to the play and then appropriately assign VLANs to those hosts.  In our example, the playbook assigned:<ul> <li>VLAN 140 on leaf-1 and leaf-3 switches (by matching <code>101</code> and <code>103</code> in the <code>hosts</code> file)</li> <li>VLAN 141 on leaf-4 switch (by matching <code>104</code> in the <code>hosts</code> file)</li> </ul> </li> </ul>"},{"location":"task3-first-ansible/#step-7-execute-playbook","title":"Step 7: Execute playbook","text":"<p>Now, we are going to execute this playbook:</p> <ul> <li>Before executing the ansible playbook, log into switch (leaf1, leaf3 or leaf4) using MTPuTTy client.  And check the existing configuration by executing the below command.  In this lab, the default configuration of Leaf-1 and Leaf-3 Ethernet \u2153 ports are configured as trunks.  While Ethernet \u2153 port of Leaf-4 has \"no switchport\" configured.</li> </ul> <pre><code>show run interface ethernet1/3\n</code></pre> <ul> <li> <p>On MTPuTTy, log back into (or launch a new ssh) into \u201cAnsible\u201d node</p> </li> <li> <p>Execute below command to run the Ansible playbook in the <code>EVPN-Ansible</code> directory:</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook vlan_provision.yml\n</code></pre> <p>Below screeenshot shows the execution of above playbook:</p> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message. </p> <ul> <li>After we push the configuration, login to the leaf-1 or leaf-3 switch, and confirm if the server facing port has the access vlan 140 configured with the below command:</li> </ul> <pre><code>show run interface ethernet1/3\n</code></pre> <p>The output of above command on leaf-3 is shown in below screenshot:</p> <p></p> <p>The output of above command on leaf-4 confirms that vlan 141 has been configured as shown in below screenshot:</p> <p></p> <p>Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.</p>"},{"location":"task4-vxlan-jinja2-new/","title":"Task 3 - Use Jinja2 templates with Ansible","text":"<p>In this section, we will use Ansible roles and Jinja2 templating engine to create templates for spine and leaf switches.  Use of Jinja template provide flexibility and agility.  Jinja2 template will appear similar to the NXOS configurations. We abstract the variables out of the configuration and use simple <code>for</code> loop to feed variables into the template.  This shows the power of using Jinja2 templating engine.</p> <p>In this task, you will configure VXLAN fabric using this Jinja2 templates for one leaf (leaf-4) and one Spine (spine2) switch.</p> <p></p>"},{"location":"task4-vxlan-jinja2-new/#step-1-install-jinja2","title":"Step 1: Install jinja2","text":"<ul> <li>On the Ansible node, install jinja2 using <code>pip install jinja2</code> command below.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d:</li> </ul> <pre><code>pip install jinja2\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p>"},{"location":"task4-vxlan-jinja2-new/#step-2-playbook-for-jinja2_spine-host","title":"Step 2: Playbook for \"jinja2_spine\" host","text":"<p>In this step, we will create a new playbook (named: jinja2_fabric.yml) for the host named \"jinja2_spine\".  This playbook will execute tasks by using Ansible role named <code>jinja2_spine</code>.  This role uses Jina2 template and Ansible to provision the VXLAN Fabric and will be configured in subsequent steps of this task.</p> <ul> <li> <p>Switch to Atom, then Right Click on the folder <code>EVPN-Ansible</code> and Click <code>New File</code> to create a new playbook named <code>jinja2_fabric.yml</code> as shown below</p> <p></p> </li> <li> <p>Name the new file <code>jinja2_fabric.yml</code> and hit enter as shown below. It will create this new file:</p> </li> </ul> <p></p> <ul> <li> <p>Also, on the lower bar of the Atom, verify that the language/grammar of YAML is selected instead of default \"Plain Text\".  If YAML is not selected, then you should choose it from the listed options.</p> </li> <li> <p>Now enter (copy and paste) below data in this playbook:</p> </li> </ul> <pre><code>---\n- hosts: jinja2_spine\nconnection: local\nroles:\n- jinja2_spine\n</code></pre> <p>The contents of the jinja2_fabric.yml file should look like below screenshot:</p> <p></p> <ul> <li> <p>On Atom, you should Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</p> </li> <li> <p>On the MTPuTTy, go back to the ssh session of the Ansible Server node (198.18.134.150)</p> </li> <li> <p>On Ansible server node (198.18.134.150), verify that the (below highlighted) 2 groups named  <code>jinja2_spine</code> and <code>jinja2_leaf</code> exists in the inventory file name \"hosts\" (under EVPN-Ansible directory) exists by issuing below commands:</p> </li> </ul> <pre><code>cd /root/EVPN-Ansible\nmore hosts\n</code></pre> <p>Below screenshot confirms the ouptut and appropriate IP addresses of above command:</p> <p></p>"},{"location":"task4-vxlan-jinja2-new/#step-3-ansible-role-structure","title":"Step 3: Ansible role structure","text":"<p>Role is very useful technique to manage a set of playbooks in Ansible.  In this lab, we will use two roles to manage configuration for Spine and Leaf switches.</p> <p>A role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.  Here are few folders:</p> <ul> <li><code>main.yml</code> file in <code>/vars</code>  folder contains variables for a role</li> <li><code>main.yml</code> file in <code>/tasks</code> folder contains Ansible playbook for a role</li> <li><code>templates</code> folder may contain Jinja2 files, if these are used for a role </li> </ul> <p>To proceed further with roles in subsequent Tasks:</p> <ul> <li>Create roles directory in folder EVPN-Ansible by issuing below commands:</li> </ul> <pre><code>cd /root/EVPN-Ansible\nmkdir roles\n</code></pre> <p>Below screenshot shows the execution of above commands:</p> <p></p> <p>This will be used in the subsequent tasks in this lab.</p>"},{"location":"task4-vxlan-jinja2-new/#step-4-initialize-two-ansible-roles-jinja2_spine--jinja2_leaf","title":"Step 4: Initialize two Ansible roles (jinja2_spine &amp; jinja2_leaf)","text":"<p>In this section, we will create two new roles for provisioning Fabric with Jina2 template.</p> <ul> <li>On the MTPuTTy, go back to Ansible Server node (198.18.134.150).  Switch to \u2018roles\u2019 directory and then create <code>\u2018jinja2_spine\u2019</code> and <code>\u2018jinja2_leaf\u2019</code> roles using <code>ansible-galaxy init ...</code> as per below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/\ncd roles/\nansible-galaxy init jinja2_spine &amp;&amp; ansible-galaxy init jinja2_leaf\n</code></pre> <ul> <li> <p>Below screenshot shows the output of above command:</p> <p></p> </li> </ul> <p>Note</p> <p>\u2018ansible-galaxy\u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.</p> <ul> <li>change directory path to <code>EVPN-Ansible/roles/jinja2_spine</code> and check the content of local directory (<code>ls</code>) as per below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/roles/jinja2_spine/\nls\n</code></pre> <ul> <li> <p>Below screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps</p> <p></p> </li> </ul> <p>Next:</p> <ul> <li>Create empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/roles\ntouch jinja2_spine/templates/spine.j2\ntouch jinja2_leaf/templates/leaf.j2\n</code></pre> <ul> <li>Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder <code>EVPN-Ansible</code>, then Click <code>Remote Sync</code>, and Select <code>Download Folder</code> as shown in below screenshot:</li> </ul> <p></p> <ul> <li>Once the download is complete, you should see the new folder (such as <code>roles</code>) and all files appear on Atom as well.</li> </ul>"},{"location":"task4-vxlan-jinja2-new/#step-5-variables-for-jina2_spine-role","title":"Step 5: Variables for \u201cjina2_spine\u201d role","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit this <code>main.yml</code> file to include the variables that will be used in jinja2 template.</p> <ul> <li> <p>Switch to Atom, then open up the project folder <code>EVPN-Ansible</code> from the left pane and Open <code>main.yml</code> file under \u201croles/jinja2_spine/vars/\u201d as shown below:</p> <p></p> </li> <li> <p>use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file to include the below variables that will be used in jinja2 template.  You can copy and paste all of the below to replace any existing content into <code>main.yml</code> file.</p> </li> </ul> <p>Note</p> <p>As per steps in previous tasks, be careful with content since its space sensitive.</p> <pre><code>---\n# vars file for jinja2_spine\nasn: 65000\nbgp_neighbors:\n- remote_as: 65000\nneighbor: 192.168.0.8\nupdate_source: Loopback0\n- remote_as: 65000\nneighbor: 192.168.0.10\nupdate_source: Loopback0\n- remote_as: 65000\nneighbor: 192.168.0.11\nupdate_source: Loopback0\nL3_interfaces:\n-  interface: Ethernet 1/1\n-  interface: Ethernet 1/2\n-  interface: Ethernet 1/3\n-  interface: Ethernet 1/4\n-  interface: loopback 0\n-  interface: loopback 1\ns1_loopback: 192.168.0.6\ns2_loopback: 192.168.0.7\n</code></pre> <ul> <li> <p>Contents of the \u2018main.yml\u2019 file should look like below screenshot:</p> <p></p> </li> <li> <p>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> </ul>"},{"location":"task4-vxlan-jinja2-new/#step-6-jinja2-template-for-jinja2_spine-role","title":"Step 6: Jinja2 template for \"jinja2_spine\" role","text":"<ul> <li>On Atom, Select the project folder <code>EVPN-Ansible</code> from the left pane.  Then under \u201croles/jinja2_spine/templates\u201d,  Open <code>spine.j2</code> file as shown in below screenshot:</li> </ul> <p>Note</p> <p>If the file does not appear on the ATOM, then go ahead and then execute steps outlined on this page to get it sync.  If the <code>spine.j2</code> file appears in above folder then can proceed below**</p> <ul> <li>use \u201cAtom\u201d to edit the \u201cspine.j2\u201d file for jinja2 template.  You can copy and paste all of the below content into <code>spine.j2</code> file.  </li> </ul> <p>Note</p> <p>As per steps in previous tasks, be careful with content since its space sensitive.</p> <pre><code>feature bgp\nfeature nv overlay\nfeature vn-segment-vlan-based\nnv overlay evpn\nfeature pim\n!\nrouter bgp {{ asn }}\nrouter-id {{ router_id }}\naddress-family ipv4 unicast\naddress-family l2vpn evpn\nretain route-target all\n!\n#for loop to configure bgp neighbor for each leaf\n{% for neighbor in bgp_neighbors %}\nneighbor {{ neighbor['neighbor'] }}\nremote-as {{neighbor['remote_as']}}\nupdate-source {{neighbor['update_source']}}\naddress-family ipv4 unicast\nsend-community both\nroute-reflector-client\naddress-family l2vpn evpn\nsend-community both\nroute-reflector-client\n!\n{% endfor %}\ninterface loopback 1\nip address {{loopback1}}/32\nip pim sparse-mode\nip router ospf 1 area 0.0.0.0\n\n!\nip pim rp-address {{loopback1}}\nip pim anycast-rp {{loopback1}} {{s1_loopback}}\nip pim anycast-rp {{loopback1}} {{s2_loopback}}\n!\n#for loop to enable pim on link to each leaf\n{% for interface in L3_interfaces %}\ninterface {{interface['interface']}}\nip pim sparse-mode\n!\n{% endfor %}\n</code></pre> <ul> <li> <p>Next on Atom, Click on <code>File</code> and then <code>Save</code> to push template file to Ansible node.</p> </li> <li> <p>You can verify that updated file content is on Ansible server (198.18.134.150) using your SSH session by issuing below commands:</p> </li> </ul> <pre><code>cd /root/EVPN-Ansible/roles/jinja2_spine/templates\nmore spine.j2\n</code></pre> <p>Partial output of above command is shown in below screenshot confirming the content:</p> <p></p>"},{"location":"task4-vxlan-jinja2-new/#step-7-tasks-for-jinja2_spine-role","title":"Step 7: Tasks for \"jinja2_spine\" role","text":"<p>The playbook for <code>jinja2_spine</code> roles has two tasks.</p> <ul> <li>First play in the task uses ansible \"template\" module to generate configuration file based on jinja2 template created in last step.  The configuration file is saved in \u201cfile\u201d folder.</li> <li>Second play in the task pushes the configuration to switch by using ansible \"cisco.nxos.nxos_config\" module.</li> </ul> <p>Note</p> <p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit the main.yml file.</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code>.  Then open <code>main.yml</code> file under <code>roles/jinja2_spine/tasks/</code> to include below content:</li> </ul> <pre><code>---\n# tasks file for jinja2_spine\n- name: Generate Spine Config\ntemplate: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n- name: Push Spine Config\ncisco.nxos.nxos_config:\nsrc: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\nmatch: none\n</code></pre> <p>Contents of the \u2018main.yml\u2019 file should look like below:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</li> </ul> <p>In the above <code>tasks/main.yml</code> file, ansible module named \u201ccisco.nxos.nxos_config\u201d is used.  This module performs below activities:</p> <ul> <li>It uses source path of the file (\u201csrc\u201d) that contains the configuration or configuration template to load into spine.</li> <li>Since \u201cmatch\u201d option is set to none, hence the module will not attempt to compare the source configuration with the running configuration on the remote device.</li> </ul>"},{"location":"task4-vxlan-jinja2-new/#step-8-run-jinja2_fabricyml-playbook-for-spine","title":"Step 8: Run \"Jinja2_fabric.yml\" playbook for spine","text":"<p>In this section you will run the playbook created earlier in step 2.  The execution of this playbook will generate configuration file for Spine-2 switch from the template.  Further, The playbook will also push the configuration file to Spine-2 switches.</p> <ul> <li>Run the ansible playbook by going to folder EVPN-Ansible and executing the below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/\nansible-playbook jinja2_fabric.yml\n</code></pre> <p>Note: You can ignore the <code>[WARNING]</code> messages for <code>ansible-pylibssh</code> and <code>timeout for nxos_facts</code>. Note: You can ignore the <code>timeout for nxos_facts</code> message.</p> <p>Note</p> <p>It will take few minutes to push configuration</p> <p>Below screenshot shows the execution of above playbook:</p> <p></p> <p>To verify the execution of this playbook, you can:</p> <ul> <li> <p>Login/SSH to Spine-2 switch using MTPuTTy to verify that configuration has been pushed by double clicking the Spine-2 icon in the left pane on MTPuTTy.    If prompted, then login with credentials of <code>admin</code> and <code>C1sco12345</code></p> </li> <li> <p>Execute below command on Spine-2 switch to confirm the configurations have been provisioned:</p> </li> </ul> <pre><code>show run bgp\n</code></pre> <p>The execution of this command is captured in below screenshot:</p> <p></p>"},{"location":"task4-vxlan-jinja2-new/#step-9-tasks-for-jinja2_leaf-role","title":"Step 9: Tasks for \"jinja2_leaf\" role","text":"<p>In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on one of the leaf (leaf-4).   We are going to add jinja2_leaf this time in the hosts list to the already created playbook in step 2.</p> <ul> <li> <p>Switch to \u201cAtom\u201d, then click on the folder <code>EVPN-Ansible</code>, select the existing playbook <code>jinja2_fabric.yml</code> file for a role for leaf (named jinja2_leaf)</p> </li> <li> <p>Add (i.e., Copy and Paste) the below content at the end of existing file i.e., add the below content to existing content in this file.  </p> </li> </ul> <p>Note</p> <p>Do not overwrite existing content of this file.  You must add below content to this file.</p> <pre><code>- hosts: jinja2_leaf\nconnection: local\nroles:\n- jinja2_leaf\n</code></pre> <p>Below screenshot shows the contents of jinja2_fabric.yml file in Atom after adding the above configs:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package</li> </ul>"},{"location":"task4-vxlan-jinja2-new/#step-10-variables-for-jinja2_leaf-role","title":"Step 10: Variables for \"jinja2_leaf\" role","text":"<ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/jinja2_leaf/vars/</code> to include following:</li> </ul> <pre><code>---\n# vars file for jinja2_leaf\nasn: 65000\nbgp_neighbors:\n-  remote_as: 65000\nneighbor: 192.168.0.6\nupdate_source: Loopback0\n-  remote_as: 65000\nneighbor: 192.168.0.7\nupdate_source: Loopback0\nrp_address: 192.168.0.100\nL3_interfaces:\n-  interface: Ethernet 1/1\n-  interface: Ethernet 1/2\n-  interface: loopback 0\n-  interface: loopback 1\nL2VNI:\n-  vlan_id: 140\nvni: 50140\nip_add: 172.21.140.1\nmask: 24\nvlan_name: L2-VNI-140-Tenant1\nmcast: 239.0.0.140\n-  vlan_id: 141\nvni: 50141\nip_add: 172.21.141.1\nmask: 24\nvlan_name: L2-VNI-141-Tenant1\nmcast: 239.0.0.141\nL3VNI:\n-  vlan_id: 999\nvlan_name: L3-VNI-999-Tenant1\nvni: 50999\n</code></pre> <p>Below screenshot shows the contents of <code>roles/jinja2_leaf/vars/main.yml</code> file in Atom:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-jinja2-new/#step-11-jinja2-template-for-jinja2_leaf-role","title":"Step 11: Jinja2 template for \"jinja2_leaf\" role","text":"<ul> <li>On Atom, you can Select the project folder <code>EVPN-Ansible</code> and open <code>leaf.j2</code> file under <code>roles/jinja2_leaf/templates/</code></li> </ul> <p>Note</p> <p>If the file does not appear on the ATOM, then go ahead and execute the steps on this page to get it sync.  If the <code>leaf.j2</code> file appears in above folder then you can proceed with below steps further.</p> <ul> <li>On \u201cAtom\u201d, edit this <code>leaf.j2</code> file for jinja2 template by copy and paste all of the below content in it.  !!! Note     As per steps in previous tasks, be careful with content since its space sensitive.</li> </ul> <pre><code>feature bgp\nfeature nv overlay\nfeature vn-segment-vlan-based\nnv overlay evpn\nfeature pim\n!\nip pim rp-address {{rp_address}}\nspanning-tree vlan 1,140,141,999 priority 4096\n{% for L2VNI in L2VNI %}\nvlan {{L2VNI['vlan_id']}}\nname {{L2VNI['vlan_name']}}\nvn-segment {{L2VNI['vni']}}\n!\n{% endfor %}\n\n{% for L3VNI in L3VNI %}\nvlan {{L3VNI['vlan_id']}}\nvn-segment {{L3VNI['vni']}}\nvrf context Tenant-1\nvni {{L3VNI['vni']}}\nrd auto\naddress-family ipv4 unicast\nroute-target both auto\nroute-target both auto evpn\n!\n{% endfor %}\n\nfabric forwarding anycast-gateway-mac 0000.2222.3333\n!\n#for loop to configure SVI\n{% for L2VNI in L2VNI %}\ninterface Vlan{{L2VNI['vlan_id']}}\nno shutdown\nvrf member Tenant-1\nno ip redirects\nip address {{L2VNI['ip_add']}}/{{L2VNI['mask']}}\nfabric forwarding mode anycast-gateway\n!\n{% endfor %}\n{% for L3VNI in L3VNI %}\ninterface vlan{{L3VNI['vlan_id']}}\nno shutdown\nvrf member Tenant-1\nip forward\n!\n{% endfor %}\n#for loop to enable PIM on L3 interface\n{% for interface in L3_interfaces %}\ninterface {{interface['interface']}}\nip pim sparse-mode\n!\n{% endfor %}\n\ninterface nve1\nno shutdown\nsource-interface loopback1\nhost-reachability protocol bgp\n{% for L2VNI in L2VNI %}\nmember vni {{L2VNI['vni']}}\nmcast-group {{L2VNI['mcast']}}\n{% endfor %}\n{% for L3VNI in L3VNI %}\nmember vni {{L3VNI['vni']}} associate-vrf\n!\n{% endfor %}\n\nrouter bgp {{ asn }}\nrouter-id {{ router_id }}\naddress-family ipv4 unicast\naddress-family l2vpn evpn\nretain route-target all\n#for loop to configure bgp neighbor with spine\n{% for neighbor in bgp_neighbors %}\nneighbor {{neighbor['neighbor']}}\nremote-as {{neighbor['remote_as']}}\nupdate-source {{neighbor['update_source']}}\naddress-family ipv4 unicast\nsend-community both\naddress-family l2vpn evpn\nsend-community both\n!\n{% endfor %}\nevpn\n{% for L2VNI in L2VNI %}\nvni {{L2VNI['vni']}} l2\nrd auto\nroute-target import auto\nroute-target export auto\n{% endfor %}\n</code></pre> <ul> <li>Next on Atom, you can Select <code>File</code> and <code>Save</code> to push template file to Ansible node.</li> </ul>"},{"location":"task4-vxlan-jinja2-new/#step-12-tasks-for-jinja2_leaf-role","title":"Step 12: Tasks for \"jinja2_leaf\" role","text":"<p>The playbook for jinja2_leaf roles has two tasks:</p> <ol> <li>First task uses ansible \"template\" module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in <code>files</code> directory.</li> <li>Second task is to push the configuration using ansible \"cisco.nxos.nxos_config\" module to switch.</li> </ol> <p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit this main.yml file.</p> <ul> <li>On Atom, you can Select project folder <code>EVPN-Ansible</code> and Click to edit \u201c<code>main.yml</code>\u201d file under \u201c<code>roles/jinja2_leaf/tasks/</code>\u201d and include following content:</li> </ul> <pre><code>---\n# tasks file for jinja2_leaf\n- name: Generate Leaf Config\ntemplate: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n- name: Push Leaf Config\ncisco.nxos.nxos_config:\nsrc: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\nmatch: none\n</code></pre> <p>Below screenshot shows how the contents of <code>jinja2_leaf/taks/main.yml</code> file looks like in Atom:</p> <p></p>"},{"location":"task4-vxlan-jinja2-new/#step-13-run-jinja2_fabricyml-playbook-for-leaf","title":"Step 13: Run \"Jinja2_fabric.yml\" playbook for leaf","text":"<p>In this section you will run the playbook created in step 8, this will generate configuration file for Leaf-4 switch.  It will also push the configurations to Spine-2 in addition to Leaf-4 switch.</p> <ul> <li>Before running the ansible-playbook, on the MTPuTTy you can login/SSH into the leaf-4, and verify that no bgp configurations exist by running below command:</li> </ul> <pre><code>show running bgp\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), run the below command (<code>ansible-playbook jinja2_fabric.yml</code>) to execute the playbook:</li> </ul> <pre><code>ansible-playbook jinja2_fabric.yml\n</code></pre> <p>Note</p> <p>It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background.</p> <p>Below screenshot shows the execution of above command:</p> <p>Note: You can ignore the <code>timeout for nxos_facts</code> message.</p> <p></p> <ul> <li>After the configuration push is successful, login/SSH (on MTpuTT SSH session) to leaf-4 switch to verify configuration has been pushed by running below command:</li> </ul> <pre><code>show running-config bgp\n</code></pre> <p>The output of above command is shown below:</p> <p></p> <p>Congratulations: you have successfully concluded this task of using jinja2 templates with Ansible for Cisco Nexus switches</p>"},{"location":"task4-vxlan-nxos/","title":"Task 4 - Config switches using Ansible NXOS modules","text":"<p>In this section, you will build remaining VXLAN fabric using Ansible NXOS modules.  These modules will be used to configure leaf-1, leaf-3 and spine-1 switches within the VXLAN Fabric.  We will configure BGP neighbors between spine and leaf switching by using Ansible NXOS modules.  The following NXOS ansible modules are used:</p> cisco.nxos.nxos_feature Manage features on Nexus switches cisco.nxos.nxos_bgp Manage BGP config cisco.nxos.nxos_bgp_neighbor Manage BGP neighbor config cisco.nxos.nxos_bgp_af Manage BGP address-famility config cisco.nxos.nxos_bgp_neighbor_af Manage BGP neighbor address-famility config <p></p> <p>In comparison to Jinja2, NXOS modules are more abstract from NXOS CLI based configuration. There is no need to have knowledge of exact NXOS CLI syntax to use NXOS modules.  You will follow the steps to configure BGP, Multicast, VXLAN and EVPN.  In each step, you will use different Ansible NXOS modules to accomplish the step.</p> <p></p> <p>After each step, you can login the switches to verify configuration changes.</p>"},{"location":"task4-vxlan-nxos/#step-1-create-new-playbook","title":"Step 1: Create new playbook","text":"<p>Again we will use roles structure to make the playbook more modular.  The roles included in the new playbook are \u201cspine\u201d and \u201cleaf\u201d.  </p> <ul> <li> <p>Switch to \u201cAtom\u201d, then right click on the folder <code>EVPN-Ansible</code> and create a new playbook named <code>nxos_fabric.yml</code>.  Enter this file name and hit enter</p> </li> <li> <p>In the <code>nxos_fabric.yml</code> enter the content as shown below:</p> </li> </ul> <pre><code>---\n\n- hosts: spine\nconnection: local\nroles:\n- spine\n\n- hosts: leaf\nconnection: local\nroles:\n- leaf\n</code></pre> <p>Below screenshot shows the actual content:</p> <p></p> <ul> <li>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-2-create-spine--leaf-roles","title":"Step 2: Create Spine &amp; Leaf roles","text":"<ul> <li>On the MTpuTTy, go back (or initiate a new login/SSH) to Ansible Server node (198.18.134.150).  Switch to \u2018roles\u2019 directory; then create \u2018spine\u2019 and \u2018leaf\u2019 roles using ansible-galaxy as per below commands:</li> </ul> <pre><code>cd ~/EVPN-Ansible/roles/\nansible-galaxy init spine &amp;&amp; ansible-galaxy init leaf\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p> <ul> <li>Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder <code>EVPN-Ansible</code>, then open <code>Remote Sync</code> select <code>Download Folder</code> as shown below:</li> </ul> <p></p>"},{"location":"task4-vxlan-nxos/#step-3-spine-role---tasks","title":"Step 3: Spine role - tasks","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder.  We are going to use \u201cAtom\u201d to edit the main.yml file.</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/spine/tasks/</code> to include following:</li> </ul> <pre><code>---\n# tasks file for spine\n#task to configure bgp neighbor to all leaf switches\n- name: Enable BGP\ncisco.nxos.nxos_feature:\nfeature: bgp\nstate: enabled\ntags: bgp\n- name: Configure BGP AS\ncisco.nxos.nxos_bgp:\nasn: \"{{ asn }}\"\nrouter_id: \"{{ router_id }}\"\nstate: present\ntags: bgp\n- name: Configure BGP AF\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: ipv4\nsafi: unicast\ntags: bgp\n- name: Configure iBGP neighbors\ncisco.nxos.nxos_bgp_neighbor:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nremote_as: \"{{ item.remote_as }}\"\nupdate_source: \"{{ item.update_source }}\"\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n- name: Configure iBGP neighbor AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: ipv4\nsafi: unicast\nroute_reflector_client: \"true\"\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul> <p>In the above <code>task/main.yml</code> file multiple NXOS ansible modules have been used:</p> <ul> <li> <p>\u201ccisco.nxos.nxos_feature\u201d module provides the capability to manage features in NX-OS features.  It is used to enable bgp as a feature in above configurations</p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp\u201d module provides the capability to manage BGP configuration in NX-OS.  Here it is used to configure bgp</p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp_af\u201d module provides the capability to manage BGP Address-family configuration in NX-OS.  </p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp_neighbor\u201d module is used to configure the BGP Neighbour in NX-OS.  </p> </li> <li> <p>\u201ccisco.nxos.nxos_bgp_neighbor_af\u201d module provides the capability to manage BGP Address-family Neighbour configuration in NX-OS.  </p> </li> </ul>"},{"location":"task4-vxlan-nxos/#step-4-spine-role---vars","title":"Step 4: Spine role - vars","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file</p> <ul> <li>Switch to Atom, then open up the project folder <code>EVPN-Ansible</code> from the left pane and open <code>main.yml</code> file under \u201croles/spine/vars/\u201d and enter below content:</li> </ul> <pre><code>---\n# vars file for spine\nasn: 65000\n\nbgp_neighbors:\n- { remote_as: 65000, neighbor: 192.168.0.8, update_source: Loopback0 }\n- { remote_as: 65000, neighbor: 192.168.0.10, update_source: Loopback0 }\n- { remote_as: 65000, neighbor: 192.168.0.11, update_source: Loopback0 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-5--leaf-role---tasks","title":"Step 5:  Leaf role - tasks","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201ctasks\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file</p> <ul> <li>On Atom, open up the project folder <code>EVPN-Ansible</code> and edit <code>main.yml</code> file under <code>roles/leaf/tasks/</code> to include following:</li> </ul> <pre><code>---\n# tasks file for leaf\n#task to configure bgp neighbor to all spine switches\n- name: Enable BGP\ncisco.nxos.nxos_feature:\nfeature: bgp\nstate: enabled\ntags: bgp\n- name: Configure BGP AS\ncisco.nxos.nxos_bgp:\nasn: \"{{ asn }}\"\nrouter_id: \"{{ router_id }}\"\nstate: present\ntags: bgp\n- name: Configure BGP AF\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: ipv4\nsafi: unicast\ntags: bgp\n- name: Configure iBGP neighbors\ncisco.nxos.nxos_bgp_neighbor:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nremote_as: \"{{ item.remote_as }}\"\nupdate_source: \"{{ item.update_source }}\"\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n- name: Configure iBGP neighbor AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: ipv4\nsafi: unicast\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: bgp\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-6-leaf-role---vars","title":"Step 6: Leaf role - vars","text":"<p>\u201cansible-galaxy\u201d automatically creates empty \u201cmain.yml\u201d file under \u201cvars\u201d folder. We can use \u201cAtom\u201d to edit the main.yml file</p> <ul> <li>Switch to ATOM, then open up the project folder <code>EVPN-Ansible</code> from the left pane and open <code>main.yml</code> file under \u201croles/leaf/vars/\u201d and enter below content:</li> </ul> <pre><code>---\n# vars file for leaf\nasn: 65000\n\nbgp_neighbors:\n- { remote_as: 65000, neighbor: 192.168.0.6, update_source: Loopback0 }\n- { remote_as: 65000, neighbor: 192.168.0.7, update_source: Loopback0 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#step-7-execute-playbook","title":"Step 7: Execute playbook","text":"<ul> <li>On the Ansible node (in MTputty SSH session), run the command (<code>ansible-playbook nxos_fabric.yml --tags \"bgp\"</code>) to execute the playbook as shown below:</li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook nxos_fabric.yml --tags \"bgp\"\n</code></pre> <p>Below screenshots shows the execution of above playbook:  </p> <ul> <li>After the configuration push is successful, login (on MTputty SSH session) to leaf-1, leaf-3 or spine-1 switch to verify configuration has been pushed and BGP neighbours are operational by running below command:</li> </ul> <pre><code>show ip bgp summary\n</code></pre> <p>The output of above command providing the BGP neighbours info on Spine-1 and Leaf-3 is shown below:</p> <p></p> <p></p>"},{"location":"task4-vxlan-nxos/#step-8-multicast-config-with-ansible-nxos-modules","title":"Step 8: Multicast config with Ansible NXOS modules","text":"<p>In this section, we will be configuring underlay multicast to support BUM traffic in the VXLAN fabric. The NXOS modules we will be using in this section are nxos_feature    Manage fatures on Nexus switchs nxos_pim_interface  Manage PIM interface configuration</p> cisco.nxos.nxos_pim_rp_address Manage static RP configuration cisco.nxos.nxos_config Manage NXOS arbitrary configuration command cisco.nxos.nxos_interface_ospf Manage configuration OSPF interface instance cisco.nxos.nxos_interfaces Manage physical attribute of interface cisco.nxos.nxos_l3_interfaces Manage L3 interfaces on Cisco NXOS network devices cisco.nxos.nxos_pim_interface Manages PIM interface configuration"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role","title":"Edit playbook for spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the <code>\u201cmain.yml\u201d</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and open <code>\u201cmain.yml\u201d</code> file under <code>\u201croles/spine/tasks/\u201d</code> and save the below tasks at the end the file.</li> </ul> <p>Note</p> <p>Do not replace existing content</p> <pre><code>#task to enable pim and configure anycast rp for underlay multicast\n- name: Enable PIM\ncisco.nxos.nxos_feature:\nfeature: pim\nstate: enabled\ntags: multicast\n- name: Configure Anycast RP interfce\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: loopback1\nenabled: true\ntags: multicast\n- name: Configure IP Address on New LP1\ncisco.nxos.nxos_l3_interfaces:\nconfig:\n- name: loopback1\nipv4:\n- address: \"{{ loopback1 }}/32\"\ntags: multicast\n- name: Configure PIM int\ncisco.nxos.nxos_pim_interface:\ninterface: \"{{ item.interface }}\"\nsparse: true\nwith_items: \"{{L3_interfaces}}\"\ntags: multicast\n- name: Enable OSPF on New LP1\ncisco.nxos.nxos_ospf_interfaces:\nconfig:\n- name: loopback1\naddress_family:\n- afi: ipv4\nprocesses:\n- process_id: \"1\"\narea:\narea_id: 0.0.0.0\ntags: multicast\n- name: Configure PIM RP\ncisco.nxos.nxos_pim_rp_address:\nrp_address: \"{{ loopback1 }}\"\ntags: multicast\n- name: Configure Anycast RP\ncisco.nxos.nxos_config:\nlines:\n- \"ip pim anycast-rp {{ loopback1 }} {{ s1_loopback }}\"\n- \"ip pim anycast-rp {{ loopback1 }} {{ s2_loopback }}\"\ntags: multicast\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role","title":"Edit variable file for Spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the variables file for Spine i.e. <code>\u201cmain.yml\u201d</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and add the below content at the end of <code>\u201cmain.yml\u201d</code> file under <code>\u201croles/spine/vars/\u201d</code></li> </ul> <p>Note</p> <p>Do not replace existing content.</p> <pre><code>  L3_interfaces:\n- { interface: Ethernet1/1 }\n- { interface: Ethernet1/2 }\n- { interface: Ethernet1/3 }\n- { interface: Ethernet1/4 }\n- { interface: loopback0 }\n- { interface: loopback1 }\ns1_loopback: 192.168.0.6\ns2_loopback: 192.168.0.7\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role","title":"Edit playbook for leaf role","text":"<ul> <li>use \u201cAtom\u201d to edit the <code>\u201cmain.yml\u201d</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and add below content at the end of <code>\u201cmain.yml\u201d</code> file under <code>\u201croles/leaf/tasks/\u201d</code>.</li> </ul> <p>Note</p> <p>Do not replace existing content.  </p> <ul> <li>On Atom, Make sure to click <code>File-&gt;Save</code> after entering the below data in this file so it is pushed to Ansible server:</li> </ul> <pre><code>#task to enable PIM for underlay multicast\n- name: Enable PIM\ncisco.nxos.nxos_feature:\nfeature: pim\nstate: enabled\ntags: multicast\n- name: Configure PIM int\ncisco.nxos.nxos_pim_interface:\ninterface: \"{{ item.interface }}\"\nsparse: true\nwith_items: \"{{L3_interfaces}}\"\ntags: multicast\n- name: Configure PIM RP\ncisco.nxos.nxos_pim_rp_address:\nrp_address: \"{{ rp_address }}\"\ntags: multicast\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role","title":"Edit variable file for leaf role","text":"<ul> <li>Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cEVPN-Ansible\u201d and add below content at the end of \u201cmain.yml\u201d file under \u201croles/leaf/vars/\u201d.  On Atom, Make sure to click File-&gt;Save after entering the below data in this file so it is pushed to Ansible server:</li> </ul> <pre><code>  rp_address: 192.168.0.100\nL3_interfaces:\n- { interface: Ethernet1/1 }\n- { interface: Ethernet1/2 }\n- { interface: loopback0 }\n- { interface: loopback1 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes","title":"Run the playbook and verify configuration changes","text":"<ul> <li>Execute the playbook by running <code>ansible-playbook nxos_fabric.yml --tags \"multicast\"</code> command on Ansible server as shown below :</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml --tags \"multicast\"\n</code></pre> <p>Below screenshots shows the output of above command:  </p> <ul> <li> <p>login to any leaf (leaf1, leaf3, leaf4) or Spine-1 switch (via SSH using MTPutty) to verify multicast configuration and PIM neighbors by executing command: <pre><code>show ip pim neighbor\n</code></pre></p> </li> <li> <p>Below screenshot shows the output of above command (<code>show ip pim neighbor</code>) from Spine-1:</p> </li> </ul> <p></p> <p>This confirms Multicast has been enabled using Ansible modules</p>"},{"location":"task4-vxlan-nxos/#step-9-vxlan-config-with-ansible-nxos-modules","title":"Step 9: VXLAN config with Ansible NXOS modules","text":"<p>In this section, we will be configuring VXLAN on leaf and spine switches. The NXOS modules we will be using in this section are</p> nxos_feature Manages features on Nexus switches cisco.nxos.nxos_evpn_global Handles EVPN control plane for VXLAN cisco.nxos.nxos_vlan Manages VLAN resources and attributes cisco.nxos.nxos_vrf Manages global VRF configuration cisco.nxos.nxos_vrf_af Manages VRF address falimily cisco.nxos.nxos_overlay_global Configuration anycast gateway MAC cisco.nxos.nxos_vxlan_vtep Manages VXLAN Network Virtualization Endpoint cisco.nxos.nxos_vxlan_vtep_vni Creates Virtual Network Identifier member"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role_1","title":"Edit playbook for spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file. Open up the project folder \u201cEVPN-Ansible\u201d and open \u201cmain.yml\u201d file under \u201croles/spine/tasks/\u201d and enter the below content (you may copy &amp; paste with correct spaces) at the end of the file</li> </ul> <p>Note</p> <p>Do not replace existing content</p> <pre><code>#task to configure vxlan fabric\n- name: Enable VXLAN Feature\ncisco.nxos.nxos_feature:\nfeature: \"{{item}}\"\n# provider: \"{{cisco.nxos.nxos_provider }}\"\nstate: enabled\nwith_items:\n- nv overlay\n- vn-segment-vlan-based\ntags: vxlan\n- name: Enable NV Overlay\ncisco.nxos.nxos_evpn_global:\nnv_overlay_evpn: true\ntags: vxlan\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role_1","title":"Edit variable file for Spine role","text":"<p>No new variable required for Spine</p>"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role_1","title":"Edit playbook for leaf role","text":"<ul> <li>use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cEVPN-Ansible\u201d and open \u201cmain.yml\u201d file under \u201croles/leaf/tasks/\u201d.  Add the below content in the file at the end of the file (i.e., in addition to existing content), and then make sure to click \u201cFile\u201d-&gt;\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server.</li> </ul> <pre><code>#task to configure VXLAN fabric\n- name: Enable VXLAN Feature\ncisco.nxos.nxos_feature:\nfeature: \"{{ item }}\"\nstate: enabled\nwith_items:\n- nv overlay\n- vn-segment-vlan-based\ntags: vxlan\n- name: Enable VXLAN Feature\ncisco.nxos.nxos_feature:\nfeature: \"{{ item }}\"\nstate: enabled\nwith_items:\n- nv overlay\n- vn-segment-vlan-based\ntags: vxlan\n- name: Enable NV Overlay\ncisco.nxos.nxos_evpn_global:\nnv_overlay_evpn: true\ntags: vxlan\n- name: Configure VLAN to VNI\ncisco.nxos.nxos_vlans:\nconfig:\n- vlan_id: \"{{ item.vlan_id }}\"\nmapped_vni: \"{{ item.vni }}\"\nname: \"{{ item.vlan_name }}\"\nwith_items:\n- \"{{ L2VNI }}\"\n- \"{{ L3VNI }}\"\ntags: vxlan\n- name: Configure Tenant VRF\ncisco.nxos.nxos_vrf:\nvrf: Tenant-1\nrd:  auto\nvni: \"{{ L3VNI[0].vni }}\"\ntags: vxlan\n- name: Configure VRF AF\ncisco.nxos.nxos_vrf_af:\nvrf: Tenant-1\nroute_target_both_auto_evpn: true\nafi: ipv4\ntags: vxlan\n- name: Configure Anycast GW\ncisco.nxos.nxos_overlay_global:\nanycast_gateway_mac: 0000.2222.3333\ntags: vxlan\n- name: Configure L2VNI\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: vlan\"{{ item.vlan_id }}\"\nfabric_forwarding_anycast_gateway: true\nwith_items: \"{{ L2VNI }}\"\ntags: vxlan\n- name: Configure L3VNI\ncisco.nxos.nxos_interfaces:\nconfig:\n- name: vlan\"{{ L3VNI[0].vlan_id }}\"\nip_forward: true\ntags: vxlan\n- name: No shut VLAN\ncisco.nxos.nxos_config:\nlines:\n- no shutdown\nparents: interface vlan{{ item.vlan_id }}\nwith_items:\n- \"{{ L2VNI }}\"\n- \"{{ L3VNI }}\"\ntags: vxlan\n- name: Assign interface to Tenant VRF\ncisco.nxos.nxos_vrf_interface:\nvrf: Tenant-1\ninterface: \"vlan{{ item.vlan_id }}\"\nwith_items:\n- \"{{ L2VNI }}\"\n- \"{{ L3VNI }}\"\ntags: vxlan\n- name: Configure SVI IP\ncisco.nxos.nxos_l3_interfaces:\nconfig:\n- name: \"vlan{{ item.vlan_id }}\"\nipv4:\n- address: \"{{ item.ip_add }}/{{ item.mask }}\"\nwith_items: \"{{ L2VNI }}\"\ntags: vxlan\n- name: Configure VTEP Tunnel\ncisco.nxos.nxos_vxlan_vtep:\ninterface: nve1\nshutdown: \"false\"\nsource_interface: Loopback1\nhost_reachability: \"true\"\ntags: vxlan\n- name: Configure L2VNI to VTEP\ncisco.nxos.nxos_vxlan_vtep_vni:\ninterface: nve1\nvni: \"{{ item.vni }}\"\nmulticast_group: \"{{ item.mcast }}\"\nwith_items: \"{{ L2VNI }}\"\ntags: vxlan\n- name: Configure L3VNI to VTEP\ncisco.nxos.nxos_vxlan_vtep_vni:\ninterface: nve1\nvni: \"{{ L3VNI[0].vni }}\"\nassoc_vrf: true\ntags: vxlan\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role_1","title":"Edit variable file for leaf role","text":"<ul> <li>Use \u201cAtom\u201d to edit the <code>main.yml</code> file. Open up the project folder <code>EVPN-Ansible</code> and open <code>main.yml</code> file under <code>roles/leaf/vars/</code>.  Add the below content in the file at the end of the file and then make sure to click <code>\u201cFile\u201d -&gt; \u201cSave\u201d</code> on Atom, so that the updated file is pushed to Ansible server</li> </ul> <p>Note</p> <p>Do not replace existing content. Below contents must be added at the end of the file.</p> <pre><code>  L2VNI:\n- { vlan_id: 140, vni: 50140, ip_add: 172.21.140.1, mask: 24, vlan_name: L2-VNI-140-Tenant1, mcast: 239.0.0.140 }\n- { vlan_id: 141, vni: 50141, ip_add: 172.21.141.1, mask: 24, vlan_name: L2-VNI-141-Tenant1, mcast: 239.0.0.141 }\nL3VNI:\n- { vlan_id: 999, vlan_name: L3-VNI-999-Tenant1, vni: 50999 }\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_1","title":"Run the playbook and verify configuration changes","text":"<ul> <li>On Ansible server (via SSH connection on MTputty), run the below command:</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml --tags \"vxlan\"\n</code></pre> <ul> <li> <p>Below screenshot show partial output of above command:</p> <p> </p> </li> <li> <p>After finishing this task: login to any leaf switch (on MTPutty) to verify VXLAN configuration and the VNI by issuing the command:  </p> </li> </ul> <pre><code>show nve vni\n</code></pre> <p>Below screenshot shows the output of above command from Leaf-3:</p> <p></p>"},{"location":"task4-vxlan-nxos/#step-10-evpn-config-with-ansible-nxos-modules","title":"Step 10: EVPN config with Ansible NXOS modules","text":"<p>In this section, we will be configuring BGP EVPN on leaf and spine switches. The NXOS modules we will be using in this section are:</p> cisco.nxos.nxos_bgp_af Manage BGP address-famility config cisco.nxos.nxos_bgp_neighbor_af Manage BGP neighbor address-famility config cisco.nxos.nxos_evpn_vni Manage Cisco EVPN VXLAN Network Identifier"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role_2","title":"Edit playbook for spine role","text":"<ul> <li>Use \u201cAtom\u201d to edit the <code>main.yml</code> file. Open up the project folder <code>\u201cEVPN-Ansible\u201d</code> and open <code>main.yml</code> file under <code>\u201croles/spine/tasks/\u201d</code>.  Add the below content in the file in addition to existing content.</li> <li>Then make sure to click <code>\u201cFile\u201d-&gt;\u201cSave\u201d</code> on Atom, so that the updated file is pushed to Ansible server.</li> </ul> <pre><code># task to configure BGP EVPN\n- name: Configure BGP EVPN\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: l2vpn\nsafi: evpn\ntags: evpn\n- name: Configure iBGP neighbor EVPN AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: l2vpn\nsafi: evpn\nroute_reflector_client: \"true\"\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: evpn\n</code></pre>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role_2","title":"Edit variable file for Spine role","text":"<p>No new variables required for Spine</p>"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role_2","title":"Edit playbook for leaf role","text":"<ul> <li>use \u201cAtom\u201d to edit the <code>main.yml</code> file. Open up the project folder <code>EVPN-Ansible</code> and open <code>main.yml</code> file under <code>\u201croles/leaf/tasks/\u201d</code>.  Add the below content in the file in addition to existing content.</li> <li>Then make sure to click <code>\u201cFile\u201d-&gt;\u201cSave\u201d</code> on Atom, so that the updated file is pushed to Ansible server.</li> </ul> <pre><code>#task to configure BGP EVPN\n- name: Configure BGP EVPN\ncisco.nxos.nxos_bgp_af:\nasn: \"{{ asn }}\"\nafi: l2vpn\nsafi: evpn\ntags: evpn\n- name: Configure iBGP neighbor EVPN AF\ncisco.nxos.nxos_bgp_neighbor_af:\nasn: \"{{ asn }}\"\nneighbor: \"{{ item.neighbor }}\"\nafi: l2vpn\nsafi: evpn\nsend_community: both\nwith_items: \"{{ bgp_neighbors }}\"\ntags: evpn\n- name: Configure L2VNI RD/RT\ncisco.nxos.nxos_evpn_vni:\nvni: \"{{ item.vni }}\"\nroute_distinguisher: auto\nroute_target_both: auto\nwith_items: \"{{ L2VNI }}\"\ntags: evpn\n</code></pre> <ul> <li>Click <code>File</code> and <code>Save</code> . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</li> </ul>"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role_2","title":"Edit variable file for leaf role","text":"<p>No new variables required for Leaf</p>"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_2","title":"Run the playbook and verify configuration changes","text":"<ul> <li>On the Ansible server (using MTPutty) run this playbook by running the command <code>ansible-playbook nxos_fabric.yml --tags \"evpn\"</code> as shown below:</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml --tags \"evpn\"\n</code></pre> <p>Below screenshot shows the output of above playbook:</p> <p></p> <ul> <li>After successful execution of the playbook: login to any leaf or spine switch to verify BGP EVPN configuration and evpn neighbor by issuing command: <pre><code>show bgp l2vpn evpn summary\n</code></pre></li> </ul> <p>Below screenshot shows the output of above command from leaf-3 switch.  As expected it shows the spine-1 and spine-2 as neighbors:</p> <p></p>"},{"location":"task4-vxlan-nxos/#step-11-run-nxos_fabric-playbook","title":"Step 11: Run nxos_fabric playbook","text":"<ul> <li>Up to this point, you have run the playbook for each step separately (using tags).  You could re-run the whole playbook without giving any tags, but no new changes should be maded to the switches.</li> </ul> <pre><code>ansible-playbook nxos_fabric.yml\n</code></pre>"},{"location":"task4-vxlan-nxos/#step-12-verify-end-to-end-ip-connectivity","title":"Step 12: Verify end-to-end IP connectivity","text":"<p>Now Let\u2019s verify the VXLAN bridging and VXLAN routing from servers that are pre-configured in following VLANs and IPs</p> Server Name Connect to switch In VLAN IP of server Server-1 Leaf-1 140 172.21.140.10 Server-3 Leaf-3 140 172.21.140.11 Server-4 Leaf-4 141 172.21.141.11 <p>Below figure shows the topology &amp; connectivity of servers to Leaf switches and their respective IP addresses:</p> <p></p> <ul> <li> <p>Switch to MTPuTTY and SSH to <code>server-1</code>.  If prompted enter the credentials of <code>root</code>and <code>C1sco12345</code></p> </li> <li> <p>Ping default gateway from server-1 by issuing command <code>ping 172.21.140.1 -c 5</code> as shown below:</p> </li> </ul> <pre><code>[root@server-1 ~]# ping 172.21.140.1 -c 5\nPING 172.21.140.1 (172.21.140.1) 56(84) bytes of data.\n64 bytes from 172.21.140.1: icmp_seq=2 ttl=255 time=15.7 ms\n64 bytes from 172.21.140.1: icmp_seq=3 ttl=255 time=4.11 ms\n</code></pre> <ul> <li>Next, Ping server 3 and server 4 from server-1 (in same VLAN and inter-VLAN respectively) by using <code>ping 172.21.140.11 -c 5</code> and <code>ping 172.21.141.11 -c 5</code> commands as shown below:</li> </ul> <pre><code>[[root@server-1 ~]# ping 172.21.140.11 -c 5\nPING 172.21.140.11 (172.21.140.11) 56(84) bytes of data.\n64 bytes from 172.21.140.11: icmp_seq=1 ttl=64 time=1032 ms\n64 bytes from 172.21.140.11: icmp_seq=2 ttl=64 time=35.7 ms\n64 bytes from 172.21.140.11: icmp_seq=3 ttl=64 time=14.4 ms\n^C\n--- 172.21.140.11 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2112ms\nrtt min/avg/max/mdev = 14.431/360.839/1032.335/474.899 ms, pipe 2\n[root@server-1 ~]# ping 172.21.141.11 -c 5\nPING 172.21.141.11 (172.21.141.11) 56(84) bytes of data.\n64 bytes from 172.21.141.11: icmp_seq=994 ttl=62 time=30.2 ms\n64 bytes from 172.21.141.11: icmp_seq=995 ttl=62 time=16.1 ms\n64 bytes from 172.21.141.11: icmp_seq=996 ttl=62 time=18.0 ms\n</code></pre> <p>Congratulation! You have successfully built VXLAN fabric using ansible + Jinja2 template and ansible + NXOS modules.</p>"},{"location":"task5-day2-pipeline0/","title":"Task 5: Day 2 operation using CI Pipeline","text":"<p>We will implement CI/CD pipeline for day 2 operation tasks in this section.  GitLab and GitLab runner will be used for this purpose</p> <ul> <li>GitLab is software development platform that incorporates multiple capabilities including Version Control, code review and CI/CD in single system. </li> <li>The cloud hosted GitLab (SaaS) is used in this lab.  VCS and CI/CD capabilities of GitLab will be used in this lab. </li> <li>To execute tasks in a private network (i.e. behind Firewall, private infrastructure), GitLab Runner is used.  </li> <li>GitLab Runner is an application that run tests &amp; results i.e., jobs in a pipeline and then send the results to GitLab.  GitLab runner will be installed on the same local server as Ansible host in this lab setup.</li> </ul> <p>As a Version Control System (VCS), a Git repository will created on the GitLab (SaaS).  All the code (Ansible playbooks, roles etc.) will be saved on this repository.  The centralized &amp; Cloud hosted (SaaS) VCS provided by GitLab allows to maintain version control, track changes of files, collaboration among many engineers etc.  In this lab, the CI/CD component of GitLab will be integrated with VCS, so that changes made on repository can automatically trigger execution of job via a pipeline file.</p> <p>The process for creating a CI/CD pipeline with GitLab &amp; GitLab runner include below procedure:</p> <ul> <li>Create a project in GitLab.</li> <li>Install and register the GitLab Runner for your project.</li> <li>Define a CI/CD job (steps, tests) in a file named <code>.gitlab-ci.yml</code> in the root of the repository.</li> <li>Conditions/rules can be defined in <code>.gitlab-ci.yml</code> file (YAML syntax) that will perform a job and display the results in GitLab pipeline e.g. When a commit to repository is done then the runner may execute the job and display the results in GitLab pipeline.</li> </ul> <p>In this section, CI/CD pipeline for day 2 operation tasks will be implemented.  Below steps will be performed:</p> <ul> <li>Create an Ansible playbook to verify underlay and overlay</li> <li>Version control for the EVPN Ansible playbooks using GitLab version control capabilities</li> <li>Create CI pipeline using GitLab</li> <li>Add new VNIs into existing fabric</li> <li>Verify CI pipeline in test and staging stages</li> <li>Commit the merger and verify CI Pipeline in production stage</li> </ul>"},{"location":"task5-day2-pipeline0/#step-1-playbook-to-verify-underlay-and-overlay","title":"Step 1: Playbook to verify underlay and overlay","text":"<p>In this step, you will create the playbook to verify underlay and overlay operation. The playbook will be applied to all leaf switches to verify the below commands:</p> <p>Underlay</p> <pre><code>-   show ip ospf neighbor\n-   show ip bgp sum\n-   show ip pim neighbor\n</code></pre> <p>Overlay</p> <pre><code>-   show nve vni\n-   show nve peer\n-   show ip route vrf Tenant-1\n-   show bgp l2vpn evpn\n-   show l2route evpn mac-ip all\n</code></pre> <ul> <li>Switch to Atom.  On the left page right click on the folder <code>EVPN-Ansible</code> and create a new playbook named <code>verify_fabric.yml</code>.   Enter this file name and hit enter.  Then Copy and Paste the below content to this newly created file:</li> </ul> <pre><code>---\n- hosts: leaf, jinja2_leaf\nconnection: local\ngather_facts: false\ntasks:\n- name: verify underlay\nregister: underlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show ip ospf neighbors\n- show ip bgp sum\n- show ip pim neighbor\ntags: underlay\n- debug: var=underlay_output.stdout_lines\ntags: underlay\n- set_fact:\nsavefile: \"{{underlay_output.stdout_lines | to_nice_yaml}}\"\n- local_action: copy content=\"{{savefile}}\" dest=./underlay.txt\n- name: Verify Overlay\nregister: overlay_output\ncisco.nxos.nxos_command:\ncommands:\n- show nve vni\n- show nve peer\n- show ip route vrf Tenant-1\n- show bgp l2vpn evpn\n- show l2route evpn mac-ip all\ntags: overlay\n- debug: var=overlay_output.stdout_lines\ntags: overlay\n- set_fact:\nsavefile: \"{{overlay_output.stdout_lines | to_nice_yaml}}\"\n- local_action: copy content=\"{{savefile}}\" dest=./overlay.txt\n</code></pre> <ul> <li> <p>Click <code>File</code> and <code>Save</code> on Atom. This will save the playbook, and also scp the playbook to Ansible server using pre-configured \u201cremote-sync\u201d package.</p> </li> <li> <p>On the Ansible node (via MTPuTTy), run verify_fabric.yml playbook and verify the output for underlay by executing below command (using respective tag).  This command will show ospf, bgp and pim neighbors for all leaf switches:</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\nansible-playbook verify_fabric.yml --tags \"underlay\"\n</code></pre> <ul> <li> <p>Below screenshot shows the partial output of above command and shows ospf, bgp and pim neighbors for all leaf switches:</p> <p></p> </li> </ul> <p>Here is complete log of execution of above playbook/command:</p> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook verify_fabric.yml --tags \"underlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [verify underlay] ***********************************************************************************************************************************************************************************************\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.101] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:12 10.0.0.21       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:12 10.0.128.5      Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.8, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     161     106        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     161     106        6    0    0 01:23:15 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.21       Ethernet1/1          01:23:08  00:01:35  1        yes     n/a     no\",\n            \"10.0.128.5      Ethernet1/2          01:23:07  00:01:31  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.104] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          1d04h    10.0.128.1      Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          1d04h    10.0.128.17     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.11, local AS number 65000\",\n            \"BGP table version is 5, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     672     662        5    0    0 07:03:12 0         \",\n            \"192.168.0.7     4 65000    1441    1433        5    0    0 22:36:01 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.128.1      Ethernet1/1          08:51:43  00:01:28  1        yes     n/a     no\",\n            \"10.0.128.17     Ethernet1/2          22:36:23  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"underlay_output.stdout_lines\": [\n        [\n            \"OSPF Process ID 1 VRF default\",\n            \" Total number of neighbors: 2\",\n            \" Neighbor ID     Pri State            Up Time  Address         Interface\",\n            \" 192.168.0.6       1 FULL/ -          01:27:11 10.0.0.29       Eth1/1 \",\n            \" 192.168.0.7       1 FULL/ -          01:27:10 10.0.128.13     Eth1/2\"\n        ],\n        [\n            \"BGP summary information for VRF default, address family IPv4 Unicast\",\n            \"BGP router identifier 192.168.0.10, local AS number 65000\",\n            \"BGP table version is 6, IPv4 Unicast config peers 2, capable peers 2\",\n            \"0 network entries and 0 paths using 0 bytes of memory\",\n            \"BGP attribute entries [0/0], BGP AS path entries [0/0]\",\n            \"BGP community entries [0/0], BGP clusterlist entries [4/16]\",\n            \"\",\n            \"Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\",\n            \"192.168.0.6     4 65000     148     107        6    0    0 01:23:17 0         \",\n            \"192.168.0.7     4 65000     150     107        6    0    0 01:23:17 0\"\n        ],\n        [\n            \"PIM Neighbor Status for VRF \\\"default\\\"\",\n            \"Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD    ECMP Redirect\",\n            \"                                                         Priority Capable State     Capable\",\n            \"10.0.0.29       Ethernet1/1          01:23:09  00:01:37  1        yes     n/a     no\",\n            \"10.0.128.13     Ethernet1/2          01:23:08  00:01:23  1        yes     n/a     no\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p>Next:</p> <ul> <li>On the Ansible node (via MTPutty), execute the verify_fabric.yml playbook by issuing below command.  As per below, this command uses the <code>--tags</code> in the syntax to execute the respective tasks (as per the tag) in the playbook.  As per the output, verify the overlay outputs (as shown below).  Note: This command will show the nve tunnel peer, host route in BGP EVPN from all leaf switches:</li> </ul> <pre><code>ansible-playbook verify_fabric.yml --tags \"overlay\"\n</code></pre> <ul> <li> <p>Below screenshot of the partial output of above command:</p> <p></p> </li> <li> <p>Below shows the complete log output of execution of above playbook command.   Verify the output for vne vni status, vne dynamic neighbors, mac-ip evpn route update for each L2VNI, l2fib etc. information:</p> </li> </ul> <pre><code>root@ubuntu:~/EVPN-Ansible# ansible-playbook verify_fabric.yml --tags \"overlay\"\n\nPLAY [leaf, jinja2_leaf] *********************************************************************************************************************************************************************************************\n\nTASK [Verify Overlay] ************************************************************************************************************************************************************************************************\nok: [198.18.4.104]\nok: [198.18.4.103]\nok: [198.18.4.101]\n\nTASK [debug] *********************************************************************************************************************************************************************************************************\nok: [198.18.4.104] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.110    Up    CP        01:23:17 000c.2939.f53f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 22:39:01, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:52, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:23:18, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 22:38:59, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.11, Vlan141, [190/0], 07:04:08, hmm\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 458, Local Router ID is 192.168.0.11\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"* i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"* i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908    (L2VNI 50141)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d HMM    --            0          172.21.141.11  Local\"\n        ]\n    ]\n}\nok: [198.18.4.103] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.18     Up    CP        01:07:55 000c.2997.621c   \",\n            \"nve1      192.168.0.111    Up    CP        01:23:20 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:28, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.18%default, [200/0], 01:07:53, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN\",\n            \" \",\n            \"172.21.140.11/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.11, Vlan140, [190/0], 01:23:18, hmm\",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:23:20, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 195, Local Router ID is 192.168.0.10\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;i                   192.168.0.18                      100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907    (L2VNI 50140)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100      32768 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 BGP    --            0          172.21.140.10  192.168.0.18   \",\n            \"140         0050.56a0.b5d1 HMM    --            0          172.21.140.11  Local          \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\nok: [198.18.4.101] =&gt; {\n    \"overlay_output.stdout_lines\": [\n        [\n            \"Codes: CP - Control Plane        DP - Data Plane          \",\n            \"       UC - Unconfigured         SA - Suppress ARP        \",\n            \"       SU - Suppress Unknown Unicast\",\n            \" \",\n            \"Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\",\n            \"--------- -------- ----------------- ----- ---- ------------------ -----\",\n            \"nve1      50140    239.0.0.140       Up    CP   L2 [140]                  \",\n            \"nve1      50141    239.0.0.141       Up    CP   L2 [141]                  \",\n            \"nve1      50999    n/a               Up    CP   L3 [Tenant-1]\"\n        ],\n        [\n            \"Interface Peer-IP          State LearnType Uptime   Router-Mac       \",\n            \"--------- ---------------  ----- --------- -------- -----------------\",\n            \"nve1      192.168.0.110    Up    CP        01:07:59 000c.2939.f53f   \",\n            \"nve1      192.168.0.111    Up    CP        01:07:59 000c.2951.176f\"\n        ],\n        [\n            \"IP Route Table for VRF \\\"Tenant-1\\\"\",\n            \"'*' denotes best ucast next-hop\",\n            \"'**' denotes best mcast next-hop\",\n            \"'[x/y]' denotes [preference/metric]\",\n            \"'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\",\n            \"\",\n            \"172.21.140.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, direct\",\n            \"172.21.140.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.1, Vlan140, [0/0], 01:24:29, local\",\n            \"172.21.140.10/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.140.10, Vlan140, [190/0], 01:24:18, hmm\",\n            \"172.21.140.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.110%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\",\n            \" \",\n            \"172.21.141.0/24, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, direct\",\n            \"172.21.141.1/32, ubest/mbest: 1/0, attached\",\n            \"    *via 172.21.141.1, Vlan141, [0/0], 01:24:25, local\",\n            \"172.21.141.11/32, ubest/mbest: 1/0\",\n            \"    *via 192.168.0.111%default, [200/0], 01:07:51, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\"\n        ],\n        [\n            \"BGP routing table information for VRF default, address family L2VPN EVPN\",\n            \"BGP table version is 210, Local Router ID is 192.168.0.8\",\n            \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, &gt;-best\",\n            \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\",\n            \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, &amp; - backup\",\n            \"\",\n            \"   Network            Next Hop            Metric     LocPrf     Weight Path\",\n            \"Route Distinguisher: 192.168.0.8:32907    (L2VNI 50140)\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\",\n            \"                      192.168.0.18                      100      32768 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:32908    (L2VNI 50141)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.10:32907\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\",\n            \"*&gt;i                   192.168.0.110                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.11:32908\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"* i                   192.168.0.111                     100          0 i\",\n            \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i                   192.168.0.111                     100          0 i\",\n            \"\",\n            \"Route Distinguisher: 192.168.0.8:3    (L3VNI 50999)\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\",\n            \"                      192.168.0.111                     100          0 i\",\n            \"*&gt;i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\",\n            \"                      192.168.0.110                     100          0 i\"\n        ],\n        [\n            \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \",\n            \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\",\n            \"(Ps):Peer Sync (Ro):Re-Originated \",\n            \"Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \",\n            \"----------- -------------- ------ ---------- --------------- ---------------\",\n            \"140         0050.56a0.7630 HMM    --            0          172.21.140.10  Local          \",\n            \"140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \",\n            \"141         000c.2979.f00d BGP    --            0          172.21.141.11  192.168.0.111\"\n        ]\n    ]\n}\n\nPLAY RECAP ***********************************************************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\nroot@ubuntu:~/EVPN-Ansible#\n</code></pre>"},{"location":"task5-day2-pipeline0/#step-2-install-git-and-gitlab-runner","title":"Step 2: Install git and gitlab runner","text":"<p>In this section you will install git and gitlab runner on Anisble node.</p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below package installation command to install git:</li> </ul> <pre><code>cd ~/EVPN-Ansible\napt-get install git\n</code></pre> <p>Below screenshot shows the execution of the command</p> <p></p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below <code>curl</code> command to download and run a shell script that adds the official GitLab repository required for runner installation:</li> </ul> <pre><code>curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash\n</code></pre> <p>Below screenshot shows the execution of the command</p> <p></p> <ul> <li>On the Ansible node (in MTPuTTy SSH session), run the below command to install <code>gitlab-runner</code></li> </ul> <pre><code>apt-get install gitlab-runner\n</code></pre> <p>Below screenshot shows the output of above command:</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-3-version-control-playbooks","title":"Step 3: Version control playbooks","text":"<p>In this step, you will use Gitlab to set version control for the playbooks.</p> <ul> <li> <p>Open chrome browser and enter gitlab.com/users/sign_in in the address bar. Sign in with Username: <code>lab.ciscolive@gmail.com</code> Password: <code>#####</code> as shown below.</p> <p></p> </li> <li> <p>After signing to <code>lab.ciscolive</code> gitlab account, click Create a project from main page as shown below:</p> <p></p> </li> <li> <p>On the next page, click on Create a blank project.  </p> </li> <li> <p>On the next page, enter Project name of your assigned as <code>POD_{ID}</code>.  Note: In the below screenshot of this page:</p> <ul> <li>You must replace the <code>{ID}</code> with your respective pod number.  Find your assigned POD_ID from table in Task1</li> <li><code>Project slug</code> is automatically populated</li> <li>Leave the default setting of <code>Visibility Level</code> to <code>Private</code></li> <li>Leave the default setting of <code>Initialize repository with a README</code> with <code>checkbox</code> selected to automatically add <code>README</code> file in git repository</li> </ul> </li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  Project name of <code>POD_1</code> is shown as an example only.</p> <p>Below screenshot shows execution of above for Pod_1 as a reference only:</p> <p></p> <ul> <li>Then click Create project  at the bottom of this <code>Create a blank project</code> page. </li> </ul> <p>A new project for your respective Pod is now created on GitLab.</p> <ul> <li>On the project page, copy the project url by clicking on Clone button, and then click <code>Copy URL</code> icon next to Clone with HTTPS as shown below in below screenshot.  This project url will be used in next step. The project url looks like lab.ciscolive/pod_{ID}.git.</li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  </p> <p>The screenshot below uses POD_1 as example.</p> <p></p> <p>By default the main branch is protected branch.  In order to add and commit files from git command, the branch settings need to be changed to unprotected.  This change of main branch to unprotected is performed as per below steps and shown in below screenshot:</p> <ul> <li>Navigate on left sidebar (pane) and Select Settings &gt; Repository.  </li> <li>Then on right pane next to Protected branches, Click Expand. </li> <li>Further, Click on Unprotect (in Red color) as shown in below screenshot.  Also, acknolwedge by clicking on Unprotect branch on the pop-up message.</li> </ul> <p></p> <p>Now the Git repository is setup properly for colloboration. And it can be used for keeping all the code i.e., Ansible playbooks and roles files.  Since all the code resides on local machine (Ansible node), so let's push this code to this newlly created centralized repository on GitLab.</p> <ul> <li>On the Ansible node (in MTputty SSH session), run the below commands to initialize git in appropriate directory:</li> </ul> <pre><code>cd ~/EVPN-Ansible\ngit init\n</code></pre> <p>Below screenshot shows the execution of the initialize commands in correct directory:</p> <p></p> <ul> <li>On the Ansible node, run following commands to add the remote repository as origin for your respective <code>Pod {ID}</code> and verify it's result:</li> </ul> <pre><code>git remote add origin https://gitlab.com/lab.ciscolive/pod_{ID}.git\ngit remote -v\n</code></pre> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  </p> <p>Below screenshot shows the execution of the above commands for Pod_1 (as an example):</p> <p></p> <ul> <li>Then to add all the files (Ansible playbooks, roles etc) to the staging area and check the status by using below commands:</li> </ul> <pre><code>git add .\ngit status\n</code></pre> <p>Below screenshot shows the execution of the above commands:</p> <p></p> <ul> <li>Next, commit the files to local repository by executing below commands:</li> </ul> <pre><code>git checkout -b main\ngit commit -m \"initial commit\"\n</code></pre> <ul> <li>Now, lets push all files to main branch on Gitlab repository with below commands.  When prompted for credentials, enter the Username: <code>lab.ciscolive</code> Password: <code>#####</code></li> </ul> <pre><code>git push -f origin main\n</code></pre> <p>Below screenshot shows output of the above command:</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-4-add-cicd-pipeline-file","title":"Step 4: Add CI/CD pipeline file","text":"<p>In this step, you will create CI/CD pipeline file. Gitlab uses a special file named <code>.gitlab-ci.yml</code> for CI/CD configuration.  </p> <p>Note</p> <p>The file name starts with <code>.</code> and it's not a mistake.  </p> <p>This file uses YAML syntax and needs to be placed in root directory of the repository.  In this file, you define your intent for the pipeline using a declarative syntax (in YAML) such as: - The stages you want run in the pipeline - The scripts you want run in each stage - The runner you want use for each script - How is the runner triggered for each script</p> <p>Pipeline file can be added from GitLab UI using pipeline file editor, or from local and push to GitLab repo using git commands.  We will use Atom to add pipeline file in this lab.</p> <ul> <li> <p>Switch to \"Atom\" application on your remote desktop.  Right click on the folder EVPN-Ansible and Click New File to create a new file named .gitlab-ci.yml</p> </li> <li> <p>In the <code>.gitlab-ci.yml</code> file enter the contents shown below:</p> </li> </ul> <pre><code>stages:\n- test\n- staging\n- production\n\nverify:\nstage: test\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml --check\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"push\" &amp;&amp; $CI_COMMIT_BRANCH != \"main\"'\nstaging:\nstage: staging\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml\n- ansible-playbook nxos_fabric.yml\n- ansible-playbook verify_fabric.yml\nartifacts:\npaths:\n- overlay.txt\n- underlay.txt\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"push\"  &amp;&amp; $CI_COMMIT_BRANCH != \"main\"'\ndeploy:\nstage: production\ntags:\n- EVPN\nscript:\n- ansible-playbook jinja2_fabric.yml\n- ansible-playbook nxos_fabric.yml\nrules:\n- if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\nwhen: manual\n</code></pre> <p>Below screenshot shows the file after adding the above contents on Atom:</p> <p></p> <ul> <li>From Atom, go to File &gt; Save to push the new pipeline to Ansible node</li> </ul> <p>Note, in the above pipeline file, you have configured</p> <ul> <li> <p>Three stages for pipeline named test, staging and production</p> </li> <li> <p>In test stage, pipeline will perform dry-run for the Ansible playbook using shell command (<code>ansible-playbook</code> with <code>--check</code> flag).  This is executed by using a git-runner with <code>tag</code> of <code>EVPN</code>.  Further, as per the <code>rules</code> setting, the script will be triggered by change in any <code>branch</code> other than <code>main</code>.</p> </li> <li> <p>In staging stage, pipeline will run mutliple playbooks to add new VNIs on the BGP EVPN fabric.  For manual verification, this stage also collects the output of show commands in multiple files - these are referred to as <code>artifacts</code>.  Further, as per the <code>rules</code> setting, this scripts will be triggered after success of <code>test</code> stage and for any <code>branch</code> other than <code>main</code>.</p> </li> <li> <p>In production stage, pipeline will run playbook to deploy EVPN fabric for new VNIs in production. Further, as per the <code>rules</code> setting, this script will be triggered after merge to main branch with manual trigger.</p> </li> </ul> <p>Now lets push this file to GitLab centralized repository by execution of below steps:</p> <ul> <li>Switch to Ansible node (via MTPuTTY), add the <code>.gitlab-ci.yml</code> file to staging area, and then commit &amp; push the file to remote repository (on GitLab) as shown below.  When prompted for credentials, enter the Username: <code>lab.ciscolive</code> Password: <code>#####</code>for GitLab access:</li> </ul> <pre><code>git add .gitlab-ci.yml\ngit commit -m \"add ci file\"\ngit push origin main\n</code></pre> <p>Below screenshot shows the outputs of the commands for Pod_1 (note your git repository will be based upon your respective Pod_{ID}):</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-5-register-local-gitlab-runner","title":"Step 5: Register local gitlab-runner","text":"<p>In this task, you will create local gitlab runner on Ansible server and register it to your project on GitLab (SaaS) to run pipeline jobs. You will assign EVPN as tag of the runner as part of registration to your project on GitLab.</p> <ul> <li>Switch to Chrome brower with gitlab project page.  From the navigation menu on the left sidebar, Select Settings &gt; CI/CD.  Then on the right pane for the <code>Runners</code>, click on Expand setting as shown in below screenshot.</li> </ul> <p>Note</p> <p>Screenshot below uses POD 1 as example, find your assigned POD ID from table in task1.</p> <p></p> <ul> <li>After expanding the <code>Runners</code> setting, under <code>Project runners</code> click on New project runner.  </li> </ul> <p></p> <ul> <li>On the next <code>New project runner</code> page, select Linux Operating systems under the <code>Platform</code>.  Further, enter a <code>Tags</code> value of EVPN, leave other settings as default, and click Submit on this page as shown in below screenshot:</li> </ul> <p></p> <ul> <li> <p>On the next <code>Register runner</code> page, under <code>Step 1</code>, copy the runner token as part of the command line (<code>gitlab-runner register ...</code>) .  This token will be used later to register the gitlab runner that runs on Ansible server/node.  </p> </li> <li> <p>Then click on Go to runners page.  If <code>Leave site?</code> prompt is displayed, then click <code>Leave</code> to proceed.</p> </li> </ul> <p>Note</p> <p>Make sure to copy the runner token in this step.</p> <p></p> <ul> <li>As shown in below screenshot, under <code>Shared runners</code>, click on the toggle switch named Enable shared runners for this project to disable the shared runners for this project.  Below screenshot shows the output once sharing has been disabled.</li> </ul> <p></p> <ul> <li> <p>Next switch to Ansible node (in MTputty SSH session) to register the GitLab runner by issuing the below command.  As part of this registration process, below settings are configured:</p> <ul> <li><code>GitLab URL</code> pointing to the GitLab running as SaaS. </li> <li><code>Registration Token</code> for (authentication) of runner to Gitlab project.</li> <li><code>Tags</code> - When a CI/CD job runs, it knows which runner to use by looking at the assigned tags.  It allows to filter a runner from a list of available runners for a job.  In this lab only a single runner is used.</li> <li><code>Executor</code>: It determines the environment where the job runs in.  In this case Ansible playbooks are executed on a <code>shell</code> enviornment.</li> </ul> </li> </ul> <pre><code>gitlab-runner register\n</code></pre> <p>When prompted provide the runner registration info as shown below:</p> <ul> <li> <p><code>Enter the GitLab instance URL:</code> https://gitlab.com/</p> </li> <li> <p><code>Enter the registration token:</code>  Paste the <code>runner token</code> value generated and copied on GitHub (previous step)</p> </li> <li> <p><code>Enter a name for the runner:</code> EVPN</p> </li> <li> <p><code>Enter an executor:</code> shell</p> </li> </ul> <p>After entering <code>shell</code>, wait few seconds for the registration of the runner.  Below screenshot shows the outputs of the commands:</p> <p></p> <ul> <li>On the Ansible node (in MTputty SSH session), run command the below commands to check status of runner:</li> </ul> <pre><code>gitlab-runner status\ngitlab-runner list\n</code></pre> <p>Below screenshots shown the output of above commands</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-6-add-new-vnis-in-staging-and-trigger-pipeline","title":"Step 6: Add new VNIs in Staging and trigger pipeline","text":"<p>In this task, you will modify the variable files to add new networks on the EVPN fabric.  Instead of applying the changes (new VNIs) directly to <code>main</code> repository, you will create a new repository named <code>newvni</code> on gitlab to validate and test it on a staging environment.  Once the changes (new VNIs) are pushed to <code>newvni</code> branch on the GitLab repository, it will automatically trigger execution of scripts, by gitlab runner, as per configuration of pipleline file (.gitlab-ci.yml).  Keep in mind that we only have one physical environment i.e., same set of switches are used for both staging and production.</p> <ul> <li>Switch to \"Atom\".  Under EVPN-Ansible, scroll to roles &gt; jinja2_leaf &gt; vars and open \"main.yml\" file.  Enter the new VNI details in this file below the existing L2VNI list as shown in the below screenshot:</li> </ul> <pre><code>  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }  </code></pre> <p>Note</p> <p>Do not completely replace existing content in this file.  Above content should be added to the end of existing L2VNI and above the existing L3VNI variables.</p> <p>Below screenshot shows the file after addition of above contents on Atom:</p> <p></p> <ul> <li> <p>From Atom, go to File &gt; Save to push the new variables to Ansible node.</p> </li> <li> <p>Next, variables for Leaf role will be added.  From \"Atom\", open \"main.yml\" file under roles &gt; leaf &gt; vars. Enter following new VNI informtion under L2VNI as shown in below screenshot:</p> </li> </ul> <pre><code>  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n- { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }\n</code></pre> <p>Note</p> <p>Do not completely replace existing content in this file.  Above content should be added to the end of existing L2VNI and above the existing L3VNI variables.</p> <p>Below screenshot shows the file after addition of above contents on Atom:</p> <p></p> <ul> <li> <p>From Atom, go to File &gt; Save to push the new leaf variable file to Ansible node.</p> </li> <li> <p>On the Ansible node (in MTputty SSH session), change directory to EVPN-Ansible project folder</p> </li> </ul> <pre><code>cd ~/EVPN-Ansible\n</code></pre> <ul> <li>Run the following git commands to create new branch newvni, <code>commit</code> the updated variable files to that new branch and <code>push</code> to Gitlab project:</li> </ul> <pre><code>git branch -m newvni\ngit add .\ngit commit -m \"newvni\"\ngit push -f origin newvni\n</code></pre> <p>When prompted, Use username: of <code>lab.ciscolive</code> and password: <code>#####</code> for gitlab access</p> <p></p>"},{"location":"task5-day2-pipeline0/#step-7-review-pipeline-testing-and-staging-results","title":"Step 7: Review pipeline testing and staging results","text":"<p>In this task, you will review the script results from pipeline. The git push command in previous task triggered pipeline test and staging stage.</p> <ul> <li>Switch to Chrome brower with GitLab project page.  On the navigation menu on left side, select CI/CD &gt; Pipelines  as shown below:</li> </ul> <p></p> <ul> <li>Wait for the jobs finish, you can also access the runner console by clicking on any of the stage under Stages heading/column on this page and view the Job details as shown below:</li> </ul> <p></p> <ul> <li>After both pipeline stages are passed, you can verify the result from the artifacts.  Click on the down arrow on the right most of the <code>pipeline</code> page and select Download artifacts to download the <code>staging:artifacts</code> as zip file.</li> </ul> <p></p> <ul> <li> <p>Unzip the downloaded artifacts.zip on your remote desktop, and you will see overlay.txt and underlay.txt files - these are the outputs from the ansible playbook.</p> </li> <li> <p>Review the <code>overlay.txt</code> file and you can see new VNIs 50200 and 50201 deployed on staging environment as shown in partial output below:</p> </li> </ul> <pre><code>-   - 'Codes: CP - Control Plane        DP - Data Plane          '\n    - '       UC - Unconfigured         SA - Suppress ARP        '\n    - '       SU - Suppress Unknown Unicast'\n    - ' '\n    - Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\n    - '--------- -------- ----------------- ----- ---- ------------------ -----'\n    - 'nve1      50140    239.0.0.140       Up    CP   L2 [140]                  '\n    - 'nve1      50141    239.0.0.141       Up    CP   L2 [141]                  '\n    - 'nve1      50200    239.0.0.200       Up    CP   L2 [200]                  '\n    - 'nve1      50201    239.0.0.201       Up    CP   L2 [201]                  '\n    - 'nve1      50203    239.0.0.203       Up    CP   L2 [203]                  '\n    - 'nve1      50204    239.0.0.204       Up    CP   L2 [204]                  '\n    - nve1      50999    n/a               Up    CP   L3 [Tenant-1]\n-   - 'Interface Peer-IP          State LearnType Uptime   Router-Mac       '\n    - '--------- ---------------  ----- --------- -------- -----------------'\n    - nve1      192.168.0.18     Up    CP        1d15h    000c.2997.621c\n-   - IP Route Table for VRF \"Tenant-1\"\n    - '''*'' denotes best ucast next-hop'\n    - '''**'' denotes best mcast next-hop'\n    - '''[x/y]'' denotes [preference/metric]'\n    - '''%&lt;string&gt;'' in via output denotes VRF &lt;string&gt;'\n    - ''\n    - '172.21.140.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.140.1, Vlan140, [0/0], 1d15h, direct'\n    - '172.21.140.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.140.1, Vlan140, [0/0], 1d15h, local'\n    - '172.21.140.10/32, ubest/mbest: 1/0'\n    - '    *via 192.168.0.18%default, [200/0], 1d13h, bgp-65000, internal, tag 65000\n        (evpn) segid: 50999 tunnelid: 0xc0a80012 encap: VXLAN'\n    - ' '\n    - '172.21.141.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.141.1, Vlan141, [0/0], 1d15h, direct'\n    - '172.21.141.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.141.1, Vlan141, [0/0], 1d15h, local'\n    - '172.21.200.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.200.1, Vlan200, [0/0], 1d15h, direct'\n    - '172.21.200.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.200.1, Vlan200, [0/0], 1d15h, local'\n    - '172.21.201.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.201.1, Vlan201, [0/0], 1d15h, direct'\n    - '172.21.201.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.201.1, Vlan201, [0/0], 1d15h, local'\n    - '172.21.203.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.203.1, Vlan203, [0/0], 1d13h, direct'\n    - '172.21.203.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.203.1, Vlan203, [0/0], 1d13h, local'\n    - '172.21.204.0/24, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.204.1, Vlan204, [0/0], 1d13h, direct'\n    - '172.21.204.1/32, ubest/mbest: 1/0, attached'\n    - '    *via 172.21.204.1, Vlan204, [0/0], 1d13h, local'\n\n[output omitted]\n</code></pre>"},{"location":"task5-day2-pipeline0/#step-8-merge-code-to-main-branch","title":"Step 8: Merge code to main branch","text":"<p>After reviewing the test result from staging environment, you confirmed the new NVIs are deployed properly, you will merge the newvni branch to main branch and deploy the new VNIs on production environment.</p> <p>Typically in real world there will (should!) be separate environments for staging and production.  In this lab, we are using same inventory for staging and production environments for simplicity purposes.  So once the merge is done then the changes can be rolled out to production (by applying ansible playbooks).</p> <ul> <li>Switch to Chrome browser with GitLab project page for your respective <code>Pod_{ID}</code> and then select Merge requests in the left sidebar.  Select New merge request on the right pane as shown in below screenshot.</li> </ul> <p></p> <ul> <li>Under the <code>Source branch</code> section, from the <code>Select source branch</code> drop down menu select newvni branch.  </li> <li>Further, on the <code>Target branch</code> section, under <code>Select target branch</code> drop down menu make sure that main branch is selected as shown in the screenshot below.</li> </ul> <p>Note</p> <p>Below screenshot uses POD 1 as example, you must use your assigned POD ID from table in task1.</p> <p></p> <ul> <li>Then click Compare branches and continue on this page.</li> </ul> <p>Next, On the New merge request page (below screenshot):</p> <ul> <li> <p>Enter the <code>Title</code> for the merge request as Newvni Pod_<code>#</code> (replace <code>#</code> with your respective Pod ID),</p> </li> <li> <p>And un-check the Delete source branch when merge request is accepted,</p> </li> <li> <p>Then click the Create merge request button as shown in the screenshot below:</p> </li> </ul> <p>Note</p> <p>You must replace <code>{ID}</code> with your respective pod number.  Project name of <code>POD_1</code> in below screenshot is shown as an example only.</p> <p></p> <p>On the next page (below screenshot), you are displayed with the details related to Merge: </p> <ul> <li>Review the details and then click on Merge button.   </li> </ul> <p></p> <p>Wait for the <code>Merged by lab.ciscolive</code> message and for <code>Merge details</code> to appear on this page.</p>"},{"location":"task5-day2-pipeline0/#so-what-is-happening","title":"So what is happening?","text":"<p>After validating and testing deployment of VNIs in a branch named <code>newvni</code>, we have merged the new code (to deploy new VNIs) on the <code>main</code> branch.  However, the pipeline has not triggered.  Why?  Since, as per the CI/CD pipeline (filename: .gitlab-ci.yml) the <code>rules</code> setting was configured for manual trigger for production deployment.  This is just to demonstrate that, as per your environment, you may decide to be extra cautios and not deploy to production without additional review.  Hence the job for deploying production will not be triggered automatically.  Instead, you will manually kickoff the production deployment job on pipeline.  Let's proceed to initiate this manual trigger:</p> <ul> <li> <p>Switch to Chrome brower with GitLab project page (your respective <code>Pod_{ID}</code>), select CI/CD &gt; Pipelines from navigation menu on the left sidebar.</p> </li> <li> <p>Deploy the changes to production enviroment by selecting on the <code>Play</code> button and then click on deploy as shown in the screenshot below:</p> </li> </ul> <p></p> <ul> <li>Wait for the job (all the Stages) to finish. You can also access the runner console output, to see execution of Ansible playbooks by clicking the pipeline stage named <code>deploy</code> under Stages on this page.  </li> </ul> <p>Note</p> <p><code>POD_1</code> in below screenshot is shown as an example only.  You must browse through to your own Pod #.</p> <p>Below screenshot shows the successful execution of this deploy stage: </p> <p></p>"},{"location":"task5-day2-pipeline0/#congratulation-you-have-completed-vxlan-fabric-and-netdevops-automation-lab","title":"Congratulation! You have completed VXLAN Fabric and NetDevOps automation Lab.","text":""}]}